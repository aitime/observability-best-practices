{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is observability","text":""},{"location":"#what-it-is","title":"What it is","text":"<p>Observability is the capability to continuously generate and discover actionable insights based on signals from the system under observation. In other words, observability allows users to understand a system\u2019s state from its external output and take (corrective) action.</p>"},{"location":"#problem-it-addresses","title":"Problem it addresses","text":"<p>Computer systems are measured by observing low-level signals such as CPU time, memory, disk space, and higher-level and business signals, including API response times, errors, transactions per second, etc.</p> <p>The observability of a system has a significant impact on its operating and development costs. Observable systems yield meaningful, actionable data to their operators, allowing them to achieve favorable outcomes (faster incident response, increased developer productivity) and less toil and downtime.</p>"},{"location":"#how-it-helps","title":"How it helps","text":"<p>Understanding that more information does not necessarily translate into a more observable system is pivotal. In fact, sometimes, the amount of information generated by a system can make it harder to identify valuable health signals from the noise generated by the application. Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decisions.</p>"},{"location":"#what-you-will-find-here","title":"What you will find here","text":"<p>This site contains our best practices for observability: what do to, what not to do, and a collection of recipes on how to do them. Most of the content here is vendor agnostic and represents what any good observability solution will provide. </p> <p>It is important that you consider observability as a solution though and not a product. Observability comes from your practices, and is integral to strong development and DevOps leadership. A well-observed application is one that places observability as a principal of operations, similar to how security must be at the forefront of how you organize a project. Attempting to \u201cbolt-on\u201d observability after the fact is an anti-pattern and meets with less success.</p> <p>This site is organized into four categories:</p> <ol> <li>Best practices by solution, such as for dashboarding, application performance monitoring, or containers</li> <li>Best practices for the use of different data types, such as for logs or traces</li> <li>Best practices for specific AWS tools (though these are largely fungible to other vendor products as well)</li> <li>Curated recipes for observability with AWS</li> </ol> <p>Success</p> <p>This site is based on real world use cases that AWS and our customers have solved for. </p> <p>Observability is at the heart of modern application development, and a critical consideration when operating distributed systems, such as microservices, or complex applications with many external integrations. We consider it to be a leading indicator of a healthy workload, and we are pleased to share our experiences with you here!</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by AWS open source observability  service team members and other volunteers from across the organization. Our goal is to improve the discovery of relevant best practices on how to set up and use AWS services and open source projects in the observability space.</p> <p>Recipes and content contributions in general so far are from the following people:</p> <ul> <li>Alolita Sharma</li> <li>Aly Shah Imtiaz</li> <li>Dieter Adant</li> <li>Dinesh Boddula</li> <li>Elamaran Shanmugam</li> <li>Eric Hsueh</li> <li>Helen Ashton</li> <li>Imaya Kumar Jagannathan</li> <li>Jason Derrett</li> <li>Kevin Lewin</li> <li>Mahesh Biradar</li> <li>Michael Hausenblas</li> <li>Munish Dabra</li> <li>Rich McDonough</li> <li>Rob Sable</li> <li>Rodrigue Koffi</li> <li>Sheetal Joshi</li> <li>Tomasz Wrzonski</li> <li>Tyler Lynch</li> <li>Vijayan Sarathy</li> <li>Vikram Venkataraman</li> <li>Yiming Peng</li> </ul> <p>Note that all recipes published on this site are available via the  MIT-0 license, a modification to the usual MIT license  that removes the requirement for attribution.</p>"},{"location":"faq/","title":"Frequently asked questions","text":""},{"location":"faq/#how-are-logs-different-from-traces","title":"How are logs different from traces?","text":"<p>Logs are limited to a single application and the events that relate to it. For example, if a user logs into a web site hosted on a microservices platform, and makes a purchase on this site, there may be logs related to that user emitted from multiple applications:</p> <ol> <li>A front-end web server</li> <li>Authentication service</li> <li>The inventory service</li> <li>A payment processing backend</li> <li>Outbound mailer that sends the user a receipt</li> </ol> <p>Every one of these may log something about this user, and that data is all valuable. However, traces will present a single, cohesive view of the user's entire interaction across that single transaction, spanning all of these discrete components.</p> <p>In this way, a trace a collection of events from multiple services intended to show a single view of an activity, whereas logs are bound to the context of the application that created them.</p>"},{"location":"faq/#what-signal-types-are-immutable","title":"What signal types are immutable?","text":"<p>All three of the basic signal types (metrics, logs, and traces) are immutable, though some implementations have greater or lesser assurance of this. For example, immutability of logs is a strict requirement in many governance frameworks - and many tools exist to ensure this. Metrics and traces should likewise always be immutable. </p> <p>This leads to a question as to handling \"bad data\", or data that was inaccurate. With  AWS observability services, there is no facility to delete metrics or traces that were emitted in error. CloudWatch Logs does allow for the deletion of an entire log stream, but you cannot retroactively change data once it has been collected. This is by design, and an important feature to ensure customer data is treated with the utmost care.</p>"},{"location":"faq/#why-does-immutability-matter-for-observability","title":"Why does immutability matter for observability?","text":"<p>Immutability is paramount to observability! If past data can be modified then you would lose critical errors, or outliers in behaviour, that inform your choices when evolving your systems and operations. For example, a metric datapoint that shows a large gap in time does not simply show a lack of data collection, it may indicate a much larger issue in your infrastructure. Likewise, with \"null\" data - even empty timeseries are valuable.</p> <p>From a governance perspective, changing application logs or tracing after the fact violates the principal of non-reputability, where you would not be able to trust that the data in your system is precisely as it was intended be by the source application. </p>"},{"location":"faq/#what-is-a-blast-radius","title":"What is a blast radius?","text":"<p>The blast radius of a change is the potential damage that it can create in your environment. For example, if you make a database schema change then the potential risk could include the data in the database plus all of the applications that depend on it.</p> <p>Generally speaking, working to reduce the blast radius of a change is a best practice, and breaking a change into smaller, safer, and reversible chunks is always recommended wherever feasible.</p>"},{"location":"faq/#what-is-a-cloud-first-approach","title":"What is a \"cloud first\" approach?","text":"<p>Cloud-first strategies are where organization move all or most of their infrastructure to cloud-computing platforms. Instead of using physical resources like servers, they house resources in the cloud. </p> <p>To those used to co-located hardware, this might seem radical. However, the opposite is also true. Developers who adopt a cloud-first mentality find the idea of tying your servers to a physical location unthinkable. Cloud-first teams don\u2019t think of their servers as discrete pieces of hardware or even virtual servers. Instead, they think of them as software to fulfill a business function.</p> <p>Cloud-first is to the 2020's what mobile-first was to the 2010's, and virtualization was to the early 2000's. </p>"},{"location":"faq/#what-is-technical-debt","title":"What is technical debt?","text":"<p>Taken from Wikipedia:</p> <p>In software development, technical debt (also known as design debt or code debt) is the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.</p> <p>Basically, you accumulate debt over time as you add more to your workload without removing legacy code, applications, or human processes. Technical debt detracts from your absolute productivity. </p> <p>For example, if you have to spend 10% of your time performing maintenance on a legacy system that provides little or no direct value to your business, then that 10% is a cost that you pay. Reduction of technical debt equals increasing effective time to create new products that add value. </p>"},{"location":"faq/#what-is-the-separation-of-concerns","title":"What is the separation of concerns","text":"<p>In the context of observability solutions, the separation of concerns means to divide functional areas of a workload or an application into discrete components that are independently managed. Each component addresses a separate concern (such as log structure and the emitting of logs). Controlling configuration of a component without modifying the underlying code means that developers can focus on their concerns (application functionality and feature development), while DevOps personas can focus on optimizing system performance and troubleshooting.</p> <p>Separation of concerns is a core concept in computer science.</p>"},{"location":"faq/#what-is-operational-excellence","title":"What is operational excellence?","text":"<p>Operational excellence is the performance of best practices that align with operating workloads. AWS has an entire framework dedicated to being Well-Architected. See this page to get started with operational excellence.</p>"},{"location":"guides/","title":"Best practices overview","text":"<p>Observability is a broad topic with a mature landscape of tools. Not every tool is right for every solution though! To help you navigate through your observability requirements, configuration, and final deployment, we have summarized five key best practices that will inform your decision making process on your Observability strategy.</p>"},{"location":"guides/#monitor-what-matters","title":"Monitor what matters","text":"<p>The most important consideration with observability is not your servers, network, applications, or customers. It is what matters to you, your business, your project, or your users.</p> <p>Start first with what your success criteria are. For example, if you run an e-commerce application, your measures of success may be number of purchases made over the past hour. If you run a non-profit, then it may be donations vs. your target for the month. A payment processor may watch for transaction processing time, whereas universities would want to measure student attendance.</p> <p>Tip</p> <p>Success metrics are different for everyone! We may use an e-commerce application as an example here, but your projects can have a very different measurement. Regardless, the advice remains the same: know what good looks like and measure for it.</p> <p>Regardless of your application, you must start with identifying your key metrics. Then work backwards1 from that to see what impacts it from an application or infrastructure perspective. For example, if high CPU on your web servers endangers customer satisfaction, and in-turn your sales, then monitoring CPU utilization is important!</p>"},{"location":"guides/#know-your-objectives-and-measure-them","title":"Know your objectives, and measure them!","text":"<p>Having identified your important top-level KPIs, your next job is to have an automated way to track and measure them. A critical success factor is doing so in the same system that watches your workload's operations. For our e-commerce workload example this may mean:</p> <ul> <li>Publishing sales data into a time series</li> <li>Tracking user registrations in this same system</li> <li>Measure how long customers stay on web pages, and (again) push this data to a time series</li> </ul> <p>Most customers have this data already, though not necessarily in the right places from an observability perspective. Sales data can typically be found in relational databases or business intelligence reporting systems, along with user registrations. And data from visit duration can be extracted from logs or from Real User Monitoring.</p> <p>Regardless of your metric data's original location or format, it must be maintained as a time series. Every key metric that matters most to you, whether it is business, personal, academic, or for any other purpose, must be in a time series format for you to correlate it with other observability data (sometimes known as signals or telemetry).</p> <p> Figure 1: example of a time series</p>"},{"location":"guides/#context-propagation-and-tool-selection","title":"Context propagation and tool selection","text":"<p>Tool selection is important and has a profound difference in how you operate and remediate problems. But worse than choosing a sub-optimal tool is tooling for all basic signal types. For example, collecting basic logs from a workload, but missing transaction traces, leaves you with a gap. The result is an incohesive view of your entire application experiece. All modern approaches to observability depend on \"connecting the dots\" with application traces.</p> <p>A complete picture of your health and operations requires tools that collect logs, metrics, and traces, and then performs correlation, analysis, anomaly detection, dashboarding, alarms and more.</p> <p>Info</p> <p>Some observability solutions may not contain all of the above but are intended to augment, extend, or give added value to existing systems. In all cases, tool interoperability and extensibility is an important consideration when beginning an observability project.</p>"},{"location":"guides/#every-workload-is-different-but-common-tools-make-for-a-faster-results","title":"Every workload is different, but common tools make for a faster results","text":"<p>Using a common set of tools across every workload has add benefits such as reducing operational friction and training, and generally you should strive for a reduced number of tools or vendors. Doing so lets you rapidly deploy existing observability solutions to new environments or workloads, and with faster time-to-resolution when things go wrong.</p> <p>Your tools should be broad enough to observe every tier of your workload: basic infrastructure, applications, web sites, and everything in between. In places where a single tool is not possible, the best practice is to use those that have an open standard, are open source, and therefore have the broadest cross-platform integration possibilities.</p>"},{"location":"guides/#integrate-with-existing-tools-and-processes","title":"Integrate with existing tools and processes","text":"<p>Don't reinvent the wheel! \"Round\" is a great shape already, and we should always be building collaborative and open systems, not data silos.</p> <ul> <li>Integrate with existing identity providers (e.g. Active Directory, SAML based IdPs).</li> <li>If you have existing IT trouble tracking system (e.g. JIRA, ServiceNow) then integrate with it to quickly manage problems as they arise.</li> <li>Use existing workload management and escalation tools (e.g. PagerDuty, OpsGenie) if you already have them!</li> <li>Infrastructure as code tools such as Anisble, SaltStack, CloudFormation, TerraForm, and CDK are all great tools. Use them to manage your observability as well as everything else, and build your observability solution with the same infrastructure as code tools you already use today (see include observability from day one).</li> </ul>"},{"location":"guides/#use-automation-and-machine-learning","title":"Use automation and machine learning","text":"<p>Computers are good at finding patterns, and at finding when data does not follow a pattern! If you have hundreds, thousands, or even millions of datapoints to monitor, then it would impossible to understand healthy thresholds for every single one of them. But many observability solutions have anomaly detection and machine learning capabilities that manage the undifferentiated heavy lifting of baselining your data.</p> <p>We refer to this as \"knowing what good looks like\". If you have load-tested your workload thoroughly then you may know these healthy performance metrics already, but for a complex distributed application it can be unwieldy to create baselines for every metric. This is where anomaly detection, automation, and machine learning are invaluable.</p> <p>Leverage tools that manage the baselining and alerting of applications health on your behalf, thereby letting you focus on your goals, and monitor what matters.</p>"},{"location":"guides/#collect-telemetry-from-all-tiers-of-your-workload","title":"Collect telemetry from all tiers of your workload","text":"<p>Your applications do not exist in isolation, and interactions with your network infrastructure, cloud providers, internet service providers, SasS partners, and other components both within and outside your control can all impact your outcomes. It is important that you have a holistic view of your entire workload.</p>"},{"location":"guides/#focus-on-integrations","title":"Focus on integrations","text":"<p>If you have to pick one area to instrument, it will undoubtedly be your integrations between components. This is where the power of observability is most evident. As a rule, every time one component or service calls another, that call must have at least these data points measured:</p> <ol> <li>The duration of the request and response</li> <li>The status of the response</li> </ol> <p>And to create the cohesive, holistic view that observability requires, a single unique identier for the entire request chain must be included in the signals collected.</p>"},{"location":"guides/#dont-forget-about-the-end-user-experience","title":"Don't forget about the end-user experience","text":"<p>Having a complete view of your workload means understanding it at all tiers, including how your end users experience it. Measuring, quantifying, and understanding when your objectives are at risk from a poor user experience is just as important as watching for free disk space or CPU utilization - if not more important!</p> <p>If your workloads are ones that interact directly with the end user (such as any application served as a web site or mobile app) then Real User Monitoring monitors not just the \"last mile\" of delivery to the user, but how they actually have experienced your application. Ultimately, none of the observability journey matters if your users are unable to actually use your services.</p>"},{"location":"guides/#data-is-power-but-dont-sweat-the-small-stuff","title":"Data is power, but don't sweat the small stuff","text":"<p>Depending on the size of your application, you may have a very large number of components to collect signals from. While doing so is important and empowering, there can be diminished returns from your efforts. This is why the best practice is to start by monitoring what matters, use this as a way to map your important integrations and critical components, and focus on the right details.</p>"},{"location":"guides/#include-observability-from-day-one","title":"Include observability from day one","text":"<p>Like security, observability should not be an afterthought to your development or operations. The best practice is to put observability early in your planning, just like security, which creates a model for people to work with and reduces opaque corners of your application. Adding transaction tracing after major development work is done takes time, even with auto-instrumentation. The effort returns far greater returns! But doing so late in your development cycle may create some rework.</p> <p>Rather than bolting observability in your workload later one, use it to help accelerate your work. Proper logging, metric, and trace collection enables faster application development, fosters good practices, and lays the foundation for rapid problem solving going forward.</p> <ol> <li> <p>Amazon uses the working backwards process extensively as a way to obsession over our customers and their outcomes, and we highly recommend that anyone working on observability solutions work backwards from their own objectives in the same way. You can read more about working backwards on Werner Vogels's blog.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/apm/","title":"Application Performance Monitoring","text":""},{"location":"guides/choosing-a-tracing-agent/","title":"Choosing a tracing agent","text":""},{"location":"guides/choosing-a-tracing-agent/#choose-the-right-agent","title":"Choose the right agent","text":"<p>AWS directly supports two toolsets for trace collection (plus our wealth of observability partners: </p> <ul> <li>The AWS Distro for OpenTelemetry, commonly called ADOT</li> <li>The X-Ray SDKs and daemon</li> </ul> <p>The selection of which tool or tools to use is a principal decision you must make as you evolve your observability solution. These tools are not mutually-exclusive, and you can mix them together as necessary. And there is a best practice for making this selection. However, first you should understand the current state of OpenTelemetry (OTEL).</p> <p>OTEL is the current industry standard specification for observabillity signalling, and contains definitions for each of the three core signal types: metrics, traces, and logs. However, OTEL has not always existed and has evolved out of earlier specifications such as OpenMetrics and OpenTracing. Observability vendors began openly supporting OpenTelemetry Line Protocol (OTLP) in recent years. </p> <p>AWS X-Ray and CloudWatch pre-date the OTEL specification, as do other leading observability solutions. However, the AWS X-Ray service readily accepts OTEL traces using ADOT. ADOT has the integrations already built into it to emit telemetry into X-Ray directly, as well as to other ISV solutions.</p> <p>Any transaction tracing solution requires an agent and an integration into the underlying application in order to collect signals. And this, in turn, creates technical debt in the form of libraries that must be tested, maintained, and upgraded, as well as possibly retooling if you choose to change your solution in the future.</p> <p>The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. ADOT is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.</p> <p>Success</p> <p>We recommend instrumenting your application with the AWS Distro for OpenTelemetry if you need the following:</p> <ul> <li> <p>The ability to send traces to multiple different tracing backends without having to re-instrument your code. For example, of you wish to shift from using the X-Ray console to Zipkin, then only configuration of the collector would change, leaving your applicaiton code untouched.</p> </li> <li> <p>Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community. </p> </li> </ul> <p>Success</p> <p>We recommend choosing an X-Ray SDK for instrumenting your application if you need the following:</p> <ul> <li> <p>A tightly integrated single-vendor solution.</p> </li> <li> <p>Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET</p> </li> </ul>"},{"location":"guides/containers/","title":"Containers","text":""},{"location":"guides/dashboards/","title":"Dashboarding","text":""},{"location":"guides/dashboards/#best-practices","title":"Best practices","text":""},{"location":"guides/dashboards/#create-views-for-your-important-personas","title":"Create views for your important personas","text":""},{"location":"guides/dashboards/#share-your-business-metrics-along-side-operational-ones","title":"Share your business metrics along side operational ones","text":""},{"location":"guides/dashboards/#dashboards-should-build-bridges-not-walls","title":"Dashboards should build bridges, not walls","text":""},{"location":"guides/dashboards/#your-dashboards-should-tell-a-story","title":"Your dashboards should tell a story","text":""},{"location":"guides/dashboards/#dashboards-should-be-available-to-technical-and-non-technical-users","title":"Dashboards should be available to technical and non-technical users","text":""},{"location":"guides/dashboards/#recommendations","title":"Recommendations","text":""},{"location":"guides/dashboards/#leverage-existing-identity-providers","title":"Leverage existing identity providers","text":""},{"location":"guides/databases/","title":"Database monitoring prescriptive guidance","text":""},{"location":"guides/full-stack/","title":"Full-stack","text":""},{"location":"guides/strategy/","title":"Creating an observability strategy","text":""},{"location":"recipes/","title":"Recipes","text":"<p>In here you will find curated guidance, how-to's, and links to other resources that help with the application of observability (o11y) to various use cases. This includes managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana as well as agents, for example OpenTelemetry and Fluent Bit. Content here is not resitricted to AWS tools alone though, and many open source projects are referenced here. </p> <p>We want to address the needs of both developers and infrastructure folks equally, so many of the recipes \"cast a wide net\". We encourge you to explore and find the solutions that work best for what you are seeking to accomplish.</p> <p>Info</p> <p>The content here is derived from actual customer engagement by our Solutions Architects, Professional Services, and feedback from other customers. Everything you will find here has been implemented by our actual customers in their own environments.</p> <p>The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specific solution:</p> dimension examples Destinations Prometheus \u00b7 Grafana \u00b7 OpenSearch \u00b7 CloudWatch \u00b7 Jaeger Agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent Languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust Infra &amp; databases RDS \u00b7 DynamoDB \u00b7 MSK Compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda \u00b7 AppRunner Compute engine Fargate \u00b7 EC2 \u00b7 Lightsail <p>Example solution requirement</p> <p>I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption</p> <p>One stack that would fit this need is the following:</p> <ol> <li>Destination: An S3 bucket for further consumption of data</li> <li>Agent: FluentBit to emit log data from EKS</li> <li>Language: Python</li> <li>Infra &amp; DB: N/A</li> <li>Compute unit: Kubernetes (EKS)</li> <li>Compute engine: EC2</li> </ol> <p>Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes.</p> <p>To simplify navigation, we're grouping the six dimension into the following categories:</p> <ul> <li>By Compute: covering compute engines and units</li> <li>By Infra &amp; Data: covering infrastructure and databases</li> <li>By Language: covering languages</li> <li>By Destination: covering telemetry and analytics</li> <li>Tasks: covering anomaly detection, alerting, troubleshooting, and more</li> </ul> <p>Learn more about dimensions \u2026</p>"},{"location":"recipes/#how-to-use","title":"How to use","text":"<p>You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, <code>By Compute</code> -&gt; <code>EKS</code> -&gt; <code>Fargate</code> -&gt; <code>Logs</code>.</p> <p>Alternatively, you can search the site pressing <code>/</code> or the <code>s</code> key:</p> <p></p> <p>License</p> <p>All recipes published on this site are available via the  MIT-0 license, a modification to the usual MIT license  that removes the requirement for attribution.</p>"},{"location":"recipes/#how-to-contribute","title":"How to contribute","text":"<p>Start a discussion on what you plan to do and we take it from there.</p>"},{"location":"recipes/#learn-more","title":"Learn more","text":"<p>The recipes on this site are a good practices collection. In addition, there  are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so  check out:</p> <ul> <li>observability @ aws, a playlist of AWS folks talking about    their projects and services.</li> <li>AWS observability workshops, to try out the offerings in a   structured manner.</li> <li>The AWS monitoring and observability homepage with pointers   to case studies and partners.</li> </ul>"},{"location":"recipes/aes/","title":"Amazon OpenSearch Service","text":"<p>Amazon OpenSearch Service (AOS), successor to Amazon Elasticsearch Service, makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed  search and analytics suite derived from Elasticsearch. It offers the latest  versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), and visualization capabilities powered by OpenSearch Dashboards and Kibana  (1.5 to 7.10 versions). </p> <p>Check out the following recipes:</p> <ul> <li>AOS tutorial: a quick start guide</li> <li>Get started with AOS: T-shirt-size your domain</li> <li>Getting started with AOS</li> <li>Log Analytics with AOS</li> <li>Getting started with Open Distro for Elasticsearch</li> <li>Know your data with Machine Learning</li> <li>Send CloudTrail Logs to AOS</li> <li>Searching DynamoDB Data with AOS</li> <li>Getting Started with Trace Analytics in AOS</li> </ul>"},{"location":"recipes/alerting/","title":"Alerting","text":"<p>This section has a selection of recipes for various alerting systems and scenarios.</p> <ul> <li>Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS</li> </ul>"},{"location":"recipes/amg/","title":"Amazon Managed Grafana","text":"<p>Amazon Managed Grafana is a fully managed service based on open  source Grafana, enabling you to analyze your metrics, logs, and traces without having to provision servers, configure and update software, or do the heavy  lifting involved in securing and scaling Grafana in production. You can create, explore, and share observability dashboards with your team, connecting to multiple data sources.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/amg/#basics","title":"Basics","text":"<ul> <li>Getting Started</li> <li>Using Terraform for automation</li> </ul>"},{"location":"recipes/amg/#authentication-and-access-control","title":"Authentication and Access Control","text":"<ul> <li>Direct SAML integration with identity providers</li> <li>Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO</li> <li>Integrating Google authentication via SAMLv2</li> <li>Setting up Amazon Managed Grafana cross-account data source using customer managed IAM roles</li> <li>Fine-grained access control in Amazon Managed Grafana using Grafana Teams</li> </ul>"},{"location":"recipes/amg/#data-sources-and-visualizations","title":"Data sources and Visualizations","text":"<ul> <li>Using Athena in Amazon Managed Grafana</li> <li>Using Redshift in Amazon Managed Grafana</li> <li>Viewing custom metrics from statsd with Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Setting up cross-account data source using customer managed IAM roles</li> </ul>"},{"location":"recipes/amg/#others","title":"Others","text":"<ul> <li>Monitoring hybrid environments</li> <li>Managing Grafana and Loki in a regulated multitenant environment</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Workshop for Getting Started</li> </ul>"},{"location":"recipes/amp/","title":"Amazon Managed Service for Prometheus","text":"<p>Amazon Managed Service for Prometheus (AMP) is a Prometheus-compatible monitoring service that makes it easy to monitor containerized applications at scale.  With AMP, you can use the Prometheus query language (PromQL) to monitor the performance of containerized workloads without having to manage the underlying  infrastructure required to manage the ingestion, storage, and querying of operational metrics.</p> <p>Check out the following recipes:</p> <ul> <li>Getting Started with AMP</li> <li>Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG</li> <li>Setting up cross-account ingestion into AMP</li> <li>Metrics collection from ECS using AMP</li> <li>Configuring Grafana Cloud Agent for AMP</li> <li>Set up cross-region metrics collection for AMP workspaces</li> <li>Best practices for migrating self-hosted Prometheus on EKS to AMP</li> <li>Workshop for Getting Started with AMP</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> <li>Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager</li> <li>Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Introducing Amazon EKS Observability Accelerator</li> <li>Installing the Prometheus mixin dashboards with AMP and Amazon Managed Grafana</li> <li>Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager</li> </ul>"},{"location":"recipes/anomaly-detection/","title":"Anomaly Detection","text":"<p>This section contains recipes for anomaly detection.</p> <ul> <li>Enabling Anomaly Detection for a CloudWatch Metric</li> </ul>"},{"location":"recipes/apprunner/","title":"AWS App Runner","text":"<p>AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner builds and deploys the web application automatically, load balances traffic with encryption, scales to meet your traffic needs, and makes it easy for your services to communicate with other AWS services and applications that run in a private Amazon VPC. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/apprunner/#general","title":"General","text":"<ul> <li>Container Day - Docker Con | How Developers can get to production web applications at scale easily</li> <li>AWS Blog | Centralized observability for AWS App Runner services</li> <li>AWS Blog | Observability for AWS App Runner VPC networking</li> <li>AWS Blog | Controlling and monitoring AWS App Runner applications with Amazon EventBridge</li> </ul>"},{"location":"recipes/apprunner/#logs","title":"Logs","text":"<ul> <li>Viewing App Runner logs streamed to CloudWatch Logs</li> </ul>"},{"location":"recipes/apprunner/#metrics","title":"Metrics","text":"<ul> <li>Viewing App Runner service metrics reported to CloudWatch</li> </ul>"},{"location":"recipes/apprunner/#traces","title":"Traces","text":"<ul> <li>Getting Started with AWS X-Ray tracing for App Runner using AWS Distro for OpenTelemetry</li> <li>Containers from the Couch | AWS App Runner X-Ray Integration</li> <li>AWS Blog | Tracing an AWS App Runner service using AWS X-Ray with OpenTelemetry</li> <li>AWS Blog | Enabling AWS X-Ray tracing for AWS App Runner service using AWS Copilot CLI</li> </ul>"},{"location":"recipes/cw/","title":"Amazon CloudWatch","text":"<p>Amazon CloudWatch (CW) is a monitoring and observability service built  for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch collects monitoring and operational data in the form of logs, metrics,  and events, providing you with a unified view of AWS resources, applications,  and services that run on AWS and on-premises servers.</p> <p>Check out the following recipes:</p> <ul> <li>Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS</li> <li>Implementing CloudWatch-centric observability for Kubernetes-native developers in EKS</li> <li>Create Canaries via CW Synthetics</li> <li>Cloudwatch Logs Insights for Quering Logs</li> <li>Lambda Insights</li> <li>Anomaly Detection via CloudWatch</li> <li>Metrics Alarms via CloudWatch</li> <li>Choosing container logging options to avoid backpressure</li> <li>Introducing CloudWatch Container Insights Prometheus Support with AWS Distro for OpenTelemetry on ECS and EKS</li> <li>Monitoring ECS containerized Applications and Microservices using CW Container Insights</li> <li>Monitoring EKS containerized Applications and Microservices using CW Container Insights</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> <li>Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch</li> <li>Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags</li> </ul>"},{"location":"recipes/dimensions/","title":"Dimensions","text":"<p>In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions.</p> <p></p> <p>What is a signal?</p> <p>When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply.</p> <p>Let's now have a look at each of the six dimensions one by one:</p>"},{"location":"recipes/dimensions/#destinations","title":"Destinations","text":"<p>In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure.</p> <p></p> <p>Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations.</p> <p>So, how do the signals arrive in the destinations? Glad you asked, it's \u2026</p>"},{"location":"recipes/dimensions/#agents","title":"Agents","text":"<p>How the signals are collected and routed to analytics. The signals can come  from two sources: either your application source code (see also the language section) or from things your application depends on,  such as state managed in datastores as well as infrastructure like VPCs (see also the infra &amp; data section).</p> <p>Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases.</p>"},{"location":"recipes/dimensions/#languages","title":"Languages","text":"<p>This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such  as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation. You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics.</p>"},{"location":"recipes/dimensions/#infrastructure-databases","title":"Infrastructure &amp; databases","text":"<p>With this dimension we mean any sort of application-external dependencies,  be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. </p> <p>Commonalities</p> <p>One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box.</p> <p>This dimension includes but is not limited to:</p> <ul> <li>AWS infrastructure, for example VPC flow logs.</li> <li>Secondary APIs such as Kubernetes control plane logs.</li> <li>Signals from datastores, such as or S3, RDS or SQS.</li> </ul>"},{"location":"recipes/dimensions/#compute-unit","title":"Compute unit","text":"<p>The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes.</p>"},{"location":"recipes/dimensions/#compute-engine","title":"Compute engine","text":"<p>This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.</p>"},{"location":"recipes/dynamodb/","title":"Amazon DynamoDB","text":"<p>Amazon DynamoDB is a key-value and document database that delivers  single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and  restore, and in-memory caching for internet-scale applications. </p> <p>Check out the following recipes:</p> <ul> <li>Monitoring Amazon DynamoDB for operational awareness</li> <li>Searching DynamoDB data with Amazon Elasticsearch Service</li> <li>DynamoDB Contributor Insights</li> </ul>"},{"location":"recipes/ecs/","title":"Amazon Elastic Container Service","text":"<p>Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale  containerized applications, deeply integrating with the rest of AWS.</p> <p>Check out the following recipes, grouped by compute engine:</p>"},{"location":"recipes/ecs/#general","title":"General","text":"<ul> <li>Deployment patterns for the AWS Distro for OpenTelemetry Collector with ECS</li> <li>Simplifying Amazon ECS monitoring set up with AWS Distro for OpenTelemetry</li> </ul>"},{"location":"recipes/ecs/#ecs-on-ec2","title":"ECS on EC2","text":""},{"location":"recipes/ecs/#logs","title":"Logs","text":"<ul> <li>Under the hood: FireLens for Amazon ECS Tasks</li> </ul>"},{"location":"recipes/ecs/#metrics","title":"Metrics","text":"<ul> <li>Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS</li> <li>Metrics collection from ECS using Amazon Managed Service for Prometheus</li> <li>Sending Envoy metrics from AWS App Mesh to Amazon CloudWatch</li> </ul>"},{"location":"recipes/ecs/#ecs-on-fargate","title":"ECS on Fargate","text":""},{"location":"recipes/ecs/#logs_1","title":"Logs","text":"<ul> <li>Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit</li> </ul>"},{"location":"recipes/eks/","title":"Amazon Elastic Kubernetes Service","text":"<p>Amazon Elastic Kubernetes Service (EKS) gives you the flexibility to  start, run, and scale Kubernetes applications in the AWS Cloud or on-premises. </p> <p>Check out the following recipes, grouped by compute engine:</p>"},{"location":"recipes/eks/#eks-on-ec2","title":"EKS on EC2","text":""},{"location":"recipes/eks/#logs","title":"Logs","text":"<ul> <li>Fluent Bit Integration in CloudWatch Container Insights for EKS</li> <li>Logging with EFK Stack</li> <li>Sample logging architectures for Fluent Bit and FluentD on EKS</li> </ul>"},{"location":"recipes/eks/#metrics","title":"Metrics","text":"<ul> <li>Getting Started with Amazon Managed Service for Prometheus</li> <li>Using ADOT in EKS on EC2 to ingest metrics to AMP and visualize in AMG</li> <li>Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus</li> <li>Monitoring cluster using Prometheus and Grafana</li> <li>Monitoring with Managed Prometheus and Managed Grafana</li> <li>CloudWatch Container Insights</li> <li>Set up cross-region metrics collection for AMP workspaces</li> <li>Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus</li> <li>Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana</li> <li>Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> </ul>"},{"location":"recipes/eks/#traces","title":"Traces","text":"<ul> <li>Migrating X-Ray tracing to AWS Distro for OpenTelemetry</li> <li>Tracing with X-Ray</li> </ul>"},{"location":"recipes/eks/#eks-on-fargate","title":"EKS on Fargate","text":""},{"location":"recipes/eks/#logs_1","title":"Logs","text":"<ul> <li>Fluent Bit for Amazon EKS on AWS Fargate is here</li> <li>Sample logging architectures for Fluent Bit and FluentD on EKS</li> </ul>"},{"location":"recipes/eks/#metrics_1","title":"Metrics","text":"<ul> <li>Using ADOT in EKS on Fargate to ingest metrics to AMP and visualize in AMG</li> <li>CloudWatch Container Insights</li> <li>Set up cross-region metrics collection for AMP workspaces</li> </ul>"},{"location":"recipes/eks/#traces_1","title":"Traces","text":"<ul> <li>Using ADOT in EKS on Fargate with AWS X-Ray</li> <li>Tracing with X-Ray</li> </ul>"},{"location":"recipes/infra/","title":"Infrastructure &amp; Databases","text":""},{"location":"recipes/infra/#networking","title":"Networking","text":"<ul> <li>Monitor your Application Load Balancers</li> <li>Monitor your Network Load Balancers</li> <li>VPC Flow Logs</li> <li>VPC Flow logs analysis using Amazon Elasticsearch Service</li> </ul>"},{"location":"recipes/infra/#compute","title":"Compute","text":"<ul> <li>Amazon EKS control plane logging</li> <li>AWS Lambda monitoring and observability</li> </ul>"},{"location":"recipes/infra/#databases-storage-and-queues","title":"Databases, storage and queues","text":"<ul> <li>Amazon Relational Database Service</li> <li>Amazon DynamoDB</li> <li>Amazon Managed Streaming for Apache Kafka</li> <li>Logging and monitoring in Amazon S3</li> <li>Amazon SQS and AWS X-Ray</li> </ul>"},{"location":"recipes/infra/#others","title":"Others","text":"<ul> <li>Prometheus exporters</li> </ul>"},{"location":"recipes/java/","title":"Java","text":"<ul> <li>StatsD and Java Support in AWS Distro for OpenTelemetry</li> </ul>"},{"location":"recipes/lambda/","title":"AWS Lambda","text":"<p>AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster  scaling logic, maintaining event integrations, or managing runtimes.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/lambda/#logs","title":"Logs","text":"<ul> <li>Deploy and Monitor a Serverless Application</li> </ul>"},{"location":"recipes/lambda/#metrics","title":"Metrics","text":"<ul> <li>Introducing CloudWatch Lambda Insights</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> </ul>"},{"location":"recipes/lambda/#traces","title":"Traces","text":"<ul> <li>Auto-instrumenting a Python application with an AWS Distro for OpenTelemetry Lambda layer</li> <li>Tracing AWS Lambda functions in AWS X-Ray with OpenTelemetry</li> </ul>"},{"location":"recipes/msk/","title":"Amazon Managed Streaming for Apache Kafka","text":"<p>Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service that makes it  easy for you to build and run applications that use Apache Kafka to process  streaming data. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition,  Amazon MSK secures your Apache Kafka cluster by encrypting data at rest.</p> <p>Check out the following recipes:</p> <ul> <li>Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus</li> </ul>"},{"location":"recipes/nodejs/","title":"Node.js","text":"<ul> <li>NodeJS library to generate embedded CloudWatch metrics</li> </ul>"},{"location":"recipes/rds/","title":"Amazon Relational Database Service","text":"<p>Amazon Relational Database Service (RDS) makes it easy to set up,  operate, and scale a relational database in the cloud. It provides cost-efficient  and resizable capacity while automating time-consuming administration tasks such  as hardware provisioning, database setup, patching and backups.</p> <p>Check out the following recipes:</p> <ul> <li>Build proactive database monitoring for RDS with CloudWatch Logs, Lambda, and SNS</li> <li>Monitor RDS for PostgreSQL and Aurora for PostgreSQL database log errors and set up notifications using CloudWatch</li> <li>Logging and monitoring in Amazon RDS</li> <li>Performance Insights metrics published to CloudWatch</li> </ul>"},{"location":"recipes/telemetry/","title":"Telemetry","text":"<p>Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed:</p> <p></p> <p>Success</p> <p>See the Data types section for a detailed breakdown of the best practices for each type of telemetry.</p> <p>Let's further dive into the concepts introduced in above figure.</p>"},{"location":"recipes/telemetry/#sources","title":"Sources","text":"<p>We consider sources as something where signals come from. There are two types of sources:</p> <ol> <li>Things under your control, that is, the application source code, via instrumentation.</li> <li>Everything else you may use, such as managed services, not under your (direct) control.    These types of sources are typically provided by AWS, exposing signals via an API.</li> </ol>"},{"location":"recipes/telemetry/#agents","title":"Agents","text":"<p>In order to transpor signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals  from the sources and, typically via configuration, determine where signals  shoud go, optionally supporting filtering and aggregation.</p> <p>Agents? Routing? Shipping? Ingesting?</p> <p>There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly  different things, we will use them here interchangeably. Canonically,  we will refer to those intermediary transport components as agents.</p>"},{"location":"recipes/telemetry/#destinations","title":"Destinations","text":"<p>Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.</p>"},{"location":"recipes/troubleshooting/","title":"Troubleshooting","text":"<p>We include troubleshooting recipes for various situations and dimensions in this section.</p> <ul> <li>Troubleshooting performance bottleneck in DynamoDB</li> </ul>"},{"location":"recipes/workshops/","title":"Workshops","text":"<p>This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling.</p> <ul> <li>One Observability Workshop</li> <li>EKS Workshop</li> <li>ECS Workshop</li> <li>App Runner Workshop</li> </ul>"},{"location":"recipes/recipes/amg-athena-plugin/","title":"Using Athena in Amazon Managed Grafana","text":"<p>In this recipe we show you how to use Amazon Athena\u2014a serverless,  interactive query service allowing you to analyze data in Amazon S3 using  standard SQL\u2014in Amazon Managed Grafana. This integration is enabled by the Athena data source for Grafana, an open source plugin available for you to use in any DIY Grafana instance as well as  pre-installed in Amazon Managed Grafana.</p> <p>Note</p> <p>This guide will take approximately 20 minutes to complete.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You have access to Amazon Athena from your account.</li> </ul>"},{"location":"recipes/recipes/amg-athena-plugin/#infrastructure","title":"Infrastructure","text":"<p>Let's first set up the necessary infrastructure.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#set-up-amazon-athena","title":"Set up Amazon Athena","text":"<p>We want to see how to use Athena in two different scenarios: one scenario around geographical data along with the Geomap plugin, and one in a security-relevant scenario around VPC flow logs.</p> <p>First, let's make sure Athena is set up and the datasets are loaded.</p> <p>Warning</p> <p>You have to use the Amazon Athena console to execute these queries. Grafana in general has read-only access to the data sources, so can not be used to create or update data.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#load-geographical-data","title":"Load geographical data","text":"<p>In this first use case we use a dataset from the Registry of Open Data on AWS. More specifically, we will use OpenStreetMap (OSM) to demonstrate the usage of the Athena plugin for a geographical data motivated use case. For that to work, we need to first get the OSM data into Athena.</p> <p>So, first off, create a new database in Athena. Go to the Athena console and there use the following three  SQL queries to import the OSM data into the database.</p> <p>Query 1:</p> <pre><code>CREATE EXTERNAL TABLE planet (\nid BIGINT,\ntype STRING,\ntags MAP&lt;STRING,STRING&gt;,\nlat DECIMAL(9,7),\nlon DECIMAL(10,7),\nnds ARRAY&lt;STRUCT&lt;ref: BIGINT&gt;&gt;,\nmembers ARRAY&lt;STRUCT&lt;type: STRING, ref: BIGINT, role: STRING&gt;&gt;,\nchangeset BIGINT,\ntimestamp TIMESTAMP,\nuid BIGINT,\nuser STRING,\nversion BIGINT\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/planet/';\n</code></pre> <p>Query 2:</p> <pre><code>CREATE EXTERNAL TABLE planet_history (\nid BIGINT,\ntype STRING,\ntags MAP&lt;STRING,STRING&gt;,\nlat DECIMAL(9,7),\nlon DECIMAL(10,7),\nnds ARRAY&lt;STRUCT&lt;ref: BIGINT&gt;&gt;,\nmembers ARRAY&lt;STRUCT&lt;type: STRING, ref: BIGINT, role: STRING&gt;&gt;,\nchangeset BIGINT,\ntimestamp TIMESTAMP,\nuid BIGINT,\nuser STRING,\nversion BIGINT,\nvisible BOOLEAN\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/planet-history/';\n</code></pre> <p>Query 3:</p> <pre><code>CREATE EXTERNAL TABLE changesets (\nid BIGINT,\ntags MAP&lt;STRING,STRING&gt;,\ncreated_at TIMESTAMP,\nopen BOOLEAN,\nclosed_at TIMESTAMP,\ncomments_count BIGINT,\nmin_lat DECIMAL(9,7),\nmax_lat DECIMAL(9,7),\nmin_lon DECIMAL(10,7),\nmax_lon DECIMAL(10,7),\nnum_changes BIGINT,\nuid BIGINT,\nuser STRING\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/changesets/';\n</code></pre>"},{"location":"recipes/recipes/amg-athena-plugin/#load-vpc-flow-logs-data","title":"Load VPC flow logs data","text":"<p>The second use case is a security-motivated one: analyzing network traffic using VPC Flow Logs.</p> <p>First, we need to tell EC2 to generate VPC Flow Logs for us. So, if you have  not done this already, you go ahead now and create VPC flow logs  either on the network interfaces level, subnet level, or VPC level.</p> <p>Note</p> <p>To improve query performance and minimize the storage footprint, we store the VPC flow logs in Parquet, a columnar storage format that supports nested data.</p> <p>For our setup it doesn't matter what option you choose (network interfaces,  subnet, or VPC), as long as you publish them to an S3 bucket in Parquet format as shown below:</p> <p></p> <p>Now, again via the Athena console, create the table for the VPC flow logs data in the same database you imported the OSM data, or create a new one, if you prefer to do so.</p> <p>Use the following SQL query and make sure that you're replacing <code>VPC_FLOW_LOGS_LOCATION_IN_S3</code> with your own bucket/folder:</p> <pre><code>CREATE EXTERNAL TABLE vpclogs (\n`version` int, `account_id` string, `interface_id` string, `srcaddr` string, `dstaddr` string, `srcport` int, `dstport` int, `protocol` bigint, `packets` bigint, `bytes` bigint, `start` bigint, `end` bigint, `action` string, `log_status` string, `vpc_id` string, `subnet_id` string, `instance_id` string, `tcp_flags` int, `type` string, `pkt_srcaddr` string, `pkt_dstaddr` string, `region` string, `az_id` string, `sublocation_type` string, `sublocation_id` string, `pkt_src_aws_service` string, `pkt_dst_aws_service` string, `flow_direction` string, `traffic_path` int\n)\nSTORED AS PARQUET\nLOCATION 'VPC_FLOW_LOGS_LOCATION_IN_S3'\n</code></pre> <p>For example, <code>VPC_FLOW_LOGS_LOCATION_IN_S3</code> could look something like the following if you're using the S3 bucket <code>allmyflowlogs</code>:</p> <pre><code>s3://allmyflowlogs/AWSLogs/12345678901/vpcflowlogs/eu-west-1/2021/\n</code></pre> <p>Now that the datasets are available in Athena, let's move on to Grafana.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#set-up-grafana","title":"Set up Grafana","text":"<p>We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace, for example by using the Getting Started guide, or use an existing one.</p> <p>Warning</p> <p>To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the  IAM policies necessary to read the Athena resources. Further, note the following:</p> <ol> <li>The Athena workgroup you plan to use needs to be tagged with the key  <code>GrafanaDataSource</code> and value <code>true</code> for the service managed permissions to be permitted to use the workgroup.</li> <li>The service-managed IAM policy only grants access to query result buckets  that start with <code>grafana-athena-query-results-</code>, so for any other bucket you MUST add permissions manually.</li> <li>You have to add the <code>s3:Get*</code> and <code>s3:List*</code> permissions for the underlying data source  being queried manually.</li> </ol> <p>To set up the Athena data source, use the left-hand toolbar and choose the  lower AWS icon and then choose \"Athena\". Select your default region you want  the plugin to discover the Athena data source to use, and then select the  accounts that you want, and finally choose \"Add data source\".</p> <p>Alternatively, you can manually add and configure the Athena data source by  following these steps:</p> <ol> <li>Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\".</li> <li>Search for \"Athena\".</li> <li>[OPTIONAL] Configure the authentication provider (recommended: workspace IAM    role).</li> <li>Select your targeted Athena data source, database, and workgroup.</li> <li>If your workgroup doesn't have an output location configured already,    specify the S3 bucket and folder to use for query results. Note that the    bucket has to start with <code>grafana-athena-query-results-</code> if you want to    benefit from the service-managed policy.</li> <li>Click \"Save &amp; test\".</li> </ol> <p>You should see something like the following:</p> <p></p>"},{"location":"recipes/recipes/amg-athena-plugin/#usage","title":"Usage","text":"<p>And now let's look at how to use our Athena datasets from Grafana.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#use-geographical-data","title":"Use geographical data","text":"<p>The OpenStreetMap (OSM) data in Athena can answer a number of questions, such as \"where are certain amenities\". Let's see that in action.</p> <p>For example, a SQL query against the OSM dataset to list places that offer food in the Las Vegas region is as follows:</p> <pre><code>SELECT tags['amenity'] AS amenity,\ntags['name'] AS name,\ntags['website'] AS website,\nlat, lon\nFROM planet\nWHERE type = 'node'\nAND tags['amenity'] IN ('bar', 'pub', 'fast_food', 'restaurant')\nAND lon BETWEEN -115.5 AND -114.5\nAND lat BETWEEN 36.1 AND 36.3\nLIMIT 500;\n</code></pre> <p>Info</p> <p>The Las Vegas region in above query is defined as everything with a latitude  between <code>36.1</code> and <code>36.3</code> as well as a longitude between <code>-115.5</code> and <code>-114.5</code>. You could turn that into a set of variables (one for each corner) and make the Geomap plugin adaptable to other regions.</p> <p>To visualize the OSM data using above query, you can import an example dashboard,  available via osm-sample-dashboard.json that looks as follows:</p> <p></p> <p>Note</p> <p>In above screen shot we use the Geomap visualization (in the left panel) to plot the data points.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#use-vpc-flow-logs-data","title":"Use VPC flow logs data","text":"<p>To analyze the VPC flow log data, detecting SSH and RDP traffic, use the following SQL queries.</p> <p>Getting a tabular overview on SSH/RDP traffic:</p> <pre><code>SELECT\nsrcaddr, dstaddr, account_id, action, protocol, bytes, log_status\nFROM vpclogs\nWHERE\nsrcport in (22, 3389)\nOR\ndstport IN (22, 3389)\nORDER BY start ASC;\n</code></pre> <p>Getting a time series view on bytes accepted and rejected:</p> <pre><code>SELECT\nfrom_unixtime(start), sum(bytes), action\nFROM vpclogs\nWHERE\nsrcport in (22,3389)\nOR\ndstport IN (22, 3389)\nGROUP BY start, action\nORDER BY start ASC;\n</code></pre> <p>Tip</p> <p>If you want to limit the amount of data queried in Athena, consider using the <code>$__timeFilter</code> macro.</p> <p>To visualize the VPC flow log data, you can import an example dashboard,  available via vpcfl-sample-dashboard.json that looks as follows:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use Athena from Grafana!</p>"},{"location":"recipes/recipes/amg-athena-plugin/#cleanup","title":"Cleanup","text":"<p>Remove the OSM data from the Athena database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amg-automation-tf/","title":"Using Terraform for Amazon Managed Grafana automation","text":"<p>In this recipe we show you how use Terraform to automate Amazon Managed Grafana,  for example to add datasources or dashboards consistently across a number of workspaces.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/amg-automation-tf/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS command line is installed and configured in your local environment.</li> <li>You have the Terraform command line installed in your local environment.</li> <li>You have an Amazon Managed Service for Prometheus workspace ready to use.</li> <li>You have an Amazon Managed Grafana workspace ready to use.</li> </ul>"},{"location":"recipes/recipes/amg-automation-tf/#set-up-amazon-managed-grafana","title":"Set up Amazon Managed Grafana","text":"<p>In order for Terraform to authenticate against Grafana, we are  using an API Key, which acts as a kind of password. </p> <p>Info</p> <p>The API key is an RFC 6750 HTTP Bearer header with a 51 character long alpha-numeric value authenticating the caller with every request against the Grafana API.</p> <p>So, before we can set up the Terraform manifest, we first need to create an API key. You do this via the Grafana UI as follows.</p> <p>First, select from the left-hand side menu in the <code>Configuration</code> section the <code>API keys</code> menu item:</p> <p></p> <p>Now create a new API key, give it a name that makes sense for your task at hand, assign it <code>Admin</code> role and set the duration time to, for example, one day:</p> <p></p> <p>Note</p> <p>The API key is valid for a limited time, in AMG you can use values up to 30 days.</p> <p>Once you hit the <code>Add</code> button you should see a pop-up dialog that contains the API key:</p> <p></p> <p>Warning</p> <p>This is the only time you will see the API key, so store it from here in a safe place, we will need it in the Terraform manifest later.</p> <p>With this we've set up everything we need in Amazon Managed Grafana in order to use Terraform for automation, so let's move on to this step.</p>"},{"location":"recipes/recipes/amg-automation-tf/#automation-with-terraform","title":"Automation with Terraform","text":""},{"location":"recipes/recipes/amg-automation-tf/#preparing-terraform","title":"Preparing Terraform","text":"<p>For Terraform to be able to interact with Grafana, we're using the official Grafana provider in version 1.13.3 or above.</p> <p>In the following, we want to automate the creation of a data source, in our case we want to add a Prometheus data source, to be exact, an AMP workspace.</p> <p>First, create a file called <code>main.tf</code> with the following content:</p> <pre><code>terraform {\nrequired_providers {\ngrafana = {\nsource  = \"grafana/grafana\"\nversion = \"&gt;= 1.13.3\"\n}\n}\n}\n\nprovider \"grafana\" {\nurl  = \"INSERT YOUR GRAFANA WORKSPACE URL HERE\"\nauth = \"INSERT YOUR API KEY HERE\"\n}\n\nresource \"grafana_data_source\" \"prometheus\" {\ntype          = \"prometheus\"\nname          = \"amp\"\nis_default    = true\nurl           = \"INSERT YOUR AMP WORKSPACE URL HERE \"\njson_data {\nhttp_method     = \"POST\"\nsigv4_auth      = true\nsigv4_auth_type = \"workspace-iam-role\"\nsigv4_region    = \"eu-west-1\"\n}\n}\n</code></pre> <p>In above file you need to insert three values that depend on your environment.</p> <p>In the Grafana provider section:</p> <ul> <li><code>url</code> \u2026 the Grafana workspace URL which looks something like the following:       <code>https://xxxxxxxx.grafana-workspace.eu-west-1.amazonaws.com</code>.</li> <li><code>auth</code> \u2026 the API key you have created in the previous step.</li> </ul> <p>In the Prometheus resource section, insert the <code>url</code> which is the AMP  workspace URL in the form of  <code>https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx</code>.</p> <p>Note</p> <p>If you're using Amazon Managed Grafana in a different region than the one shown in the file, you will have to, in addition to above, also set the <code>sigv4_region</code> to your region.</p> <p>To wrap up the preparation phase, let's now initialize Terraform:</p> <pre><code>$ terraform init\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding grafana/grafana versions matching \"&gt;= 1.13.3\"...\n- Installing grafana/grafana v1.13.3...\n- Installed grafana/grafana v1.13.3 (signed by a HashiCorp partner, key ID 570AA42029AE241A)\n\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>With that, we're all set and can use Terraform to automate the data source creation as explained in the following.</p>"},{"location":"recipes/recipes/amg-automation-tf/#using-terraform","title":"Using Terraform","text":"<p>Usually, you would first have a look what Terraform's plan is, like so:</p> <pre><code>$ terraform plan\n\nTerraform used the selected providers to generate the following execution plan. \nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # grafana_data_source.prometheus will be created\n+ resource \"grafana_data_source\" \"prometheus\" {\n+ access_mode        = \"proxy\"\n+ basic_auth_enabled = false\n+ id                 = (known after apply)\n+ is_default         = true\n+ name               = \"amp\"\n+ type               = \"prometheus\"\n+ url                = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxx/\"\n\n+ json_data {\n+ http_method     = \"POST\"\n+ sigv4_auth      = true\n+ sigv4_auth_type = \"workspace-iam-role\"\n+ sigv4_region    = \"eu-west-1\"\n}\n}\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.\n</code></pre> <p>If you're happy with what you see there, you can apply the plan:</p> <pre><code>$ terraform apply\n\nTerraform used the selected providers to generate the following execution plan. \nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # grafana_data_source.prometheus will be created\n+ resource \"grafana_data_source\" \"prometheus\" {\n+ access_mode        = \"proxy\"\n+ basic_auth_enabled = false\n+ id                 = (known after apply)\n+ is_default         = true\n+ name               = \"amp\"\n+ type               = \"prometheus\"\n+ url                = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx/\"\n\n+ json_data {\n+ http_method     = \"POST\"\n+ sigv4_auth      = true\n+ sigv4_auth_type = \"workspace-iam-role\"\n+ sigv4_region    = \"eu-west-1\"\n}\n}\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\ngrafana_data_source.prometheus: Creating...\ngrafana_data_source.prometheus: Creation complete after 1s [id=10]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n</code></pre> <p>When you now go to the data source list in Grafana you should see something like the following:</p> <p></p> <p>To verify if your newly created data source works, you can hit the blue <code>Save &amp; test</code> button at the bottom and you should see a <code>Data source is working</code> confirmation message as a result here.</p> <p>You can use Terraform also to automate other things, for example, the Grafana  provider supports managing folders and dashboards.</p> <p>Let's say you want to create a folder to organize your dashboards, for example:</p> <pre><code>resource \"grafana_folder\" \"examplefolder\" {\n  title = \"devops\"\n}\n</code></pre> <p>Further, say you have a dashboard called <code>example-dashboard.json</code>, and you want to create it in the folder from above, then you would use the following snippet:</p> <pre><code>resource \"grafana_dashboard\" \"exampledashboard\" {\n  folder = grafana_folder.examplefolder.id\n  config_json = file(\"example-dashboard.json\")\n}\n</code></pre> <p>Terraform is a powerful tool for automation and you can use it as shown here to manage your Grafana resources. </p> <p>Note</p> <p>Keep in mind, though, that the state in Terraform is, by default, managed locally. This means, if you plan to collaboratively work with Terraform, you need to pick one of the options available that allow you to share the state across a team.</p>"},{"location":"recipes/recipes/amg-automation-tf/#cleanup","title":"Cleanup","text":"<p>Remove the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/","title":"Configure Google Workspaces authentication with Amazon Managed Grafana using SAML","text":"<p>In this guide, we will walk through how you can setup Google Workspaces as an identity provider (IdP) for Amazon Managed Grafana using SAML v2.0 protocol.</p> <p>In order to follow this guide you need to create a paid Google Workspaces  account in addition to having an Amazon Managed Grafana workspace created.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/#create-amazon-managed-grafana-workspace","title":"Create Amazon Managed Grafana workspace","text":"<p>Log into the Amazon Managed Grafana console and click Create workspace. In the following screen, provide a workspace name as shown below. Then click Next:</p> <p></p> <p>In the Configure settings page, select Security Assertion Markup Language (SAML)  option so you can configure a SAML based Identity Provider for users to log in:</p> <p></p> <p>Select the data sources you want to choose and click Next: </p> <p>Click on Create workspace button in the Review and create screen: </p> <p>This will create a new Amazon Managed Grafana workspace as shown below:</p> <p></p>"},{"location":"recipes/recipes/amg-google-auth-saml/#configure-google-workspaces","title":"Configure Google Workspaces","text":"<p>Login to Google Workspaces with Super Admin permissions and go to Web and mobile apps under Apps section. There, click on Add App  and select Add custom SAML app. Now give the app a name as shown below.  Click CONTINUE.:</p> <p></p> <p>On the next screen, click on DOWNLOAD METADATA button to download the SAML metadata file. Click CONTINUE.</p> <p></p> <p>On the next screen, you will see the ACS URL, Entity ID and Start URL fields. You can get the values for these fields from the Amazon Managed Grafana console. </p> <p>Select EMAIL from the drop down in the Name ID format field and select Basic Information &gt; Primary email in the Name ID field.</p> <p>Click CONTINUE. </p> <p></p> <p>In the Attribute mapping screen, make the mapping between Google Directory attributes and App attributes as shown in the screenshot below</p> <p></p> <p>For users logging in through Google authentication to have Admin privileges in Amazon Managed Grafana, set the Department field\u2019s value as monitoring. You can choose any field and any value for this. Whatever you choose to use on the Google Workspaces side, make sure you make the mapping on Amazon Managed Grafana SAML settings to reflect that.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/#upload-saml-metadata-into-amazon-managed-grafana","title":"Upload SAML metadata into Amazon Managed Grafana","text":"<p>Now in the Amazon Managed Grafana console, click Upload or copy/paste option and select Choose file button to upload the SAML metadata file downloaded from Google Workspaces, earlier. </p> <p>In the Assertion mapping section, type in Department in the Assertion attribute role field and monitoring in the Admin role values field.  This will allow users logging in with Department as monitoring to  have Admin privileges in Grafana so they can perform administrator duties such as creating dashboards and datasources.</p> <p>Set values under Additional settings - optional section as shown in the  screenshot below. Click on Save SAML configuration:</p> <p></p> <p>Now Amazon Managed Grafana is set up to authenticate users using Google Workspaces. </p> <p>When users login, they will be redirected to the Google login page like so:</p> <p></p> <p>After entering their credentials, they will be logged into Grafana as shown in the screenshot below. </p> <p>As you can see, the user was able to successfully login to Grafana using Google Workspaces authentication.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/","title":"Using Redshift in Amazon Managed Grafana","text":"<p>In this recipe we show you how to use Amazon Redshift\u2014a petabyte-scale data  warehouse service using standard SQL\u2014in Amazon Managed Grafana. This integration is enabled by the Redshift data source for Grafana, an open source plugin available for you to use in any DIY Grafana instance as well as  pre-installed in Amazon Managed Grafana.</p> <p>Note</p> <p>This guide will take approximately 10 minutes to complete.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have admin access to Amazon Redshift from your account.</li> <li>Tag your Amazon Redshift cluster with <code>GrafanaDataSource: true</code>. </li> <li>In order to benefit from the service-managed policies, create the database     credentials in one of the following ways:<ol> <li>If you want to use the default mechanism, that is, the temporary credentials  option, to authenticate against the Redshift database, you must create a database  user named <code>redshift_data_api_user</code>.</li> <li>If you want to use the credentials from Secrets Manager, you must tag the  secret with <code>RedshiftQueryOwner: true</code>.</li> </ol> </li> </ol> <p>Tip</p> <p>For more information on how to work with the service-managed or custom policies, see the examples in the Amazon Managed Grafana docs.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#infrastructure","title":"Infrastructure","text":"<p>We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace, for example by using the Getting Started guide, or use an existing one.</p> <p>Note</p> <p>To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the  IAM policies necessary to read the Athena resources.</p> <p>To set up the Athena data source, use the left-hand toolbar and choose the  lower AWS icon and then choose \"Redshift\". Select your default region you want  the plugin to discover the Redshift data source to use, and then select the  accounts that you want, and finally choose \"Add data source\".</p> <p>Alternatively, you can manually add and configure the Redshift data source by  following these steps:</p> <ol> <li>Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\".</li> <li>Search for \"Redshift\".</li> <li>[OPTIONAL] Configure the authentication provider (recommended: workspace IAM    role).</li> <li>Provide the \"Cluster Identifier\", \"Database\", and \"Database User\" values.</li> <li>Click \"Save &amp; test\".</li> </ol> <p>You should see something like the following:</p> <p></p>"},{"location":"recipes/recipes/amg-redshift-plugin/#usage","title":"Usage","text":"<p>We will be using the Redshift Advance Monitoring setup. Since all is available out of the box, there's nothing else to configure at this point.</p> <p>You can import the Redshift monitoring dashboard, included in the Redshift plugin. Once imported you should see something like this:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use Redshift from Grafana!</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#cleanup","title":"Cleanup","text":"<p>Remove the Redshift database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/","title":"Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager","text":"<p>In this recipe, we will demonstrate how you can use Terraform to provision Amazon Managed Service for Prometheus and configure rules management and alert manager to send notification to a SNS topic if a certain condition is met.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#prerequisites","title":"Prerequisites","text":"<p>You will need the following to complete the setup:</p> <ul> <li>Amazon EKS cluster</li> <li>AWS CLI version 2</li> <li>Terraform CLI</li> <li>AWS Distro for OpenTelemetry(ADOT)</li> <li>eksctl</li> <li>kubectl</li> <li>jq</li> <li>helm</li> <li>SNS topic</li> <li>awscurl</li> </ul> <p>In the recipe, we will use a sample application in order to demonstrate the metric scraping using ADOT and remote write the metrics to the Amazon Managed Service for Prometheus workspace. Fork and clone the sample app from the repository at aws-otel-community.</p> <p>This Prometheus sample app generates all 4 Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint</p> <p>A health check endpoint also exists at /</p> <p>The following is a list of optional command line flags for configuration:</p> <p>listen_address: (default = 0.0.0.0:8080) defines the address and port that the sample app is exposed to. This is primarily to conform with the test framework requirements.</p> <p>metric_count: (default=1) the amount of each type of metric to generate. The same amount of metrics is always generated per metric type.</p> <p>label_count: (default=1) the amount of labels per metric to generate.</p> <p>datapoint_count: (default=1) the number of data-points per metric to generate.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#enabling-metric-collection-using-aws-distro-for-opentelemetry","title":"Enabling Metric collection using AWS Distro for Opentelemetry","text":"<ol> <li>Fork and clone the sample app from the repository at aws-otel-community. Then run the following commands.</li> </ol> <pre><code>cd ./sample-apps/prometheus\ndocker build . -t prometheus-sample-app:latest\n</code></pre> <ol> <li>Push this image to a registry such as Amazon ECR. You can use the following command to create a new ECR repository in your account. Make sure to set  as well. <pre><code>aws ecr create-repository \\\n--repository-name prometheus-sample-app \\\n--image-scanning-configuration scanOnPush=true \\\n--region &lt;YOUR_REGION&gt;\n</code></pre> <ol> <li>Deploy the sample app in the cluster by copying this Kubernetes configuration and applying it. Change the image to the image that you just pushed by replacing <code>PUBLIC_SAMPLE_APP_IMAGE</code> in the prometheus-sample-app.yaml file.</li> </ol> <pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-sample-app.yaml -o prometheus-sample-app.yaml\nkubectl apply -f prometheus-sample-app.yaml\n</code></pre> <ol> <li>Start a default instance of the ADOT Collector. To do so, first enter the following command to pull the Kubernetes configuration for ADOT Collector.</li> </ol> <pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-daemonset.yaml -o prometheus-daemonset.yaml\n</code></pre> <p>Then edit the template file, substituting the remote_write endpoint for your Amazon Managed Service for Prometheus workspace for <code>YOUR_ENDPOINT</code> and your Region for <code>YOUR_REGION</code>.  Use the remote_write endpoint that is displayed in the Amazon Managed Service for Prometheus console when you look at your workspace details. You'll also need to change <code>YOUR_ACCOUNT_ID</code> in the service account section of the Kubernetes configuration to your AWS account ID.</p> <p>In this recipe, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code> to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. 5. Enter the following command to deploy the ADOT collector.</p> <pre><code>kubectl apply -f eks-prometheus-daemonset.yaml\n</code></pre>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#configure-workspace-with-terraform","title":"Configure workspace with Terraform","text":"<p>Now, we will  provision a Amazon Managed Service for Prometheus workspace and will define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in <code>expr</code>) holds true for a specified time period (<code>for</code>). Code in the Terraform language is stored in plain text files with the .tf file extension. There is also a JSON-based variant of the language that is named with the .tf.json file extension.</p> <p>We will now use the main.tf to deploy the resources using terraform. Before running the terraform command, we will export the <code>region</code> and <code>sns_topic</code> variable.</p> <pre><code>export TF_VAR_region=&lt;your region&gt;\nexport TF_VAR_sns_topic=&lt;ARN of the SNS topic used by the SNS receiver&gt;\n</code></pre> <p>Now, we will execute the below commands to provision the workspace: </p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <p>Once the above steps are complete, verify the setup end-to-end by using awscurl and query the endpoint. Ensure the <code>WORKSPACE_ID</code> variable is replaced with the appropriate Amazon Managed Service for Prometheus workspace id.</p> <p>On running the below command, look for the metric \u201cmetric:recording_rule\u201d, and, if you successfully find the metric, then you\u2019ve successfully created a recording rule:</p> <pre><code>awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/api/v1/rules  --service=\"aps\"\n</code></pre> <p>Sample Output:</p> <pre><code>\"status\":\"success\",\"data\":{\"groups\":[{\"name\":\"alert-test\",\"file\":\"rules\",\"rules\":[{\"state\":\"firing\",\"name\":\"metric:alerting_rule\",\"query\":\"rate(adot_test_counter0[5m]) \\u003e 5\",\"duration\":0,\"labels\":{},\"annotations\":{},\"alerts\":[{\"labels\":{\"alertname\":\"metric:alerting_rule\"},\"annotations\":{},\"state\":\"firing\",\"activeAt\":\"2021-09-16T13:20:35.9664022Z\",\"value\":\"6.96890019778219e+01\"}],\"health\":\"ok\",\"lastError\":\"\",\"type\":\"alerting\",\"lastEvaluation\":\"2021-09-16T18:41:35.967122005Z\",\"evaluationTime\":0.018121408}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:41:35.967104769Z\",\"evaluationTime\":0.018142997},{\"name\":\"test\",\"file\":\"rules\",\"rules\":[{\"name\":\"metric:recording_rule\",\"query\":\"rate(adot_test_counter0[5m])\",\"labels\":{},\"health\":\"ok\",\"lastError\":\"\",\"type\":\"recording\",\"lastEvaluation\":\"2021-09-16T18:40:44.650001548Z\",\"evaluationTime\":0.018381387}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:40:44.649986468Z\",\"evaluationTime\":0.018400463}]},\"errorType\":\"\",\"error\":\"\"}\n</code></pre> <p>We can further query the alertmanager endpoint to confirm the same</p> <pre><code>awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/alertmanager/api/v2/alerts --service=\"aps\" -H \"Content-Type: application/json\"\n</code></pre> <p>Sample Output:</p> <pre><code>[{\"annotations\":{},\"endsAt\":\"2021-09-16T18:48:35.966Z\",\"fingerprint\":\"114212a24ca97549\",\"receivers\":[{\"name\":\"default\"}],\"startsAt\":\"2021-09-16T13:20:35.966Z\",\"status\":{\"inhibitedBy\":[],\"silencedBy\":[],\"state\":\"active\"},\"updatedAt\":\"2021-09-16T18:44:35.984Z\",\"generatorURL\":\"/graph?g0.expr=sum%28rate%28envoy_http_downstream_rq_time_bucket%5B1m%5D%29%29+%3E+5\\u0026g0.tab=1\",\"labels\":{\"alertname\":\"metric:alerting_rule\"}}]\n</code></pre> <p>This confirms the alert was triggered and sent to SNS via the SNS receiver</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#clean-up","title":"Clean up","text":"<p>Run the following command to terminate the Amazon Managed Service for Prometheus workspace. Make sure you delete the EKS Cluster that was created as well:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"recipes/recipes/amp-mixin-dashboards/","title":"Adding kubernetes-mixin dashboards to Managed Grafana","text":"<p>Even as a managed service, EKS still exposes many of the metrics from the Kubernetes control plane. The Prometheus community has put together a series of dashboards to review and investigate these metrics. This document will show you how to install them in an environment hosted by Amazon Managed Service for Prometheus.</p> <p>The Prometheus mixin project expects prometheus to be installed via the Prometheus Operator, but the Terraform blueprints install the Prometheus agent via the default helm charts. In order for the scraping jobs and the dashboards to line up, we need to update the Prometheus rules and the mixin dashboard configuration, then upload the dashboard to our Grafana instance.</p>"},{"location":"recipes/recipes/amp-mixin-dashboards/#prerequisites","title":"Prerequisites","text":"<ul> <li>An EKS cluster - Starting from: https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/examples/complete-kubernetes-addons</li> <li>A Cloud9 environment</li> <li>kubectl in Cloud9 configured to manage the EKS cluster</li> <li>IAM credentials for EKS</li> <li>An instance of AMP</li> <li>An instance of Amazon Managed Grafana</li> </ul>"},{"location":"recipes/recipes/amp-mixin-dashboards/#installing-the-mixin-dashboards","title":"Installing the mixin dashboards","text":"<p>Starting from a fresh Cloud9 instance and using the AWS blueprint for terraform complete addon example as the target EKS cluster as linked in the Prerequisites:</p> <p>Expand the file system of the Cloud9 instance to at least 20 gb. In the EC2 console, extend the EBS volume to 20 GB thenfrom the Cloud9 shell, run the commands below:</p> <pre><code>sudo growpart /dev/nvme0n1 1\nsudo xfs_growfs -d /\n</code></pre> <p>Upgrade awscli to version 2:</p> <pre><code>sudo yum remove -y awscli\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nln -s /usr/local/bin/aws /usr/bin/aws\n</code></pre> <p>Install prerequisites: </p> <pre><code>sudo yum install -y jsonnet\ngo install -a github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest\nexport PATH=\"$PATH:~/go/bin\"\n</code></pre> <p>Download and install the jsonnet libraries for the kubernetes-mixin project:</p> <pre><code>git clone https://github.com/kubernetes-monitoring/kubernetes-mixincd kubernetes-mixin/\njb install\n</code></pre> <p>Edit config.libsonnet and replace the \u201cselectors\u201c section with the following in order to match the prometheus job names:</p> <pre><code> // Selectors are inserted between {} in Prometheus queries.\ncadvisorSelector: 'job=\"kubernetes-nodes-cadvisor\"',\nkubeletSelector: 'job=\"kubernetes-nodes\"',\nkubeStateMetricsSelector: 'job=\"kubernetes-service-endpoints\"',\nnodeExporterSelector: 'job=\"kubernetes-service-endpoints\"',\nkubeSchedulerSelector: 'job=\"kube-scheduler\"',\nkubeControllerManagerSelector: 'job=\"kube-controller-manager\"',\nkubeApiserverSelector: 'job=\"kubernetes-apiservers\"',\nkubeProxySelector: 'job=\"kubernetes-nodes\"',\npodLabel: 'pod',\nhostNetworkInterfaceSelector: 'device!~\"veth.+\"',\nhostMountpointSelector: 'mountpoint=\"/\"',\nwindowsExporterSelector: 'job=\"kubernetes-windows-exporter\"',\ncontainerfsSelector: 'container!=\"\"',\n</code></pre> <p>Build the prometheus rules, alerts, and grafana dashboards:</p> <pre><code>make prometheus_alerts.yaml\nmake prometheus_rules.yaml\nmake dashboards_out\n</code></pre> <p>Upload the prometheus rules to managed prometheus. Replace &lt;&gt; with the ID of your managed prometheus instance and &lt;&gt; with the appropriate value <pre><code>base64 prometheus_rules.yaml &gt; prometheus_rules.b64\naws amp create-rule-groups-namespace --data file://prometheus_rules.b64 --name kubernetes-mixin  --workspace-id &lt;&lt;WORKSPACE-ID&gt; --region &lt;&lt;REGION&gt;&gt;\n</code></pre> <p>Download the contents of the \u2018dashboard_out\u2019 folder from the Cloud9 environment and upload them using the Grafana web UI.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/","title":"Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager","text":"<p>Customers want to migrate their existing Prometheus workloads to the cloud and utilize all that the cloud offers. AWS has services like Amazon EC2 Auto Scaling, which lets you scale out Amazon Elastic Compute Cloud (Amazon EC2) instances based on metrics like CPU or memory utilization. Applications that use Prometheus metrics can easily integrate into EC2 Auto Scaling without needing to replace their monitoring stack. In this post, I will walk you through configuring Amazon EC2 Auto Scaling to work with Amazon Managed Service for Prometheus Alert Manager. This approach lets you move a Prometheus-based workload to the cloud while taking advantage of services like autoscaling.</p> <p>Amazon Managed Service for Prometheus provides support for alerting rules that use PromQL. The Prometheus alerting rules documentation provides the syntax and examples of valid alerting rules. Likewise, the Prometheus alert manager documentation references both the syntax and examples of valid alert manager configurations.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#solution-overview","title":"Solution overview","text":"<p>First, let\u2019s briefly review Amazon EC2 Auto Scaling\u2018s concept of an Auto Scaling group which is a logical collection of Amazon EC2 instances. An Auto Scaling group can launch EC2 instances based on a predefined launch template. The launch template contains information used to launch the Amazon EC2 instance, including the AMI ID, the instance type, network settings, and AWS Identity and Access Management (IAM) instance profile.</p> <p>Amazon EC2 Auto Scaling groups have a minimum size, maximum size, and desired capacity concepts. When Amazon EC2 Auto Scaling detects that the current running capacity of the Auto Scaling group is above or below the desired capacity, it will automatically scale out or scale in as needed. This scaling approach lets you utilize elasticity within your workload while still keeping bounds on both capacity and costs.</p> <p>To demonstrate this solution, I have created an Amazon EC2 Auto Scaling group that contains two Amazon EC2 instances. These instances remote write instance metrics to an Amazon Managed Service for Prometheus workspace. I have set the Auto Scaling group\u2019s minimum size to two (to maintain high availability), and I\u2019ve set the group\u2019s maximum size to 10 (to help control costs). As more traffic hits the solution, additional Amazon EC2 instances are automatically added to support the load, up to the Amazon EC2 Auto Scaling group\u2019s maximum size. As the load decreases, those Amazon EC2 instances are terminated until the Amazon EC2 Auto Scaling group reaches the group\u2019s minimum size. This approach lets you have a performant application by utilizing the elasticity of the cloud.</p> <p>Note that as you scrape more and more resources, you could quickly overwhelm the capabilities of a single Prometheus server. You can avoid this situation by scaling Prometheus servers linearly with the workload. This approach ensures that you can collect metric data at the granularity that you want.</p> <p>To support the Auto Scaling of a Prometheus workload, I have created an Amazon Managed Service for Prometheus workspace with the following rules:</p> <p><code>YAML</code></p> <pre><code>groups:\n- name: example\nrules:\n- alert: HostHighCpuLoad\nexpr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &gt; 60\nfor: 5m\nlabels:\nseverity: warning\nevent_type: scale_up\nannotations:\nsummary: Host high CPU load (instance {{ $labels.instance }})\ndescription: \"CPU load is &gt; 60%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n- alert: HostLowCpuLoad\nexpr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &lt; 30\nfor: 5m\nlabels:\nseverity: warning\nevent_type: scale_down\nannotations:\nsummary: Host low CPU load (instance {{ $labels.instance }})\ndescription: \"CPU load is &lt; 30%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n</code></pre> <p>This rules set creates a <code>HostHighCpuLoad</code> and a <code>HostLowCpuLoad</code> rules. These alerts trigger when the CPU is greater than 60% or less than 30% utilization over a five-minute period.</p> <p>After raising an alert, the alert manager will forward the message into an Amazon SNS topic, passing an <code>alert_type</code> (the alert name) and <code>event_type</code> (scale_down or scale_up).</p> <p><code>YAML</code></p> <pre><code>alertmanager_config: |\n  route: \n    receiver: default_receiver\n    repeat_interval: 5m\n\n  receivers:\n    - name: default_receiver\n      sns_configs:\n        - topic_arn: &lt;ARN OF SNS TOPIC GOES HERE&gt;\n          send_resolved: false\n          sigv4:\n            region: us-east-1\n          message: |\n            alert_type: {{ .CommonLabels.alertname }}\n            event_type: {{ .CommonLabels.event_type }}\n</code></pre> <p>An AWS Lambda function is subscribed to the Amazon SNS topic. I have written logic in the Lambda function to inspect the Amazon SNS message and determine if a <code>scale_up</code> or <code>scale_down</code> event should happen. Then, the Lambda function increments or decrements the desired capacity of the Amazon EC2 Auto Scaling group. The Amazon EC2 Auto Scaling group detects a requested change in capacity, and then invokes or deallocates Amazon EC2 instances.</p> <p>The Lambda code to support Auto Scaling is as follows:</p> <p><code>Python</code></p> <pre><code>import json\nimport boto3\nimport os\n\ndef lambda_handler(event, context):\n    print(event)\n    msg = event['Records'][0]['Sns']['Message']\n\n    scale_type = ''\n    if msg.find('scale_up') &gt; -1:\n        scale_type = 'scale_up'\n    else:\n        scale_type = 'scale_down'\n\n    get_desired_instance_count(scale_type)\n\ndef get_desired_instance_count(scale_type):\n\n    client = boto3.client('autoscaling')\n    asg_name = os.environ['ASG_NAME']\n    response = client.describe_auto_scaling_groups(AutoScalingGroupNames=[ asg_name])\n\n    minSize = response['AutoScalingGroups'][0]['MinSize']\n    maxSize = response['AutoScalingGroups'][0]['MaxSize']\n    desiredCapacity = response['AutoScalingGroups'][0]['DesiredCapacity']\n\n    if scale_type == \"scale_up\":\n        desiredCapacity = min(desiredCapacity+1, maxSize)\n    if scale_type == \"scale_down\":\n        desiredCapacity = max(desiredCapacity - 1, minSize)\n\n    print('Scale type: {}; new capacity: {}'.format(scale_type, desiredCapacity))\n    response = client.set_desired_capacity(AutoScalingGroupName=asg_name, DesiredCapacity=desiredCapacity, HonorCooldown=False)\n</code></pre> <p>The full architecture can be reviewed in the following figure.</p> <p></p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#testing-out-the-solution","title":"Testing out the solution","text":"<p>You can launch an AWS CloudFormation template to automatically provision this solution.</p> <p>Stack prerequisites:</p> <ul> <li>An Amazon Virtual Private Cloud (Amazon VPC)</li> <li>An AWS Security Group that allows outbound traffic</li> </ul> <p>Select the Download Launch Stack Template link to download and set up the template in your account. As part of the configuration process, you must specify the subnets and the security groups that you want associated with the Amazon EC2 instances. See the following figure for details.</p> <p>## Download Launch Stack Template </p> <p></p> <p>This is the CloudFormation stack details screen, where the stack name has been set as prometheus-autoscale. The stack parameters include a URL of the Linux installer for Prometheus, the URL for the Linux Node Exporter for Prometheus, the subnets and security groups used in the solution, the AMI and instance type to use, and the maximum capacity of the Amazon EC2 Auto Scaling group.</p> <p>The stack will take approximately eight minutes to deploy. Once complete, you will find two Amazon EC2 instances that have been deployed and are running in the Amazon EC2 Auto Scaling group that has been created for you. To validate that this solution auto-scales via Amazon Managed Service for Prometheus Alert Manager, you apply load to the Amazon EC2 instances using the AWS Systems Manager Run Command and the AWSFIS-Run-CPU-Stress automation document.</p> <p>As stress is applied to the CPUs in the Amazon EC2 Auto Scaling group, alert manager publishes these alerts, which the Lambda function responds to by scaling up the Auto Scaling group.  As CPU consumption decreases, the low CPU alert in the Amazon Managed Service for Prometheus workspace fires, alert manager publishes the alert to the Amazon SNS topic, and the Lambda function responds by responds by scaling down the Auto Scaling group, as demonstrated in the following figure.</p> <p></p> <p>The Grafana dashboard has a line showing that CPU has spiked to 100%. Although the CPU is high, another line shows that the number of instances has stepped up from 2 to 10. Once CPU has decreased, the number of instances slowly decreases back down to 2.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#costs","title":"Costs","text":"<p>Amazon Managed Service for Prometheus is priced based on the metrics ingested, metrics stored, and metrics queried. Visit the Amazon Managed Service for Prometheus pricing page for the latest pricing and pricing examples.</p> <p>Amazon SNS is priced based on the number of monthly API requests made. Message delivery between Amazon SNS and Lambda is free, but it does charge for the amount of data transferred between Amazon SNS and Lambda. See the latest Amazon SNS pricing details.</p> <p>Lambda is priced based on the duration of your function execution and the number of requests made to the function. See the latest AWS Lambda pricing details.</p> <p>There are no additional charges for using Amazon EC2 Auto Scaling.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#conclusion","title":"Conclusion","text":"<p>By using Amazon Managed Service for Prometheus, alert manager, Amazon SNS, and Lambda, you can control the scaling activities of an Amazon EC2 Auto Scaling group. The solution in this post demonstrates how you can move existing Prometheus workloads to AWS, while also utilizing Amazon EC2 Auto Scaling. As load increases to the application, it seamlessly scales to meet demand.</p> <p>In this example, the Amazon EC2 Auto Scaling group scaled based on CPU, but you can follow a similar approach for any Prometheus metric from your workload. This approach provides fine-grained control over scaling actions, thereby making sure that you can scale your workload on the metric that provides the most business value.</p> <p>In previous blog posts, we\u2019ve also demonstrated how you can use Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty and how to integrate Amazon Managed Service for Prometheus with Slack. These solutions show how you can receive alerts from your workspace in the way that is most useful to you.</p> <p>For next steps, see how to create your own rules configuration file for Amazon Managed Service for Prometheus, and set up your own alert receiver. Moreover, check out Awesome Prometheus alerts for some good examples of alerting rules that can be used within alert manager.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/","title":"Using AWS Distro for OpenTelemetry in EKS on EC2 with Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Grafana (AMG) to visualize the metrics.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS)  on EC2 cluster and Amazon Elastic Container Registry (ECR)  repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the  ADOT Collector to  scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus.</p> <p></p> <p>The ADOT Collector includes two components specific to Prometheus: </p> <ul> <li>the Prometheus Receiver, and </li> <li>the AWS Prometheus Remote Write Exporter.</li> </ul> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have docker installed into your environment.</li> </ul>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#create-eks-on-ec2-cluster","title":"Create EKS on EC2 cluster","text":"<p>Our demo application in this recipe will be running on top of EKS.  You can either use an existing EKS cluster or create one using cluster-config.yaml.</p> <p>This template will create a new cluster with two EC2 <code>t2.large</code> nodes. </p> <p>Edit the template file and set <code>&lt;YOUR_REGION&gt;</code> to one of the supported regions for AMP.</p> <p>Make sure to overwrite <code>&lt;YOUR_REGION&gt;</code> in your session, for example in bash:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\n</code></pre> <p>Create your cluster using the following command.</p> <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-an-ecr-repository","title":"Set up an ECR repository","text":"<p>In order to deploy our application to EKS we need a container registry.  You can use the following command to create a new ECR registry in your account.  Make sure to set <code>&lt;YOUR_REGION&gt;</code> as well.</p> <pre><code>aws ecr create-repository \\\n--repository-name prometheus-sample-app \\\n--image-scanning-configuration scanOnPush=true \\\n--region &lt;YOUR_REGION&gt;\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>create a workspace using the AWS CLI </p> <pre><code>aws amp create-workspace --alias prometheus-sample-app\n</code></pre> <p>Verify the workspace is created using:</p> <pre><code>aws amp list-workspaces\n</code></pre> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-ec2.yaml  and edit this YAML doc with the parameters described in the next steps.</p> <p>In this example, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code>  to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish  the sample app endpoint from <code>kube-system</code> endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. </p> <p>Use the following steps to edit the downloaded file for your environment:</p> <p>1. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. </p> <p>2. Replace <code>&lt;YOUR_ENDPOINT&gt;</code> with the remote write URL of your workspace.</p> <p>Get your AMP remote write URL endpoint by executing the following queries. </p> <p>First, get the workspace ID like so:</p> <pre><code>YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\\n                    --alias prometheus-sample-app \\\n                    --query 'workspaces[0].workspaceId' --output text)\n</code></pre> <p>Now get the remote write URL endpoint URL for your workspace using:</p> <pre><code>YOUR_ENDPOINT=$(aws amp describe-workspace \\\n                --workspace-id $YOUR_WORKSPACE_ID  \\\n                --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write\n</code></pre> <p>Warning</p> <p>Make sure that <code>YOUR_ENDPOINT</code> is in fact the remote write URL, that is,  the URL should end in <code>/api/v1/remote_write</code>.</p> <p>After creating deployment file we can now apply this to our cluster by using the following command: </p> <pre><code>kubectl apply -f adot-collector-ec2.yaml\n</code></pre> <p>Info</p> <p>For more information check out the AWS Distro for OpenTelemetry (ADOT)  Collector Setup.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amg","title":"Set up AMG","text":"<p>Setup a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide.</p> <p>Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.</p> <p></p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#application","title":"Application","text":"<p>In this recipe we will be using a sample application  from the AWS Observability repository.</p> <p>This Prometheus sample app generates all four Prometheus metric types  (counter, gauge, histogram, summary) and exposes them at the <code>/metrics</code> endpoint.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#build-container-image","title":"Build container image","text":"<p>To build the container image, first clone the Git repository and change into the directory as follows:</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git &amp;&amp; \\\ncd ./aws-otel-community/sample-apps/prometheus\n</code></pre> <p>First, set the region (if not already done above) and account ID to what is applicable in your case.  Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. For example, in the Bash shell this would look as follows:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>Next, build the container image:</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Note</p> <p>If <code>go mod</code> fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile.</p> <p>Change the following line in the Docker file: <code>RUN GO111MODULE=on go mod download</code> to: <code>RUN GOPROXY=direct GO111MODULE=on go mod download</code></p> <p>Now you can push the container image to the ECR repo you created earlier on.</p> <p>For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\\ndocker login --username AWS --password-stdin \\\n\"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#deploy-sample-app","title":"Deploy sample app","text":"<p>Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>AWS_DEFAULT_REGION</code> in the file with your own values:</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl apply -f prometheus-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","title":"Verify your pipeline is working","text":"<p>To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs.</p> <p>Enter the following command to follow the ADOT collector logs:</p> <pre><code>kubectl -n adot-col logs adot-collector -f\n</code></pre> <p>One example output in the logs of the scraped metrics from the sample app  should look like the following:</p> <pre><code>...\nResource labels:\n-&gt; service.name: STRING(kubernetes-service-endpoints)\n-&gt; host.name: STRING(192.168.16.238)\n-&gt; port: STRING(8080)\n-&gt; scheme: STRING(http)\nInstrumentationLibraryMetrics #0\nMetric #0\nDescriptor:\n-&gt; Name: test_gauge0\n-&gt; Description: This is my gauge\n-&gt; Unit: -&gt; DataType: DoubleGauge\nDoubleDataPoints #0\nStartTime: 0\nTimestamp: 1606511460471000000\nValue: 0.000000\n...\n</code></pre> <p>Tip</p> <p>To verify if AMP received the metrics, you can use awscurl. This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace <code>$AMP_ENDPOINT</code> with the endpoint for your AMP workspace:</p> <p><code>$ awscurl --service=\"aps\" \\          --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}</code></p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","title":"Create a Grafana dashboard","text":"<p>You can import an example dashboard, available via prometheus-sample-app-dashboard.json, for the sample app that looks as follows:</p> <p></p> <p>Further, use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on EC2 to  ingest metrics.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the resources and cluster</li> </ol> <pre><code>kubectl delete all --all\neksctl delete cluster --name amp-eks-ec2\n</code></pre> <ol> <li>Remove the AMP workspace</li> </ol> <pre><code>aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <ol> <li>Remove the amp-iamproxy-ingest-role IAM role </li> </ol> <pre><code>aws delete-role --role-name amp-iamproxy-ingest-role\n</code></pre> <ol> <li>Remove the AMG workspace by removing it from the console. </li> </ol>"},{"location":"recipes/recipes/eks-observability-accelerator/","title":"Introducing Amazon EKS Observability Accelerator","text":"<p>Observability is critical for any application and understanding system behavior and performance. It takes time and effort to detect and remediate performance slowdowns or disruptions. Customers often spend much time writing configuration files and work quite to achieve end-to-end monitoring for applications. Infrastructure as Code (IaC) tools, such as AWS CloudFormation, Terraform, and Ansible, reduce manual efforts by helping administrators and developers instantiate infrastructure using configuration files.</p> <p>Amazon Elastic Kubernetes Service (Amazon EKS) is a powerful and  extensible container orchestration technology that lets you deploy and manage containerized applications at scale. Building a tailored Amazon EKS cluster amidst the wide range of tooling and design choices available and making sure that it meets your application\u2019s specific needs can take a significant amount of time. This situation becomes even more cumbersome when you implement observability, which is critical for analyzing any application\u2019s performance.</p> <p>Customers have been asking us for examples demonstrating the integration of various open-source tools on Amazon EKS and the configuration of observability solutions incorporating best practices for specific application requirements. On May 17, 2022, AWS announced EKS Observability Accelerator, which is used to configure and deploy purpose-built observability solutions on Amazon EKS clusters for specific workloads using Terraform modules. Customers can use this solution to get started with Amazon Managed Service for Prometheus, AWS Distro for OpenTelemetry, and Amazon Managed Grafana by running a single command and beginning to monitor applications.</p> <p>We built the Terraform modules to enable observability on Amazon EKS clusters for the following workloads:</p> <ul> <li>Java/JMX</li> <li>NGINX</li> <li>Memcached</li> <li>HAProxy</li> </ul> <p>AWS will continue to add examples for more workloads in the future.</p> <p>In this post, you will walk through the steps for using EKS Observability Accelerator to build the Amazon EKS cluster and configure opinionated observability components to monitor specific workloads, which is a Java/JMX application.</p>"},{"location":"recipes/recipes/eks-observability-accelerator/#prerequisites","title":"Prerequisites","text":"<p>Make sure you complete the prerequisites before proceeding with this solution</p> <ul> <li>Install Terraform</li> <li>Install Kubectl</li> <li>Install docker</li> <li>AWS Command Line Interface (AWS CLI) version 2</li> <li>jq</li> <li>An AWS Account</li> <li>Configure the credentials in AWS CLI</li> <li>An existing Amazon Managed Grafana Workspace</li> </ul>"},{"location":"recipes/recipes/eks-observability-accelerator/#deployment-steps","title":"Deployment steps","text":"<p>Imagine that you\u2019re a Kubernetes operator and in charge of provisioning the Kubernetes environment for your organization. The requirements you get from teams can be diverse and require spending a significant amount of time provisioning the Kubernetes environment and incorporating those configurations. The clock resets every time a new request comes, so re-inventing the wheel continues.</p> <p>To simplify this and reduce the work hours, we came up with EKS Blueprints. EKS Blueprints is a collection of Terraform modules that aim to make it easier and faster for customers to adopt Amazon EKS and start deploying typical workloads. It\u2019s open -source and can be used by anyone to configure and manage complete Amazon EKS clusters that are fully bootstrapped with the operational software needed to deploy and operate workloads.</p> <p>The EKS Blueprints repository contains The Amazon EKS Observability Accelerator module. You\u2019ll use it to configure observability for the Java/JMX application deployed on the Amazon EKS cluster.</p>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-1-cloning-the-repository","title":"Step 1: Cloning the repository","text":"<p>First, you\u2019ll clone the repository that contains the EKS blueprints:</p> <pre><code>git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git\n</code></pre>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-2-generate-a-grafana-api-key","title":"Step 2: Generate a Grafana API Key","text":"<p>Before we deploy the Terraform module, we\u2019ll create a Grafana API key and configure the Terraform variable file to use the keys to deploy the dashboard. We\u2019ll use an existing Amazon Managed Grafana workspace and must log in to the workspace URL to configure the API keys.</p> <p>Follow these steps to create the key</p> <ul> <li>Use your SAML/SSO credential to log in to the Amazon Managed Grafana workspace.</li> <li>Hover to the left side control panel, and select the API keys tab under the gear icon.</li> </ul> <p></p> <ul> <li>Click Add API key, fill in the Name field and select the Role as Admin.</li> <li>Fill in the Time to live field. It\u2019s the API key life duration. For example, 1d to expire the key after one day. Supported units are: s,m,h,d,w,M,y</li> </ul> <p></p> <ul> <li>Click Add</li> <li>Copy and keep the API key safe, we will use this Key in our next step</li> </ul> <p></p> <p></p>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-3-configuring-the-environment","title":"Step 3: Configuring the environment","text":"<p>Next, you\u2019ll configure the environment to deploy the Terraform module to provision the EKS cluster, AWS OTEL Operator, and Amazon Managed Service for Prometheus.</p> <p>Deploying the Terraform module involves the below steps:</p> <ul> <li>Plan: Terraform plan creates an execution plan and previews the infrastructure changes.</li> <li>Apply: Terraform executes the plan\u2019s action and modifies the environment.</li> </ul> <p>Next, configure the environment either by creating the variables file or setting up the environment variables.</p> <p>A \u201c.tfvars\u201d file is an alternative to using the \u201c-var\u201d flag or environment variables. The file defines the variable values used by the script.</p> <p>For this blog post, we will create a new file named \u201cdev.tfvars\u201d under ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java</p> <p>Make sure you edit the dev.tfvars file with corresponding Grafana workspace endpoint and Grafana API key. Also, if you want to customize the configuration, add the necessary variables to dev.tfvars file.</p> <pre><code>cd ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java\n</code></pre> <pre><code>vi dev.tfvars\n</code></pre> <pre><code>grafana_endpoint=\u201d&lt;Your Grafana workspace endpoint&gt;\"\ngrafana_api_key=\u201dYour Grafana API Key&gt;\"\n</code></pre> <p>Note</p> <p>API_KEY \u2013 is the key that you have created Grafana_Endpoint \u2013 Grafana workspace URL. </p> <p>Make sure to include with \u201chttps://\u201d otherwise, terraform module will fail.</p>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-4-deploying-the-terraform-modules","title":"Step 4: Deploying the Terraform modules","text":"<p>The first step is to initialize the working directory using terraform init command, which initializes a working directory containing Terraform configuration files. This command runs after writing a new Terraform configuration or cloning an existing one from version control.</p> <pre><code>terraform init\n</code></pre> <p>This command performs  initialization steps to prepare the current working directory for use with Terraform. Once the initialization completes, you should receive the following notification.</p> <p></p> <p>Additionally, we can execute the terraform validate command to evaluate the configuration files in a directory. Validate runs checks that verify whether or not a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state.</p> <pre><code>terraform validate\n</code></pre> <p></p> <p>The next step is to run the terraform plan command to create an execution plan, which lets you preview the Terraform infrastructure changes. By default, when terraform creates a plan it:</p> <ul> <li>Reads the current state of any already-existing remote objects to ensure that the Terraform state is up-to-date.</li> <li>It compares the current configuration to the initial state and reports any differences.</li> <li>Proposes a set of change actions that should, if applied, make the remote objects match the configuration.</li> </ul> <p>The plan command alone will not carry out the proposed changes. So you can use this command to check whether the proposed changes match what you expected before applying the changes or share your changes with your team for broader review.</p> <pre><code>terraform plan -var-file=./dev.tfvars\n</code></pre> <p></p> <p></p> <p>Finally, you\u2019ll run the <code>terraform apply</code> command to provision the resources, and it takes about 20 minutes to complete. This command deploys the following resources:</p> <pre><code>terraform apply -var-file=./dev.tfvars -auto-approve\n</code></pre> <ol> <li>Creates an Amazon EKS cluster named <code>aws001-preprod-dev-eks</code></li> <li>Creates an Amazon Managed Service for Prometheus workspace named <code>amp-ws-aws001-preprod-dev-eks</code></li> <li>Creates a Kubernetes namespace named <code>opentelemetry-operator-system, adot-collector-java</code></li> <li>Deploys the AWS ADOT collector into the namespace with the configuration to collect metrics for Java/JMX workloads</li> <li>Builds a dashboard to visualize Java/JMX metrics in an existing Amazon Managed Grafana workspace specified in the earlier step and configures the Amazon Managed Service for Prometheus workspace as a data source</li> </ol> <p>After provisioning the EKS cluster, you\u2019ll add the Amazon EKS Cluster endpoint to the kubeconfig and verify if the resources provision successfully.</p> <pre><code>aws eks --region $AWS_REGION update-kubeconfig --name aws001-preprod-dev-eks\n</code></pre> <p>Verify the creation of Amazon Managed Service for Prometheus workspace and ADOT collector by running the following command:</p> <pre><code>aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"amp-ws-aws001-preprod-dev-eks\").workspaceId'\n</code></pre> <p>Listing all the pods from the cluster</p> <pre><code>kubectl get pods -A\n</code></pre> <p></p> <p>We should be able to verify the connection between Amazon Managed Grafana and Amazon Managed Prometheus by heading to the configuration page and looking at the default data source.</p> <p></p> <p></p> <ul> <li>Select the Data source named <code>amp</code></li> <li>Scroll down and select <code>Save &amp; test</code></li> </ul> <p></p> <p>It should display a success message like below</p> <p></p>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-5-deploying-sample-javajmx-application","title":"Step 5: Deploying sample Java/JMX application","text":"<p>You\u2019ll deploy a sample Java/JMX application and start[nn1]  scraping the JMX metrics. The sample application generates JMX metrics, such as the JVM memory pool, JVM memory usage, and thread, and exports it in the Prometheus format. You\u2019ll deploy a load generator and a bad load generator to get a wide range of metrics to visualize eventually.</p> <p>The EKS Observability accelerator collects the metrics for the AWS OTEL operator deployment. ADOT exporter will ingest these metrics into the Amazon Managed Service for Prometheus workspace.</p> <p>We\u2019ll reuse an example from the AWS OpenTelemetry collector repository:</p> <pre><code># Switch to home directory\n\ncd ~/\n\n#Clone the git repository\n\ngit clone https://github.com/aws-observability/aws-otel-test-framework.git\n\n#Setup environment variables\nexport AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\n\n#Login to registry\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n\n#Create ECR Repository\naws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\\n--image-scanning-configuration scanOnPush=true \\\n--region $AWS_REGION\n\n#Build Docker image and push to ECR\n\ncd ~/aws-otel-test-framework/sample-apps/jmx\n\ndocker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest .\n\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest\n\n#Deploy the sample application\n\nexport SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\n\ncurl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml &gt; metrics-sample.yaml\n\nsed -e \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" metrics-sample.yaml -i\nsed -e \"s/{{region}}/$AWS_REGION/g\" metrics-sample.yaml -i\nsed -e \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" metrics-sample.yaml -i\n\nkubectl apply -f metrics-sample.yaml\n\n#Verify the application\n\nkubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE\n</code></pre> <p></p>"},{"location":"recipes/recipes/eks-observability-accelerator/#step-6-visualize-the-jmx-metrics-on-amazon-managed-grafana","title":"Step 6: Visualize the JMX metrics on Amazon Managed Grafana","text":"<p>To visualize the JMX metrics collected by the AWS ADOT operator, log in to the Grafana workspace.</p> <ul> <li>Select Dashboards and choose Manage</li> </ul> <p></p> <ul> <li>Select the Observability folder and choose the dashboard named EKS Accelerator \u2013 Observability \u2013 Java/JMX</li> </ul> <p></p> <p>The Terraform module added the Amazon Managed Service for Prometheus workspace as the default data source, and created a custom dashboard to visualize the metrics.</p> <p></p> <p>You can also deploy sample applications for NGINX, HAProxy, and Memcached.</p>"},{"location":"recipes/recipes/eks-observability-accelerator/#clean-up","title":"Clean up","text":"<p>Run the following command to tear down the resources provisioned by the Terraform module:</p> <pre><code>terraform destroy -var-file=./dev.tfvars -auto-approve\n</code></pre>"},{"location":"recipes/recipes/eks-observability-accelerator/#conclusion","title":"Conclusion","text":"<p>Customers can now leverage EKS Observability Accelerator  to deploy the opinionated EKS clusters and configure observability for specific workloads without spending much time manually deploying the resources and configuring the agent to scrape the metrics. Furthermore, the solution provides the extensibility to connect the Amazon Managed Prometheus workspace with Amazon Managed Grafana and configure alerts and notifications.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus . Then we're using Amazon Managed Grafana to visualize the metrics.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the  ADOT Collector to  scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus. </p> <p></p> <p>The ADOT Collector includes two components specific to Prometheus: </p> <ul> <li>the Prometheus Receiver, and </li> <li>the AWS Prometheus Remote Write Exporter.</li> </ul> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have Docker installed into your environment.</li> </ul>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-eks-on-fargate-cluster","title":"Create EKS on Fargate cluster","text":"<p>Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster-config.yaml template file by changing <code>&lt;YOUR_REGION&gt;</code> to one of the supported regions for AMP.</p> <p>Make sure to set <code>&lt;YOUR_REGION&gt;</code> in your shell session, for example, in Bash:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\n</code></pre> <p>Create your cluster using the following command:</p> <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-ecr-repository","title":"Create ECR repository","text":"<p>In order to deploy our application to EKS we need a container repository.  You can use the following command to create a new ECR repository in your account. Make sure to set <code>&lt;YOUR_REGION&gt;</code> as well.</p> <pre><code>aws ecr create-repository \\\n--repository-name prometheus-sample-app \\\n--image-scanning-configuration scanOnPush=true \\\n--region &lt;YOUR_REGION&gt;\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>First, create an Amazon Managed Service for Prometheus workspace using the AWS CLI with:</p> <pre><code>aws amp create-workspace --alias prometheus-sample-app\n</code></pre> <p>Verify the workspace is created using:</p> <pre><code>aws amp list-workspaces\n</code></pre> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-fargate.yaml  and edit this YAML doc with the parameters described in the next steps.</p> <p>In this example, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code>  to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish  the sample app endpoint from <code>kube-system</code> endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. </p> <p>Use the following steps to edit the downloaded file for your environment:</p> <p>1. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. </p> <p>2. Replace <code>&lt;YOUR_ENDPOINT&gt;</code> with the remote write URL of your workspace.</p> <p>Get your AMP remote write URL endpoint by executing the following queries. </p> <p>First, get the workspace ID like so:</p> <pre><code>YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\\n                    --alias prometheus-sample-app \\\n                    --query 'workspaces[0].workspaceId' --output text)\n</code></pre> <p>Now get the remote write URL endpoint URL for your workspace using:</p> <pre><code>YOUR_ENDPOINT=$(aws amp describe-workspace \\\n                --workspace-id $YOUR_WORKSPACE_ID  \\\n                --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write\n</code></pre> <p>Warning</p> <p>Make sure that <code>YOUR_ENDPOINT</code> is in fact the remote write URL, that is,  the URL should end in <code>/api/v1/remote_write</code>.</p> <p>After creating deployment file we can now apply this to our cluster by using the following command: </p> <pre><code>kubectl apply -f adot-collector-fargate.yaml\n</code></pre> <p>Info</p> <p>For more information check out the AWS Distro for OpenTelemetry (ADOT)  Collector Setup.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amg","title":"Set up AMG","text":"<p>Set up a new AMG workspace using the  Amazon Managed Grafana \u2013 Getting Started guide.</p> <p>Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.</p> <p></p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#application","title":"Application","text":"<p>In this recipe we will be using a sample application  from the AWS Observability repository.</p> <p>This Prometheus sample app generates all four Prometheus metric types  (counter, gauge, histogram, summary) and exposes them at the <code>/metrics</code> endpoint.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#build-container-image","title":"Build container image","text":"<p>To build the container image, first clone the Git repository and change into the directory as follows:</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git &amp;&amp; \\\ncd ./aws-otel-community/sample-apps/prometheus\n</code></pre> <p>First, set the region (if not already done above) and account ID to what is applicable in your case.  Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. For example, in the Bash shell this would look as follows:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>Next, build the container image:</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Note</p> <p>If <code>go mod</code> fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile.</p> <p>Change the following line in the Docker file: <code>RUN GO111MODULE=on go mod download</code> to: <code>RUN GOPROXY=direct GO111MODULE=on go mod download</code></p> <p>Now you can push the container image to the ECR repo you created earlier on.</p> <p>For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\\ndocker login --username AWS --password-stdin \\\n\"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#deploy-sample-app","title":"Deploy sample app","text":"<p>Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>AWS_DEFAULT_REGION</code> in the file with your own values:</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl apply -f prometheus-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","title":"Verify your pipeline is working","text":"<p>To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs.</p> <p>Enter the following command to follow the ADOT collector logs:</p> <pre><code>kubectl -n adot-col logs adot-collector -f\n</code></pre> <p>One example output in the logs of the scraped metrics from the sample app  should look like the following:</p> <pre><code>...\nResource labels:\n-&gt; service.name: STRING(kubernetes-service-endpoints)\n-&gt; host.name: STRING(192.168.16.238)\n-&gt; port: STRING(8080)\n-&gt; scheme: STRING(http)\nInstrumentationLibraryMetrics #0\nMetric #0\nDescriptor:\n-&gt; Name: test_gauge0\n-&gt; Description: This is my gauge\n-&gt; Unit: -&gt; DataType: DoubleGauge\nDoubleDataPoints #0\nStartTime: 0\nTimestamp: 1606511460471000000\nValue: 0.000000\n...\n</code></pre> <p>Tip</p> <p>To verify if AMP received the metrics, you can use awscurl. This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace <code>$AMP_ENDPOINT</code> with the endpoint for your AMP workspace:</p> <p><code>$ awscurl --service=\"aps\" \\          --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}</code></p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","title":"Create a Grafana dashboard","text":"<p>You can import an example dashboard, available via prometheus-sample-app-dashboard.json, for the sample app that looks as follows:</p> <p></p> <p>Further, use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on Fargate to  ingest metrics.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#cleanup","title":"Cleanup","text":"<p>First remove the Kubernetes resources and destroy the EKS cluster:</p> <pre><code>kubectl delete all --all &amp;&amp; \\\neksctl delete cluster --name amp-eks-fargate\n</code></pre> <p>Remove the Amazon Managed Service for Prometheus workspace:</p> <pre><code>aws amp delete-workspace --workspace-id \\\n    `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <p>Remove the  IAM role:</p> <pre><code>aws delete-role --role-name adot-collector-role\n</code></pre> <p>Finally, remove the Amazon Managed Grafana  workspace by removing it via the AWS console. </p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with AWS X-Ray","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to  ingest traces into AWS X-Ray and visualize the traces in Amazon Managed Grafana.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the  ADOT Collector to  collect traces from an instrumented app and ingest them into X-Ray:</p> <p></p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have Docker installed into your environment.</li> <li>You have the aws-observability/aws-o11y-recipes   repo cloned into your local environment.</li> </ul>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#create-eks-on-fargate-cluster","title":"Create EKS on Fargate cluster","text":"<p>Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster_config.yaml.</p> <p>Create your cluster using the following command:</p> <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#create-ecr-repository","title":"Create ECR repository","text":"<p>In order to deploy our application to EKS we need a container repository. We will use the private ECR registry, but you can also use ECR Public, if you want to share the container image.</p> <p>First, set the environment variables, such as shown here (substitute for your region):</p> <pre><code>export REGION=\"eu-west-1\"\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>You can use the following command to create a new ECR repository in your account: </p> <pre><code>aws ecr create-repository \\\n--repository-name ho11y \\\n--image-scanning-configuration scanOnPush=true \\\n--region $REGION\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-fargate.yaml  and edit this YAML doc with the parameters described in the next steps.</p> <pre><code>kubectl apply -f adot-collector-fargate.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#set-up-managed-grafana","title":"Set up Managed Grafana","text":"<p>Set up a new workspace using the  Amazon Managed Grafana \u2013 Getting Started guide and add X-Ray as a data source.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#signal-generator","title":"Signal generator","text":"<p>We will be using <code>ho11y</code>, a synthetic signal generator available via the sandbox of the recipes repository. So, if you haven't cloned the repo into your local environment, do now:</p> <pre><code>git clone https://github.com/aws-observability/aws-o11y-recipes.git\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#build-container-image","title":"Build container image","text":"<p>Make sure that your <code>ACCOUNTID</code> and <code>REGION</code> environment variables are set,  for example:</p> <pre><code>export REGION=\"eu-west-1\"\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>To build the <code>ho11y</code> container image, first change into the <code>./sandbox/ho11y/</code> directory and build the container image :</p> <p>Note</p> <p>The following build step assumes that the Docker daemon or an equivalent OCI image  build tool is running.</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#push-container-image","title":"Push container image","text":"<p>Next, you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $REGION | \\\ndocker login --username AWS --password-stdin \\\n\"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#deploy-signal-generator","title":"Deploy signal generator","text":"<p>Edit x-ray-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>REGION</code> in the file with your own values (overall, in three locations):</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.REGION.amazonaws.com/ho11y:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl -n example-app apply -f x-ray-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending traces from <code>ho11y</code> running in EKS to X-Ray and visualize it in AMG.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#verify-pipeline","title":"Verify pipeline","text":"<p>To verify if the ADOT collector is ingesting traces from <code>ho11y</code>, we make one of the services available locally and invoke it.</p> <p>First, let's forward traffic as so:</p> <pre><code>kubectl -n example-app port-forward svc/frontend 8765:80\n</code></pre> <p>With above command, the <code>frontend</code> microservice (a <code>ho11y</code> instance configured to talk to two other <code>ho11y</code> instances) is available in your local environment and you can invoke it as follows (triggering the creation of traces):</p> <pre><code>$ curl localhost:8765/\n{\"traceId\":\"1-6193a9be-53693f29a0119ee4d661ba0d\"}\n</code></pre> <p>Tip</p> <p>If you want to automate the invocation, you can wrap the <code>curl</code> call into a <code>while true</code> loop.</p> <p>To verify our setup, visit the X-Ray view in CloudWatch where you should see something like shown below:</p> <p></p> <p>Now that we have the signal generator set up and active and the OpenTelemetry pipeline set up, let's see how to consume the traces in Grafana.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#grafana-dashboard","title":"Grafana dashboard","text":"<p>You can import an example dashboard, available via x-ray-sample-dashboard.json that looks as follows:</p> <p></p> <p>Further, when you click on any of the traces in the lower <code>downstreams</code> panel, you can dive into it and view it in the \"Explore\" tab like so:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on Fargate to  ingest traces.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#cleanup","title":"Cleanup","text":"<p>First remove the Kubernetes resources and destroy the EKS cluster:</p> <pre><code>kubectl delete all --all &amp;&amp; \\\neksctl delete cluster --name xray-eks-fargate\n</code></pre> <p>Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console. </p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/","title":"Exporting CloudWatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a CloudWatch Metric Stream and use Kinesis Data Firehose and AWS Lambda to ingest metrics into Amazon Managed Service for Prometheus (AMP).</p> <p>We will be setting up a stack using AWS Cloud Development Kit (CDK) to create a Firehose Delivery Stream, Lambda, and a S3 Bucket to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p> <p>CloudWatch Metric Streams allow forwarding of the streaming metric data to a  HTTP endpoint or S3 bucket.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>The AWS CDK Typescript is installed in your environment.</li> <li>Node.js and Go.</li> <li>The repo has been cloned to your local machine.</li> </ul>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#create-an-amp-workspace","title":"Create an AMP workspace","text":"<p>Our demo application in this recipe will be running on top of AMP.  Create your AMP Workspace via the following command:</p> <pre><code>aws amp create-workspace --alias prometheus-demo-recipe\n</code></pre> <p>Ensure your workspace has been created with the following command:</p> <pre><code>aws amp list-workspaces\n</code></pre> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#install-dependencies","title":"Install dependencies","text":"<p>From the root of the aws-o11y-recipes repository, change your directory to CWMetricStreamExporter via the command:</p> <pre><code>cd sandbox/CWMetricStreamExporter\n</code></pre> <p>This will now be considered the root of the repo, going forward.</p> <p>Change directory to <code>/cdk</code> via the following command:</p> <pre><code>cd cdk\n</code></pre> <p>Install the CDK dependencies via the following command:</p> <pre><code>npm install\n</code></pre> <p>Change directory back to the root of the repo, and then change directory  to <code>/lambda</code> using following command:</p> <pre><code>cd lambda\n</code></pre> <p>Once in the <code>/lambda</code> folder, install the Go dependencies using:</p> <pre><code>go get\n</code></pre> <p>All the dependencies are now installed.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#modify-config-file","title":"Modify config file","text":"<p>In the root of the repo, open <code>config.yaml</code> and modify the AMP workspace URL  by replacing the <code>{workspace}</code> with the newly created workspace id, and the  region your AMP workspace is in.</p> <p>For example, modify the following with:</p> <pre><code>AMP:\nremote_write_url: \"https://aps-workspaces.us-east-2.amazonaws.com/workspaces/{workspaceId}/api/v1/remote_write\"\nregion: us-east-2\n</code></pre> <p>Change the names of the Firehose Delivery Stream and S3 Bucket to your liking.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#deploy-stack","title":"Deploy stack","text":"<p>Once the <code>config.yaml</code> has been modified with the AMP workspace ID, it is time  to deploy the stack to CloudFormation. To build the CDK and the Lambda code,  in the root of the repo run the following commend:</p> <pre><code>npm run build\n</code></pre> <p>This build step ensures that the Go Lambda binary is built, and deploys the CDK to CloudFormation.</p> <p>Accept the following IAM changes to deploy the stack:</p> <p></p> <p>Verify that the stack has been created by running the following command:</p> <pre><code>aws cloudformation list-stacks\n</code></pre> <p>A stack by the name <code>CDK Stack</code> should have been created.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#create-cloudwatch-stream","title":"Create CloudWatch stream","text":"<p>Navigate to the CloudWatch consoloe, for example  <code>https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metric-streams:streamsList</code>  and click \"Create metric stream\".</p> <p>Select the metics needed, either all metrics of only from selected namespaces.</p> <p>Configure the Metric Stream by using an existing Firehose which was created by the CDK. Change the output format to JSON instead of OpenTelemetry 0.7. Modify the Metric Stream name to your liking, and click \"Create metric stream\":</p> <p></p> <p>To verify the Lambda function invocation, navigate to the Lambda console and click the function <code>KinesisMessageHandler</code>. Click the <code>Monitor</code> tab and <code>Logs</code> subtab, and under <code>Recent Invocations</code> there should be entries of the Lambda function being triggered.</p> <p>Note</p> <p>It may take upto 5 minutes for invocations to show in the Monitor tab.</p> <p>That is it! Congratulations, your metrics are now being streamed from CloudWatch to Amazon Managed Service for Prometheus.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#cleanup","title":"Cleanup","text":"<p>First, delete the CloudFormation stack:</p> <pre><code>cd cdk\ncdk destroy\n</code></pre> <p>Remove the AMP workspace:</p> <pre><code>aws amp delete-workspace --workspace-id \\\n    `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <p>Last but not least, remove the CloudWatch Metric Stream by removing it from the console.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/","title":"Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags","text":"<p>In this recipe we show you how to use Metrics explorer to filter, aggregate, and visualize metrics by resource tags and resource properties - Use metrics explorer to monitor resources by their tags and properties.</p> <p>There are number of ways to create visualizations with Metrics explorer; in this walkthrough we simply leverage the AWS Console.</p> <p>Note</p> <p>This guide will take approximately 5 minutes to complete.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to AWS account</li> <li>Access to Amazon CloudWatch Metrics explorer via AWS Console</li> <li>Resource tags set for the relevant resources </li> </ul>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#metrics-explorer-tag-based-queries-and-visualizations","title":"Metrics Explorer tag based queries and visualizations","text":"<ul> <li> <p>Open the CloudWatch console </p> </li> <li> <p>Under Metrics, click on the Explorer menu </p> </li> </ul> <p></p> <ul> <li>You can either choose from one of the Generic templates or a Service based templates list; in this example we use the EC2 Instances by type template </li> </ul> <p></p> <ul> <li>Choose metrics you would like to explore; remove obsolete once, and add other metrics you would like to see</li> </ul> <p></p> <ul> <li>Under From, choose a resource tag or a resource property you are looking for; in the below example we show number of CPU and Network related metrics for different EC2 instances with Name: TeamX Tag</li> </ul> <p></p> <ul> <li>Please note, you can combine time series using an aggregation function under Aggregated by; in the below example TeamX metrics are aggregated by Availability Zone</li> </ul> <p></p> <p>Alternatively, you could aggregate TeamX and TeamY by the Team Tag, or choose any other configuration that suits your needs</p> <p></p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#dynamic-visualizations","title":"Dynamic visualizations","text":"<p>You can easily customize resulting visualizations by using From, Aggregated by and Split by options. Metrics explorer visualizations are dynamic, so any new tagged resource automatically appears in the explorer widget.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#reference","title":"Reference","text":"<p>For more information on Metrics explorer please refer to the following article: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metrics-Explorer.html</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/","title":"Monitoring hybrid environments using Amazon Managed Service for Grafana","text":"<p>In this recipe we show you how to visualize metrics from an Azure Cloud environment to Amazon Managed Service for Grafana (AMG) and create alert notifications in AMG to be sent to Amazon Simple Notification Service and Slack.</p> <p>As part of the implementation, we will create an AMG workspace, configure the Azure Monitor plugin as the data source for AMG and configure the Grafana dashboard. We will be creating two notification channels: one for Amazon SNS and one for slack.We will also configure alerts in the AMG dashboard to be sent to the notification channels.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to enable AWS-SSO</li> </ul>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#architecture","title":"Architecture","text":"<p>First, create an AMG workspace to visualize the metrics from Azure Monitor. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role.</p> <p>Note</p> <p>You must assign an Admin role to at least one user in the workspace.</p> <p>In Figure 1, the user name is grafana-admin. The user type is Admin. On the Data sources tab, choose the required data source. Review the configuration, and then choose Create workspace. </p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-the-data-source-and-custom-dashboard","title":"Configure the data source and custom dashboard","text":"<p>Now, under Data sources, configure the Azure Monitor plugin to start querying and visualizing the metrics from the Azure environment. Choose Data sources to add a data source. </p> <p>In Add data source, search for Azure Monitor and then configure the parameters from the app registration console in the Azure environment. </p> <p>To configure the Azure Monitor plugin, you need the directory (tenant) ID and the application (client) ID. For instructions, see the article about creating an Azure AD application and service principal. It explains how to register the app and grant access to Grafana to query the data.</p> <p></p> <p>After the data source is configured, import a custom dashboard to analyze the Azure metrics. In the left pane, choose the + icon, and then choose Import.</p> <p>In Import via grafana.com, enter the dashboard ID, 10532.</p> <p></p> <p>This will import the Azure Virtual Machine dashboard where you can start analyzing the Azure Monitor metrics. In my setup, I have a virtual machine running in the Azure environment.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-the-notification-channels-on-amg","title":"Configure the notification channels on AMG","text":"<p>In this section, you\u2019ll configure two notifications channels and then send alerts.</p> <p>Use the following command to create an SNS topic named grafana-notification and subscribe an email address.</p> <pre><code>aws sns create-topic --name grafana-notification\naws sns subscribe --topic-arn arn:aws:sns:&lt;region&gt;:&lt;account-id&gt;:grafana-notification --protocol email --notification-endpoint &lt;email-id&gt;\n</code></pre> <p>In the left pane, choose the bell icon to add a new notification channel. Now configure the grafana-notification notification channel. On Edit notification channel, for Type, choose AWS SNS. For Topic, use the ARN of the SNS topic you just created. For Auth Provider, choose the workspace IAM role.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#slack-notification-channel","title":"Slack notification channel","text":"<p>To configure a Slack notification channel, create a Slack workspace or use an existing one. Enable Incoming Webhooks as described in Sending messages using Incoming Webhooks.</p> <p>After you\u2019ve configured the workspace, you should be able to get a webhook URL that will be used in the Grafana dashboard.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-alerts-in-amg","title":"Configure alerts in AMG","text":"<p>You can configure Grafana alerts when the metric increases beyond the threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. In this example, configure an alert for CPU utilization for an Azure virtual machine. When the utilization exceeds a threshold, configure AMG to send notifications to both channels.</p> <p>In the dashboard, choose CPU utilization from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications.</p> <p>In the following configuration, an alert is created if the CPU utilization exceeds 50%. Notifications will be sent to the grafana-alert-notification and slack-alert-notification channels.</p> <p></p> <p>Now, you can sign in to the Azure virtual machine and initiate stress testing using tools like stress. When the CPU utilization exceeds the threshold, you will receive notifications on both channels.</p> <p>Now configure alerts for CPU utilization with the right threshold to simulate an alert that is sent to the Slack channel.</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#conclusion","title":"Conclusion","text":"<p>In the recipe, we showed you how to deploy the AMG workspace, configure notification channels, collect metrics from Azure Cloud, and configure alerts on the AMG dashboard. Because AMG is a fully managed, serverless solution, you can spend your time on the applications that transform your business and leave the heavy lifting of managing Grafana to AWS.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS","text":"<p>In this recipe we show you how to ingest App Mesh Envoy  metrics in an Amazon Elastic Kubernetes Service (EKS) cluster  to Amazon Managed Service for Prometheus (AMP) and create a custom dashboard on Amazon Managed Grafana  (AMG) to monitor the health and performance of microservices.</p> <p>As part of the implementation, we will create an AMP workspace, install the App Mesh  Controller for Kubernetes and inject the Envoy container into the pods. We will be  collecting the Envoy metrics using Grafana Agent  configured in the EKS cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard.</p> <p>Note</p> <p>This guide will take approximately 45 minutes to complete.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#architecture","title":"Architecture","text":"<p>The Grafana agent is configured to scrape the Envoy metrics and ingest them to AMP through the AMP remote write endpoint </p> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out Getting Started with Prometheus Remote Write Exporter for AMP.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have Docker installed into your environment.</li> <li>You need AMP workspace configured in your AWS account.</li> <li>You need to install Helm.</li> <li>You need to enable AWS-SSO.</li> </ul>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#setup-an-eks-cluster","title":"Setup an EKS cluster","text":"<p>First, create an EKS cluster that will be enabled with App Mesh for running the sample application.  The <code>eksctl</code> CLI will be used to deploy the cluster using the eks-cluster-config.yaml. This template will create a new cluster with EKS.</p> <p>Edit the template file and set your region to one of the available regions for AMP:</p> <ul> <li><code>us-east-1</code></li> <li><code>us-east-2</code></li> <li><code>us-west-2</code></li> <li><code>eu-central-1</code></li> <li><code>eu-west-1</code></li> </ul> <p>Make sure to overwrite this region in your session, for example, in the Bash shell:</p> <pre><code>export AWS_REGION=eu-west-1\n</code></pre> <p>Create your cluster using the following command:</p> <pre><code>eksctl create cluster -f eks-cluster-config.yaml\n</code></pre> <p>This creates an EKS cluster named <code>AMP-EKS-CLUSTER</code> and a service account  named <code>appmesh-controller</code> that will be used by the App Mesh controller for EKS.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#install-app-mesh-controller","title":"Install App Mesh Controller","text":"<p>Next, we will run the below commands to install the App Mesh Controller  and configure the Custom Resource Definitions (CRDs): </p> <pre><code>helm repo add eks https://aws.github.io/eks-charts\n</code></pre> <pre><code>helm upgrade -i appmesh-controller eks/appmesh-controller \\\n     --namespace appmesh-system \\\n     --set region=${AWS_REGION} \\\n     --set serviceAccount.create=false \\\n     --set serviceAccount.name=appmesh-controller\n</code></pre>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>The AMP workspace is used to ingest the Prometheus metrics collected from Envoy.  A workspace is a logical Cortex server dedicated to a tenant. A workspace supports fine-grained access control for authorizing its management such as update, list,  describe, and delete, and the ingestion and querying of metrics.</p> <p>Create a workspace using the AWS CLI:</p> <pre><code>aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION\n</code></pre> <p>Add the necessary Helm repositories:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts &amp;&amp; \\\nhelm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics \n</code></pre> <p>For more details on AMP check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#scraping-ingesting-metrics","title":"Scraping &amp; ingesting metrics","text":"<p>AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster.  You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the  process of configuring the Grafana Agent to scrape the Envoy metrics and analyze them using AMP and AMG.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-grafana-agent","title":"Configure Grafana Agent","text":"<p>The Grafana Agent is a lightweight alternative to running a full Prometheus server.  It keeps the necessary parts for discovering and scraping Prometheus exporters and  sending metrics to a Prometheus-compatible backend. The Grafana Agent also includes  native support for AWS Signature Version 4 (Sigv4) for AWS Identity and Access Management (IAM)  authentication.</p> <p>We now walk you through the steps to configure an IAM role to send Prometheus metrics to AMP.  We install the Grafana Agent on the EKS cluster and forward metrics to AMP.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-permissions","title":"Configure permissions","text":"<p>The Grafana Agent scrapes operational metrics from containerized workloads running in the  EKS cluster and sends them to AMP. Data sent to AMP must be signed with valid AWS credentials using Sigv4 to authenticate and authorize each client request for the managed service.</p> <p>The Grafana Agent can be deployed to an EKS cluster to run under the identity of a Kubernetes service account.  With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. </p> <p>Prepare the IRSA setup as follows:</p> <pre><code>kubectl create namespace grafana-agent\n\nexport WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId')\nexport ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text)\nexport NAMESPACE=\"grafana-agent\"\nexport REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\"\n</code></pre> <p>You can use the gca-permissions.sh  shell script to automate the following steps (note to replace the placeholder variable  <code>YOUR_EKS_CLUSTER_NAME</code> with the name of your EKS cluster):</p> <ul> <li>Creates an IAM role named <code>EKS-GrafanaAgent-AMP-ServiceAccount-Rol</code>e with an IAM policy that has permissions to remote-write into an AMP workspace.</li> <li>Creates a Kubernetes service account named <code>grafana-agent</code> under the <code>grafana-agent</code> namespace that is associated with the IAM role.</li> <li>Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster.</li> </ul> <p>You need <code>kubectl</code> and <code>eksctl</code> CLI tools to run the <code>gca-permissions.sh</code> script.  They must be configured to access your Amazon EKS cluster.</p> <p>Now create a manifest file, grafana-agent.yaml,  with the scrape configuration to extract Envoy metrics and deploy the Grafana Agent. </p> <p>Note</p> <p>At time of writing, this solution will not work for EKS on Fargate due to the lack of support for daemon sets there.</p> <p>The example deploys a daemon set named <code>grafana-agent</code> and a deployment named  <code>grafana-agent-deployment</code>. The <code>grafana-agent</code> daemon set collects metrics  from pods on the cluster and the <code>grafana-agent-deployment</code> deployment collects metrics from services that do not live on the cluster, such as the EKS control plane.</p> <pre><code>kubectl apply -f grafana-agent.yaml\n</code></pre> <p>After the <code>grafana-agent</code> is deployed, it will collect the metrics and ingest  them into the specified AMP workspace. Now deploy a sample application on the  EKS cluster and start analyzing the metrics.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#sample-application","title":"Sample application","text":"<p>To install an application and inject an Envoy container, we use the AppMesh controller for Kubernetes.</p> <p>First, install the base application by cloning the examples repo:</p> <pre><code>git clone https://github.com/aws/aws-app-mesh-examples.git\n</code></pre> <p>And now apply the resources to your cluster:</p> <pre><code>kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application\n</code></pre> <p>Check the pod status and make sure it is running:</p> <pre><code>$ kubectl -n prod get all\n\nNAME                            READY   STATUS    RESTARTS   AGE\npod/dj-cb77484d7-gx9vk          1/1     Running   0          6m8s\npod/jazz-v1-6b6b6dd4fc-xxj9s    1/1     Running   0          6m8s\npod/metal-v1-584b9ccd88-kj7kf   1/1     Running   0          6m8s\n</code></pre> <p>Next, install the App Mesh controller and meshify the deployment:</p> <pre><code>kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/\nkubectl rollout restart deployment -n prod dj jazz-v1 metal-v1\n</code></pre> <p>Now we should see two containers running in each pod:</p> <pre><code>$ kubectl -n prod get all\nNAME                        READY   STATUS    RESTARTS   AGE\ndj-7948b69dff-z6djf         2/2     Running   0          57s\njazz-v1-7cdc4fc4fc-wzc5d    2/2     Running   0          57s\nmetal-v1-7f499bb988-qtx7k   2/2     Running   0          57s\n</code></pre> <p>Generate the traffic for 5 mins and we will visualize it AMG later:</p> <pre><code>dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'`\n\nloop_counter=0\nwhile [ $loop_counter -le 300 ] ; do \\\nkubectl exec -n prod -it $dj_pod  -c dj \\\n-- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; \\\ndone\n</code></pre>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#create-an-amg-workspace","title":"Create an AMG workspace","text":"<p>To create an AMG workspace follow the steps in the Getting Started with AMG blog post.  To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group.  By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard.</p> <p>In this example, the user name is <code>grafana-admin</code> and the user type is <code>Admin</code>. Select the required data source. Review the configuration, and then choose <code>Create workspace</code>.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-amg-datasource","title":"Configure AMG datasource","text":"<p>To configure AMP as a data source in AMG, in the <code>Data sources</code> section, choose  <code>Configure in Grafana</code>, which will launch a Grafana workspace in the browser.  You can also manually launch the Grafana workspace URL in the browser.</p> <p></p> <p>As you can see from the screenshots, you can view Envoy metrics like downstream  latency, connections, response code, and more. You can use the filters shown to  drill down to the envoy metrics of a particular application.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-amg-dashboard","title":"Configure AMG dashboard","text":"<p>After the data source is configured, import a custom dashboard to analyze the Envoy metrics.  For this we use a pre-defined dashboard, so choose <code>Import</code> (shown below), and  then enter the ID <code>11022</code>. This will import the Envoy Global dashboard so you can  start analyzing the Envoy metrics.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-alerts-on-amg","title":"Configure alerts on AMG","text":"<p>You can configure Grafana alerts when the metric increases beyond the intended threshold.  With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification.  Before you create alert rules, you must create a notification channel.</p> <p>In this example, configure Amazon SNS as a notification channel. The SNS topic must be  prefixed with <code>grafana</code> for notifications to be successfully published to the topic if you use the defaults, that is, the service-managed permissions.</p> <p>Use the following command to create an SNS topic named <code>grafana-notification</code>:</p> <pre><code>aws sns create-topic --name grafana-notification\n</code></pre> <p>And subscribe to it via an email address. Make sure you specify the region and Account ID in the  below command:</p> <pre><code>aws sns subscribe \\\n--topic-arn arn:aws:sns:&lt;region&gt;:&lt;account-id&gt;:grafana-notification \\\n--protocol email \\\n--notification-endpoint &lt;email-id&gt;\n</code></pre> <p>Now, add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type,  use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created.  For Auth provider, choose AWS SDK Default.</p> <p></p> <p>Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit.  On the Alert tab of the graph panel, configure how often the alert rule should be evaluated  and the conditions that must be met for the alert to change state and initiate its notifications.</p> <p>In the following configuration, an alert is created if the downstream latency exceeds the  threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the resources and cluster:</li> </ol> <pre><code>kubectl delete all --all\neksctl delete cluster --name AMP-EKS-CLUSTER\n</code></pre> <ol> <li>Remove the AMP workspace:</li> </ol> <pre><code>aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <ol> <li>Remove the amp-iamproxy-ingest-role IAM role:</li> </ol> <pre><code>aws delete-role --role-name amp-iamproxy-ingest-role\n</code></pre> <ol> <li>Remove the AMG workspace by removing it from the console. </li> </ol>"},{"location":"recipes/recipes/Workspaces-Monitoring-AMP-AMG/","title":"Index","text":"<p>Organizations have started adopting Amazon Workspaces as virtual cloud based desktop as a solution (DAAS) to replace their existing traditional desktop solution to shift the cost and effort of maintaining laptops and desktops to a cloud pay-as-you-go model. Organizations using Amazon Workspaces would need support of these managed services to monitor their workspaces environment for Day 2 operations. A cloud based managed open source monitoring solution such as Amazon Managed Service for Prometheus and Amazon Managed Grafana helps IT teams to quickly setup and operate a monitoring solution to save cost. Monitoring CPU, memory, network, or disk activity from Amazon Workspace eliminates guesswork while troubleshooting Amazon Workspaces environment.</p> <p>A managed monitoring solution on your Amazon Workspaces environments yields following organizational benefits:</p> <ul> <li>Service desk staff can quickly identify and drill down to Amazon Workspace issues that need investigation without guesswork by leveraging managed monitoring services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Service desks staffs can investigate Amazon Workspace issues after the event using the historical data in Amazon Managed Service for Prometheus</li> <li>Eliminates long calls that waste time questioning business users on Amazon Workspaces issues</li> </ul> <p>In this blog post, we will set up Amazon Managed Service for Prometheus, Amazon Managed Grafana, and a Prometheus server on Amazon Elastic Compute Cloud (EC2) to provide a monitoring solution for Amazon Workspaces.  We will automate the deployment of Prometheus agents on any new Amazon Workspace using Active Directory Group Policy Objects (GPO).</p> <p>Solution Architecture</p> <p>The following diagram demonstrates the solution to monitor your Amazon Workspaces environment using AWS native managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana. This solution will deploy a Prometheus server on Amazon Elastic Compute Cloud (EC2) instance which polls prometheus agents on your Amazon Workspace periodically and remote writes metrics to Amazon Managed Service for Prometheus. We will be using Amazon Managed Grafana to query and visualize metrics on your Amazon Workspaces infrastructure. </p>"},{"location":"signals/alarms/","title":"Alarms","text":"<p>An alarm refers to the state of a probe, monitor, or change in a value over or under a given threshold. A simple example would be an alarm that sends an email when a disk is full or a web site is down. More sophisticated alarms are entirely programmatic and used to drive complex interactions such as auto-scaling or creating of entire server clusters. </p> <p>Regardless of the use case though, an alarm indicates the current state of a metric. This state can be <code>OK</code>, <code>WARNING</code>, <code>ALERT</code>, or <code>NO DATA</code>, depending on the system in question. </p> <p>Alarms reflect this state for a period of time and are built on top of a timeseries. As such, they are derived from a time series. This graph below shows two alarms: one with a warning threshold, and another that is indicative of average values across this timeseries. As the volume of traffic in this shows, the alarms for the warning threshold should be in a breach state when it dips below the defined value.</p> <p></p> <p>Info</p> <p>The purpose of an alarm can be either to trigger an action (either human or progammatic), or to be informational (that a threshold is breached). Alarms provide insight into performance of a metric.</p>"},{"location":"signals/alarms/#alert-on-things-that-are-actionable","title":"Alert on things that are actionable","text":"<p>Alarm fatigue is when people get so many alerts that they have learned to ignore them. This is not an indication of a well-monitored system! Rather this is an anti-pattern.</p> <p>Success</p> <p>Create alarms for things that are actionable, and you should always work from your objectives backwards.</p> <p>For example, if you operate a web site that requires fast response times, create an alert to be delivered when your response times are exceeding a given threshold. And if you have identified that poor performance is tied to high CPU utilization then alert on this datapoint proactively before it becomes an issue. However, there may no need to alert on all CPU utilization everywhere in your environment if it does not endanger your outcomes.</p> <p>Success</p> <p>If an alarm does not need alert you, or trigger an automated process, then there is no need to have it alert you. You should remove the notifications from alarms that are superfluous. </p>"},{"location":"signals/alarms/#beware-of-the-everything-is-ok-alarm","title":"Beware of the \"everything is OK alarm\"","text":"<p>Likewise, a common pattern is the \"everything is OK\" alarm, when operators are so used to getting constant alerts that they only notice when things suddenly go silent! This is a very dangerous mode to operate in, and a pattern that works against operational excellence.</p> <p>Warning</p> <p>The \"everything is OK alarm\" usually requries a human to interpret it! This makes patterns like self-healing applications impossible.1</p>"},{"location":"signals/alarms/#fight-alarm-fatigue-with-aggregation","title":"Fight alarm fatigue with aggregation","text":"<p>Observability is a human problem, not a technology problem. And as such, your alarm strategy should focus on reducing alarms rather than creating more. As you implement  telemetry collection, it is natural to have more alerts from your environment. Be cautious though to only alert on things that are actionable. If the condition that caused the alert is not actionable then there is no need to report on it.</p> <p>This is best shown by example: if you have five web servers that use a single database for their backend, what happens to your web servers if the database is down? The answer for many people is that they get at least six alerts - five for the web servers and one for the database! </p> <p></p> <p>But there are only two alerts that make sense to deliver:</p> <ol> <li>The web site is down, and</li> <li>The database is the cause</li> </ol> <p></p> <p>Success</p> <p>Distilling your alerts into aggregates makes it easier for people to understand, and then easier to create runbooks and automation for.</p>"},{"location":"signals/alarms/#use-your-existing-itsm-and-support-processes","title":"Use your existing ITSM and support processes","text":"<p>Regardless of your monitoring and observability platform, they must integrate into your current toolchain. </p> <p>Success</p> <p>Create trouble tickets and issues using a programmatic integration from your alerts into these tools, removing human effort and streamlining processes along the way. </p> <p>This allows you to derive important operatonal data such as DORA metrics.</p> <ol> <li> <p>See https://aws.amazon.com/blogs/apn/building-self-healing-infrastructure-as-code-with-dynatrace-aws-lambda-and-aws-service-catalog/ for more about this pattern.\u00a0\u21a9</p> </li> </ol>"},{"location":"signals/anomalies/","title":"Anomalies","text":"<p>WIP</p>"},{"location":"signals/events/","title":"Events","text":""},{"location":"signals/events/#what-do-we-mean-by-events","title":"What do we mean by events?","text":"<p>Many architectures are event driven these days. In event driven architectures, events are signals from different systems which we capture and pass onto other systems. An event is typically a change in state, or an update.</p> <p>For example, in an eCommerce system you may have an event when an item is added to the cart. This event could be captured and passed on to the shopping cart part of the system to update the number of items and cost of the cart, along with the item details.</p> <p>Info</p> <p>For some customers an event may be a milestone, such as a the completion of a purchase. There is a case to be made for treating the aggregate moment of a workflow conclusion as an event, but for our purposes we do not consider a milestone itself to be an event.</p>"},{"location":"signals/events/#why-are-events-useful","title":"Why are events useful?","text":"<p>There are two main ways in which events can be useful in your Observability solution. One is to visualize events in the context of other data, and the other is to enable you to take action based on an event. </p> <p>Success</p> <p>Events are intended to give valuable information, either to people or machines, about changes and actions in your workload.</p>"},{"location":"signals/events/#visualizing-events","title":"Visualizing events","text":"<p>There are many event signals which are not directly from your application, but may have an impact on your application performance, or provide additional insight into root cause. Dashboards are the most common mechanism for visualizing your events, though some analytics or business intelligence tools also work in this context. Even email or instant messaging applications can receive visualizations readily.</p> <p>Consider a timechart of application performance, such as time to place an order on your web front end. The time chart lets you see there has been a step change in the response time a few days ago. It might be useful to know if there have been any recent deployments. Consider being able to see a timechart of recent deployments alongside, or superimposed on the same chart?</p> <p></p> <p>Tip</p> <p>Consider which events might be useful to you to understand the wider context. The events that are important to you might be code deployments, infrastructure change events, adding new data (such as publishing new items for sale, or bulk adding new users), or modifying or adding functionality (such as changing the way people add items to their cart).</p> <p>Success</p> <p>Visualize events along with other important metric data so you can correlate events.</p>"},{"location":"signals/events/#taking-action-on-events","title":"Taking action on events","text":"<p>In the Observability world, a triggered alarm is a common event. This event would likely contain an identifier for the alarm, the alarm state (such as <code>IN ALARM</code>, or <code>OK</code>), and details of what triggered this. In many cases this alarm event will be detected and an email notification sent. This is an example of an action on an alarm. </p> <p>Alarm notification is critical in observability. This is how we let the right people know there is an issue. However, when action on events mature in your observability solution, it can automatically remediate the issue without human intervention. </p>"},{"location":"signals/events/#but-what-action-to-take","title":"But what action to take?","text":"<p>We cannot automate action without first understanding what action will ease the detected issue. At the start of your Observability journey, this may often not be obvious. However, the more experience you have remediating issues, the more you can fine tune your alarms to catch areas where there is a known action. There may be built in actions in the alarm service you have, or you may need to capture the alarm event yourself and script the resolution.</p> <p>Info</p> <p>Auto-scaling systems, such as a horizontal pod autoscaling are just an implementation of this principal. Kubernetes simply abstracts this automation for you.</p> <p>Having access to data on alarm frequency and resolution will help you decide if there is a possibility for automation. Whilst wider scope alarms based on issue symptoms are great at capturing issues, you may find you need more specific criteria to link to auto remediation.</p> <p>As you do this, consider integrating this with your incident management/ticketing/ITSM tool. Many organizations track incidents, and associated resolutions and metrics such as Mean Time to Resolve (MTTR). If you do this, consider also capturing your automated resolutions in a similar manner. This lets you understand the type and proportion of issues which are automatically remediated, but also allows you to look for underlying patterns and issues. </p> <p>Tip</p> <p>Just because someone didn't have to manually fix an issue, doesn't mean you shouldn't be looking at the underlying cause. </p> <p>For example, consider a server restart every time it becomes unresponsive. The restart allows the system to continue functioning, but what is causing the unresponsiveness. How often this happens, and if there is a pattern (for example that matches with report generation, or high users, or system backups), will determine the priority and resources you put into understanding and fixing the root cause.</p> <p>Success</p> <p>Consider delivery of every event related to your key performance indicators into a message bus for consumption. And note that some observability solutions do this transparently without explicit configuration requirements.</p>"},{"location":"signals/events/#getting-your-events-into-your-observability-platform","title":"Getting your events into your Observability platform","text":"<p>Once you have identified the events which are important to you, you'll need to consider how best to get them into your Observability platform.  Your platform may have a specific way to capture events, or you may have to bring them in as logs or metric data. </p> <p>Note</p> <p>One simple way to get the information in is to write the events to a log file and ingest them in the same way as you do your other log events.</p> <p>Explore how your system will let you visualize these. Can you identify events which are related to your application? Can you combine data onto a single chart? Even if there is nothing specific, you should at least be able to create a timechart alongside your other data to visually correlate. Keep the time axis the same, and consider stacking these vertically for easy comparison.</p> <p></p>"},{"location":"signals/logs/","title":"Logs","text":"<p>Logs are a series of messages that are sent by an application, or an appliance, that are represented by one or more lines of details about an event, or sometimes about the health of that application. Typically, logs are delivered to a file, though sometimes they are sent to a collector that performs analysis and aggregation. There are many full-featured log aggregators, frameworks, and products that aim to make the task of generating, ingesting, and managing log data at any volume \u2013 from megabytes per day to terabytes per hour.</p> <p>Logs are emitted by a single application at a time and usually pertain to the scope of that one application - though developers are free to have logs be as complex and nuanced as they desire. For our purposes we consider logs to be a fundamentally different signal from traces, which are composed of events from more than one application or a service, and with context about the connection between services such as response latency, service faults, request parameters etc.</p> <p>Data in logs can also be aggregate over a period of time. For example, they may be statistical (e.g. number of requests served over the previous minute). They can be structured, free-form, verbose, and in any written language. </p> <p>The primary use cases for logging are describing,</p> <ul> <li>an event, including its status and duration, and other vital statistics</li> <li>errors or warnings related to that event (e.g. stack traces, timeouts)</li> <li>application launches, start-up and shutdown messages</li> </ul> <p>Note</p> <p>Logs are intended to be immutable, and many log management systems include mechanisms to protect against, and detect attempts, to modify log data. </p> <p>Regardless of your requirements for logging, these are the best practices that we have identified. </p>"},{"location":"signals/logs/#structured-logging-is-key-to-success","title":"Structured logging is key to success","text":"<p>Many systems will emit logs in a semi-structured format. For example, an Apache web server may write logs like this, with each line pertaining to a single web request:</p> <pre><code>192.168.2.20 - - [28/Jul/2006:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n127.0.0.1 - - [28/Jul/2006:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n</code></pre> <p>Whereas a Java stack trace may be a single event that spans multiple lines and is less structured:</p> <pre><code>Exception in thread \"main\" java.lang.NullPointerException\nat com.example.myproject.Book.getTitle(Book.java:16)\nat com.example.myproject.Author.getBookTitles(Author.java:25)\nat com.example.myproject.Bootstrap.main(Bootstrap.java:14)\n</code></pre> <p>And a Python error log event may look like this:</p> <pre><code>Traceback (most recent call last):\n  File \"e.py\", line 7, in &lt;module&gt;\n    raise TypeError(\"Again !?!\")\nTypeError: Again !?!\n</code></pre> <p>Of these three examples, only the first one is easily parsed by both humans and a log aggregation system. Using structured logs makes it easy to process log data quickly and effectively, giving both humans and machines the data they need to immediately find what they are looking for.</p> <p>The most commonly understood log format is JSON, wherein each component to an event is represented as a key/value pair. In JSON, the python example above may be rewritten to look like this:</p> <pre><code>{\n    \"level\", \"ERROR\"\n    \"file\": \"e.py\",\n    \"line\": 7,\n    \"error\": \"TypeError(\\\"Again !?!\\\")\"\n}\n</code></pre> <p>The use of structured logs makes your data transportable from one log system to another, simplifies development, and make operational diagnosis faster (with less errors). Also, using JSON embeds the schema of the log message along with the actual data, which enables sophisticated log analysis systems to index your messages automatically.</p>"},{"location":"signals/logs/#use-log-levels-appropriately","title":"Use log levels appropriately","text":"<p>There are two types of logs: those that have a level and those that are a series of events. For those that have a level, these are a critical component to a successful logging strategy. Log levels vary slightly from one framework to another, but generally they follow this structure:</p> Level Description <code>DEBUG</code> Fine-grained informational events that are most useful to debug an application. These are usually of value to devlopers and are very verbose. <code>INFO</code> Informational messages that highlight the progress of the application at coarse-grained level. <code>WARN</code> Potentially harmful situations that indicate a risk to an application. These can trigger an alarm in an applicaiton. <code>ERROR</code> Error events that might still allow the application to continue running. These are likely to trigger an alarm that requires attention. <code>FATAL</code> Very severe error events that will presumably cause an application to abort. <p>Info</p> <p>Implicitly logs that have no explicit level may be considered as <code>INFO</code>, though this behaviour may vary between applications.</p> <p>Other common log levels are <code>CRITICAL</code> and <code>NONE</code>, depending on your needs, programming language, and framework. <code>ALL</code> and <code>NONE</code> are also common, though not found in every application stack.</p> <p>Log levels are crucial for informing your monitoring and observability solution about the health of your environment, and log data should easily express this data using a logical value. </p> <p>Tip</p> <p>Logging too much data at <code>WARN</code> will fill your monitoring system with data that is of limited value, and then you may lose important data in the sheer volume of messages.  </p> <p></p> <p>Success</p> <p>Using a standardized log level strategy makes automation easier, and helps developers get to the root cause of issues quickly.</p> <p>Warning</p> <p>Without a standard approach to log levels, filtering your logs is a major challenge.</p>"},{"location":"signals/logs/#filter-logs-close-to-the-source","title":"Filter logs close to the source","text":"<p>Wherever possible, reduce the volume of logs as close to the source as possible. There are many reasons to follow this best practice:</p> <ul> <li>Ingesting logs always costs time, money, and resources.</li> <li>Filtering sensitive data (e.g. personally identifiable data) from downstream systems reduces risk exposure from data leakage.</li> <li>Downstream systems may not have the same operational concerns as the sources of data. For example, <code>INFO</code> logs from an application may be of no interest to a monitoring and alerting system that watches for <code>CRITCAL</code> or <code>FATAL</code> messages.</li> <li>Log systems, and networks, need not be placed under undue stress and traffic.</li> </ul> <p>Success</p> <p>Filter your log close to the source to keep your costs down, decrease risk of data exposure, and focus each component on the things that matter.</p> <p>Tip</p> <p>Depending on your architecture, you may wish to use infrastructure as code (IaC) to deploy changes to your application and environment in one operation. This approach allows you to deploy your log filter patterns along with applications, giving them the same rigor and treatment.</p>"},{"location":"signals/logs/#avoid-double-ingestion-antipatterns","title":"Avoid double-ingestion antipatterns","text":"<p>A common pattern that administrators pursue is copying all of their logging data into a single system with the goal querying all of their logs all from a single location. There are some manual workflow advantages to doing so, however this pattern introduces additional cost, complexity, points of failure, and operational overhead.</p> <p></p> <p>Success</p> <p>Where possible, use a combination of log levels and log filtering to avoid a wholesale propagation of log data from your environments.</p> <p>Info</p> <p>Some organizations or workloads require log shipping in order to meet regulatory requirements, store logs in a secure location, provide non-reputability, or achieve other objectives. This is a common use case for re-ingesting log data. Note that a proper application of log levels and log filtering is still appropriate to reduce the volume of superfluous data entering these log archives.</p>"},{"location":"signals/logs/#collect-metric-data-from-your-logs","title":"Collect metric data from your logs","text":"<p>Your logs contain metrics that are just waiting to be collected! Even ISV solutions or applications that you have not written yourself will emit valuable data into their logs that you can extract meaningful insights into overall workload health from. Common examples include:</p> <ul> <li>Slow query time from databases</li> <li>Uptime from web servers</li> <li>Transaction processing time</li> <li>Counts of <code>ERROR</code> or <code>WARNING</code> events over time</li> <li>Raw count of packages that are available for upgrade</li> </ul> <p>Tip</p> <p>This data is less useful when locked in a static log file. The best practice is to identify key metric data and then publish it into your metric system where it can be correlated with other signals.</p>"},{"location":"signals/logs/#log-to-stdout","title":"Log to <code>stdout</code>","text":"<p>Where possible, applications shouould log to <code>stdout</code> rather than to a fixed location such as a file or socket. This enables log agents to collect and route your log events based on rules that make sense for your own observability solution. While not possible for all applications, this is the best practice for containerized workloads.</p> <p>Note</p> <p>While applications should be generic and simple in their logging practices, remaining loosely coupled from logging solutions, the transmission of log data does still require a log collector to send data from <code>stdout</code> to a file. The important concept is to avoid application and business logic being dependant on your logging infrastructure - in other words, you should work to separate your concerns.</p> <p>Success</p> <p>Decoupling your application from your log management lets you adapt and evolve your solution without code changes, thereby minimizing the potential blast radius of changes made to your environment.</p>"},{"location":"signals/metrics/","title":"Metrics","text":"<p>Metrics are a series of numerical values that are kept in order with the time that they are created. They are used to track everything from the number of servers in your environment, their disk usage, number of requests they handle per second, or the latency in completing these requests.</p> <p>But metrics are not limited to infrastructure or application monitoring. Rather, they can be used for any kind of business or workload to track sales, call queues, and customer satisfaction. In fact, metrics are most useful when combining both operational data and business metrics, giving a well-rounded view and observable system.</p> <p>It might be worth looking into the OpenTelemetry documentation page that provides some additional context on Metrics.</p>"},{"location":"signals/metrics/#know-your-key-performance-indicatorskpis-and-measure-them","title":"Know your Key Performance Indicators(KPIs), and measure them!","text":"<p>The most important thing with metrics is to measure the right things. And what those are will be different for everyone. An e-commerce application may have sales per hour as a critical KPI, whereas a bakery would like be more interested in the number of croissants made per day.</p> <p>Warning</p> <p>There is no singular, entirely complete, and comprehensive source for your business KPIs. You must understand your project or application well enough to know what your output goals are. </p> <p>Your first step is to name your high-level goals, and most likely those goals are not expressed as a single metric that comes from your infrastructure alone. In the e-commerce example above, once you identify the meta goal which is measuring sales per hour, you then can backtrack to detailed metrics such as time spent to search a product before purchase, time taken to complete the checkout process, latency of product search results and so on. This will guide us to be intentional about collecting relevant information to observe the system.</p> <p>Success</p> <p>Having identified your KPIs, you can now work backwards to see what metrics in your workload impact them.</p>"},{"location":"signals/metrics/#correlate-with-operational-metric-data","title":"Correlate with operational metric data","text":"<p>If high CPU utilization on your web server causes slow response times, which in turn makes for dissatisfied customers and ultimately lower revenue, then measuring your CPU utilization has a direct impact on your business outcomes and should absolutely be measured!</p> <p>Or conversely, if you have an application that performs batch processing on ephemeral cloud resources (such as an Amazon EC2 fleet, or similar in other cloud provider environments), then you may want to have CPU as utilized as possible in order to accomplish the most cost-effective means of completing the batch. </p> <p>In either case, you need to have your operational data (e.g. CPU utilization) be in the same system as your business metrics so you can correlate the two. </p> <p>Success</p> <p>Store your business metrics and operational metrics in a system where you can correlate them together and draw conclusions based on observed impacts to both.</p>"},{"location":"signals/metrics/#know-what-good-looks-like","title":"Know what good looks like!","text":"<p>Understanding what a healthy baseline is can be challenging. Many people have to stress test their workloads to understand what healthy metrics look like. However, depending on your needs you may be able to observe existing operational metrics to draw safe conclusions about healthy thresholds.</p> <p>A healthy workload is one that has a balance of meeting your KPI objectives while remaining resilient, available, and cost-effective.</p> <p>Success</p> <p>Your KPIs must have an identified healthy range so you can create alarms when performance falls below, or above, what is required.</p>"},{"location":"signals/metrics/#use-anomaly-detection-algorithms","title":"Use anomaly detection algorithms","text":"<p>The challenge with knowing what good looks like is that it may be impractical to know the healthy thresholds for every metric in your system. A Relational Database Management System(RDBMS) can emit dozens of performance metrics, and when coupled with a microservices architecture you can potentially have hundreds of metrics that can impact your KPIs.</p> <p>Watching such a large number of datapoints and individually identifying their upper and lower thresholds may not always be practical for humans to do. But machine learning is very good at this sort of repetitive task. Leverage automation and machine learning wherever possible as it can help identify issues that you would otherwise not even know about!</p> <p>Success</p> <p>Use machine learning algorithms and anomaly detection models to automatically calculate your workload's performance thresholds. </p>"},{"location":"signals/traces/","title":"Traces","text":"<p>Traces represent an entire journey of the requests as they traverse through different components of an application. </p> <p>Unlike logs or metrics, traces are composed of events from more than one application or a service, and with context about the connection between services such as response latency, service faults, request parameters, and metadata.</p> <p>Tip</p> <p>There is conceptual similarity between logs and traces, however a trace is intended to be considered in a cross-service context, whereas logs are typically limited to the execution of a single service or application.</p> <p>Today's developers are leaning towards building modular and distributed applications. Some call these Service Oriented Architecture, others will refer to them as microservices. Regardless of the name, when something goes wrong in these loosely coupled applications, just looking at logs or events may not be sufficient to track down the root cause of an incident.  Having full visibility into request flow is essential and this is where traces add value. Through a series of causally related events that depict end-to-end request flow, traces help  you gain that visibility.</p> <p>Traces are an essential pillar of observability because they provide the basic information on the flow of the request as it comes and leaves the system.</p> <p>Tip</p> <p>Common use cases for traces include performance profiling, debugging production issues, and root cause analysis of failures.</p>"},{"location":"signals/traces/#instrument-all-of-your-integration-points","title":"Instrument all of your integration points","text":"<p>When all of your workload functionality and code is at one place, it is easy to look at the source code to see how a request is passed across different functions. At a system level you know which machine the app is running and if something goes wrong, you can find the root cause quickly. Imagine doing that in a microservices-based architecture where different components are loosely coupled and are running in an distributed environment. Logging into numerous systems to see their logs from each interconnected request would be impractical, if not impossible.</p> <p>This is where observability can help. Instrumentation is a key step towards increasing that observability. In broader terms Instrumentation is measuring the events in your application using code.</p> <p>A typical instrumentation approach would be to assign a unique trace identifier for each request entering the system and carry that trace id as it passes through different components while adding additional metadata.</p> <p>Success</p> <p>Every connection from one service to another should be instrumented to emit traces to a central collector. This approach helps you see into otherwise opaque aspects of your workload.</p> <p>Success</p> <p>Instrumenting your application can be a largely automated process when using an auto-instrumentation agent or library.</p>"},{"location":"signals/traces/#transaction-time-and-status-matters-so-measure-it","title":"Transaction time and status matters, so measure it!","text":"<p>A well instrumented application can produce end to end trace, which can be viewed aseither a waterfall graph like this:</p> <p></p> <p>Or a service map:</p> <p></p> <p>It is important that you measure the transaction times and response codes to every interaction. This will help in calculating the overall processing times and track it for compliance with your SLAs, SLOs, or business KPIs.</p> <p>Success</p> <p>Only by understanding and recording the response times and status codes of your interactions can you see the contributing factors overall request patterns and workload health.</p>"},{"location":"signals/traces/#metadata-annotations-and-labels-are-your-best-friend","title":"Metadata, annotations, and labels are your best friend","text":"<p>Traces are persisted and assigned a unique ID, with each trace broken down into spans or segments (depending on your tooling) that record each step within the request\u2019s path. A span indicates the entities with which the trace interacts, and, like the parent trace, each span is assigned a unique ID and time stamp and can include additional data and metadata as well. This information is useful for debugging because it gives you the exact time and location a problem occurred.</p> <p>This is best explained through a practical example. An e-commerce application may be divided into many domains: authentication, authorizatino, shipping, inventory, payment processing, fulfillment, product search, recommendations, and many more. Rather than search through traces from all of these interconnected domains though, labelling your trace with a customer ID allows you to search for only interactions that are specific to this one person. This helps you to narrow your search instantly when diagnosing an operational issue.</p> <p>Success</p> <p>While the naming convention may vary between vendors, each trace can be augmented with metadata, labels, or annotations, and these are searchable across your entire workload. Adding them does require code on your part, but greatly increases the observability of your workload. </p> <p>Warning</p> <p>Traces are not logs, so be frugal with what metadata you include in your traces. And trace data is not intended for forensics and auditing, even with a high sample rate.</p>"},{"location":"tools/adot-traces/","title":"Tracing with ADOT","text":"<p>todo</p>"},{"location":"tools/alarms/","title":"Alarms","text":"<p>Amazon CloudWatch alarms allows you to define thresholds around CloudWatch Metrics and Logs and receive notifications based on the rules configured in the CloudWatch.  </p> <p>Alarms on CloudWatch metrics:</p> <p>CloudWatch alarms allows you to define thresholds on CloudWatch metrics and receive notifications when the metrics fall outside range. Each metric can trigger multiple alarms, and each alarm can have many actions associated with it. There are two different ways you could setup metric alarms based on CloudWatch metrics.</p> <ol> <li> <p>Static threshold: A static threshold represents a hard limit that the metric should not violate. You must define the range for the static threshold like upper limit and the lower limit to understand the behaviour during the normal operations.  If the metric value falls below or above the static threshold you may configure the CloudWatch to generate the alarm.</p> </li> <li> <p>Anomaly detection: Anomaly detection is generally identified as rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well-defined notion of normal behaviour.  CloudWatch anomaly detection analyzes past metric data and creates a model of expected values. The expected values take into account the typical hourly, daily, and weekly patterns in the metric.  You can apply the anomaly detection for each metric as required and CloudWatch applies a machine-learning algorithm to define the upper limit and lower limit for each of the enabled metrics and generate an alarm only when the metrics fall out of the expected values. </p> </li> </ol> <p>Tip</p> <p>Static thresholds are best used for metrics that you have a firm understanding of, such as identified performance breakpoints in your workload, or absolute limits on infrastructure components.</p> <p>Success</p> <p>Use an anomaly detection model with your alarms when you do not have visibility into the performance of a particular metric over time, or when the metric value has not been observed under load-testing or anomalous traffic previously.</p> <p></p> <p>You can follow the instructions below on how to setup of Static and Anomaly based alarms in CloudWatch.</p> <p>Static threshold alarms</p> <p>CloudWatch anomaly Detection based alarms</p> <p>Success</p> <p>To reduce the alarm fatigue or reduce the noise from the number of alarms generated, you have two advanced methods to configure the alarms:</p> <ol> <li> <p>Composite alarms: A composite alarm includes a rule expression that takes into account the alarm states of other alarms that have been created. The composite alarm goes into <code>ALARM</code> state only if all conditions of the rule are met. The alarms specified in a composite alarm's rule expression can include metric alarms and other composite alarms. Composite alarms help to fight alarm fatigue with aggregation.</p> </li> <li> <p>Metric math based alarms: Metric math expressions can be used to build more meaningful KPIs and alarms on them. You can combine multiple metrics and create a combined utilization metric and alarm on them.</p> </li> </ol> <p>These instructions below guide you on how to setup of Composite alarms and Metric math based alarms.</p> <p>Composite Alarms</p> <p>Metric Math alarms</p> <p>Alarms on CloudWatch Logs</p> <p>You can create alarms based on the CloudWatch Logs uses CloudWatch Metric filter. Metric filters turn the log data into numerical CloudWatch metrics that you can graph or set an alarm on. Once you have setup the metrics you could use either the static or anomaly based alarms on the CloudWatch metrics generated from the CloudWatch Logs.</p> <p>You can find an example on how to setup metric filter on CloudWatch logs.</p>"},{"location":"tools/alerting_and_incident_management/","title":"Alerting and incident management","text":""},{"location":"tools/amp/","title":"Amazon Managed Service for Prometheus","text":"<p>Prometheus is a popular open source monitoring tool that provides wide ranging metrics capabilities and insights about resources such as compute nodes and application related performance data. </p> <p>Prometheus uses a pull model to collect data, where as CloudWatch uses a push model. Prometheus and CloudWatch are used for some overlapping use cases, though their operating models are very different and are priced differently.</p> <p>Amazon Managed Service for Prometheus is widely used in containerized applications hosted in Kubernetes and Amazon ECS.</p> <p>You can add Prometheus metric capabilities on your EC2 instance or ECS/EKS cluster using the CloudWatch agent or AWS Distro for OpenTelemetry. The CloudWatch agent with Prometheus support discovers and collects Prometheus metrics to monitor, troubleshoot, and alarm on application performance degradation and failures faster. This also reduces the number of monitoring tools required to improve observability.</p> <p>CloudWatch Container Insights monitoring for Prometheus automates the discovery of Prometheus metrics from containerized systems and workloads https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ ContainerInsights-Prometheus.html</p>"},{"location":"tools/cloudwatch_agent/","title":"CloudWatch Agent","text":""},{"location":"tools/cloudwatch_agent/#deploying-the-cloudwatch-agent","title":"Deploying the CloudWatch agent","text":"<p>The CloudWatch agent can be deployed as a single installation, using a distributed configuration file, layering multiple configuration files, and entirely though automation. Which approach is appropriate for you depends on your needs. 1</p> <p>Success</p> <p>Deployment to Windows and Linux hosts both have the capability to store and retrieve their configurations into Systems Manager Parameter Store. Treating the deployment of CloudWatch agent configuration through this automated mechanism is a best practice. </p> <p>Tip</p> <p>Alternatively, the configuration files for the CloudWatch agent can be deployed through the automation tool of your choice (Ansible, Puppet, etc.). The use of Systems Manager Parameter Store is not required, though it does simplify management.</p>"},{"location":"tools/cloudwatch_agent/#deployment-outside-of-aws","title":"Deployment outside of AWS","text":"<p>The use of the CloudWatch agent is not limited to within AWS, and is supported both on-premises and in other cloud environments. There are two additional considerations that must be heeded when using the CloudWatch agent outside of AWS though:</p> <ol> <li>Setting up IAM credentials2 to allow agent to make required API calls. Even in EC2 there is no unauthenticated access to the CloudWatch APIs5.</li> <li>Ensure agent has connectivity to CloudWatch, CloudWatch Logs, and other AWS endpoints3 using a route that meets your requirements. This can be either through the Internet, using AWS Direct Connect, or through a private endpoint (typically called a VPC endpoint).</li> </ol> <p>Info</p> <p>Transport between your environment(s) and CloudWatch needs to match your governance and security requirements. Broadly speaking, using private endpoints for workloads outside of AWS meets the needs of customers in even the most strictly regulated industries. However, the majority of customers will be served well through our public endpoints.</p>"},{"location":"tools/cloudwatch_agent/#use-of-private-endpoints","title":"Use of private endpoints","text":"<p>In order to push metrics and logs, the CloudWatch agent must have connectivity to the CloudWatch, and CloudWatch Logs endpoints. There are several ways to achieve this based on where the agent is installed.</p>"},{"location":"tools/cloudwatch_agent/#from-a-vpc","title":"From a VPC","text":"<p>a. You can make use of VPC Endpoints (for CloudWatch and CloudWatch Logs) in order to establish fully private and secure connection between your VPC and CloudWatch for the agent running on EC2. With this approach, agent traffic never traverses the internet.</p> <p>b. Another alternative is to have a public NAT gateway through which private subnets can connect to the internet, but cannot receive unsolicited inbound connections from the internet. </p> <p>Note</p> <p>Please note with this approach agent traffic will be logically routed via internet.</p> <p>c. If you don\u2019t have requirement to establish private or secure connectivity beyond the existing TLS and Sigv4 mechanisms, the easiest option is to have Internet Gateway to provide connectivity to our endpoints.</p>"},{"location":"tools/cloudwatch_agent/#from-on-premises-or-other-cloud-environments","title":"From on-premises or other cloud environments","text":"<p>a. Agents running outside of AWS can establish connectivity to CloudWatch public endpoints over the internet(via their own network setup) or Direct Connect Public VIF.</p> <p>b. If you require that agent traffic not route through the internet you can leverage VPC Interface endpoints, powered by AWS PrivateLink, to extend the private connectivity all the way to your on-premises network using Direct Connect Private VIF or VPN. Your traffic is not exposed to the internet, eliminating threat vectors. </p> <p>Success</p> <p>You can add ephemeral AWS access tokens for use by the CloudWatch agent by using credentials obtained from the AWS Systems Manager agent.</p> <ol> <li> <p>See Getting started with open source Amazon CloudWatch Agent for a blog that gives guidance for CloudWatch agent use and deployment.\u00a0\u21a9</p> </li> <li> <p>Guidance on setting credentials for agents running on-premises and in other cloud environments \u21a9</p> </li> <li> <p>How to verify connectivity to the CloudWatch endpoints \u21a9</p> </li> <li> <p>A blog for on-premises, private connectivity \u21a9</p> </li> <li> <p>Use of all AWS APIs related to observability is typically accomplished by an instance profile - a mechanism to grant temporary access credentials to instances and containers running in AWS.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/dashboards/","title":"Dashboards","text":"<p>Dashboards are an important part of your Observability soluution. They enable you to produce a curated visualization of your data. They enable you see a history of your data, and see it alongside other related data. They also allow you to provide context. They help you understand the bigger picture.</p> <p>Often people gather their data and create alarms, and then stop. However, alarms only show a point in time, and usually for a single metric, or small set of data. Dashboards help you see the behaviour over time.</p> <p></p>"},{"location":"tools/dashboards/#a-practical-example-consider-an-alarm-for-high-cpu","title":"A practical example: consider an alarm for high CPU","text":"<p>You know the machine is running with higher than desired CPU. Do you need to act, and how quickly? What might help you decide?</p> <ul> <li>What does normal CPU look like for this instance/application? </li> <li>Is this a spike, or a trend of increasing CPU? </li> <li>Is it impacting performance? If not, how long before it will does? </li> <li>Is this a regular occurrance? And does it usually recover on its own?</li> </ul>"},{"location":"tools/dashboards/#see-the-history-of-the-data","title":"See the history of the data","text":"<p>Now consider a dashboard, with a historic timechart of the CPU. Even with only this single metric, you can see if this is a spike, or an upward trend. You can also see how quickly it is trending upwards, and so make some decisions on the priority for action.</p>"},{"location":"tools/dashboards/#see-the-impact-on-the-workflow","title":"See the impact on the workflow","text":"<p>But what does this machine do? How important is this in our overall context? Imagine we now add a visualization of the workflow  performance, be it response time, throughput, errors, or some other measure. Now we can see if the high CPU is having an impact on the workflow or users this instance is supporting.</p>"},{"location":"tools/dashboards/#see-the-history-of-the-alarm","title":"See the history of the alarm","text":"<p>Consider adding a visualization which shows how often the alarm has triggered in the last month, and combining that with looking further back to see if this is a regular occurrance. For example, is a backup job triggering the spike? Knowing the pattern of reoccurance can help you understand the underlying issue, and make longer term decisions on how to stop the alarm reoccurring altogether.</p>"},{"location":"tools/dashboards/#add-context","title":"Add context","text":"<p>Finally, add some context to the dashboard. Include a brief description of the reason this dashboard exists, the workflow it relates to, what to do when there is an issue, links to documentation, and who to contact.</p> <p>Info</p> <p>Now we have a story, which helps the dashboard user to see what is happening, understand the impact, and make appropriate data driven decisions on what action and the urgency of it.</p>"},{"location":"tools/dashboards/#dont-try-to-visualize-everything-all-at-once","title":"Don't try to visualize everything all at once","text":"<p>We often talk about alarm fatigue. Too many alarms, without identifiable actions and priorities, can overload your team and lead to inefficiencies. Alarms should be for things which are important to you, and actionable.</p> <p>Dashboards are more flexible here. They don't demand your attention in the same way, so you have more freedom to visualize things that you may not be certain are important yet, or that support your exploration. Still, don't over do it! Everything can suffer from too much of a good thing.</p> <p>Dashboards should provide a picture of something that is important to you. In the same was as deciding what data to ingest, you need to think about what matters to you for dashboards.  For your dashboards, think about</p> <ul> <li>Who will be viewing this?<ul> <li>What is their background and knowledge? </li> <li>How much context do they need? </li> </ul> </li> <li>What questions are they trying to answer?</li> <li>What actions will they be taking as a result of seeing this data?</li> </ul> <p>Tip</p> <p>Sometimes it can be hard to know what your dashboard story should be, and how much to include. So where could you start to design your dashboard? Lets look at two ways: KPI driven, or incident driven.</p>"},{"location":"tools/dashboards/#design-your-dashboard-kpi-driven","title":"Design your dashboard: KPI driven","text":"<p>One way to understand this is to work back from your KPIs. This is usually a very user driven approach. For layout, typically we are working top down, getting to more detail as we move further down a dashboard, or navigate to lower level dashboards. </p> <p>First, understand your KPIs. What they mean. This wil help you decide how you want to visualize these.  Many KPIs are shown as a single number. For example, what percentage of customers are successfully completing a specific workflow, and in what time? But over what time period? You may well meet your KPI if you average over a week, but still have smaller periods of time within this that breach your standards. Are these breaches important to you? Do they impact your customer experience. If so, you may consider different periods and time charts to see your KPIs. And maybe not everyone needs to see the detail, so perhaps you move the breakdown of KPIs to a separate dashboard, for a separate audience.</p> <p>Next, what contribute to those KPIs? What workflows need to be running in order for those actions to happen? Can you measure these?</p> <p>Identify the main components and add visualizations of their performance. When a KPI breeches, you should be able to quickly look and see where in the workflow the main impact is.</p> <p>And you can keep going down - what impacts the perfomance of those workflows? Remember your audience as you decide the level of depth. </p> <p>Consider the example of an e-commerce system with a KPI for the number of order placed. For an order to be placed, users must be able to perform the following action: search for products, add them to their cart, add their delivery details, and pay for the order. For each of these workflows, you might consider checking key components are functioning. For example by using RUM or Synthetics to get data on action success and see if the user is being impacted by an issue. You might consider a measurement of throughput, latency, failed action percentages to see if the performance of each action is as expected. You might consider measurements of the underlying infrastructure to see what might be impacting performance.</p> <p>However, don't put all of your information on the same dashboard. Again, consider your user audience.</p> <p>Success</p> <p>Create layers of dashboards that allow drilldown and provide the right context for the right users.</p>"},{"location":"tools/dashboards/#design-your-dashboard-incident-driven","title":"Design your dashboard: Incident driven","text":"<p>For many people, incident resolution is a key driver for observability. You have been alerted to an issue, by a user, or by an Observability alarm, and you need to quickly find a fix and potentially a root cause of the issue.</p> <p>Success</p> <p>Start by looking at your recent incidents. Are there common patterns? Which were the most impactful for your company? Which ones repeat?</p> <p>In this case, we're designing a dashboard for those trying to understand the severity, identify the root cause and fix the incident.</p> <p>Think back to the specific indcident. </p> <ul> <li>How did you verify the incident was as reported?<ul> <li>What did you check? Endpoints? Errors? </li> </ul> </li> <li>How did you understand the impact, and therefore priority of the issue?</li> <li>What did you look at for cause of the issue?  </li> </ul> <p>Application Performance Monitoring (APM) can help here, with Synthetics for regular baseline and testing of endpoints and workflows, and RUM for the actual customer experience. You can use this data to quickly visualize which workflows are impacted, and by how much.</p> <p>Visualizations which show the error count over time, and the top # errors, can help you to focus on the right area, and show you specific details of errors. This is where we are often using log data, and dynamic visualizations of error codes and reasons.</p> <p>It can be very useful here to have some kind of filtering or drilldown, to get to the specifics as quickly as possible. Think about ways to implement this without too much overhead. For example, having a single dashboard which you can filter to get closer to the details.</p>"},{"location":"tools/dashboards/#layout","title":"Layout","text":"<p>The layout of your dashboard is also important. </p> <p>Success</p> <p>Typically the most significant visualizations for your user want to be top left, or otherwise aligned with a natural beginning of page navigation.</p> <p>You can use layout to help tell the story. For example, you may use a top-down layout, where the further down you scroll, the more details you see. Or perhaps a left-right display would be useful with higher level services on the left, and their dependencies as you move to the right.</p>"},{"location":"tools/dashboards/#create-dynamic-content","title":"Create dynamic content","text":"<p>Many of your workloads will be designed to grow or shrink as demand dictates, and your dashboards need to take this into account. For example you may have your instances in an autoscaling group, and when you hit a certain load, additional instances are added.</p> <p>Success</p> <p>A dashboard showing data from specific instances, specified by some kind of ID, will not allow the data from those new instances to be seen. Add metadata to your resources and data, so you can create your visualizations to capture all instances with a specific metadata value. This way they will reflect the actual state.</p> <p>Another example of dynamic visualizations might be being able to find the top 10 errors occurring now, and how they have behaved over recent history. You want to be able to see a table, or a chart, without knowledge of which errors might occur.</p>"},{"location":"tools/dashboards/#think-about-symptoms-first-over-causes","title":"Think about symptoms first over causes","text":"<p>When you observe symptoms, you are considering about the impact this has on your users and systems. Many underlying causes might give the same symptoms. This enables you to capture more issues, including unknown issues. As you understand causes, your lower level dashboards may be more specific to these to help you quickly diagnose and fix issues.</p> <p>Tip</p> <p>Don't capture the specific JavaScript error that impacted the users last week. Capture the impact on the workflow it disrupted, and then show the top count of JavaScript errors over recent history, or which have dramatically increased in recent history.</p>"},{"location":"tools/dashboards/#use-topbottom-n","title":"Use top/bottom N","text":"<p>Most of the time there is no need to visualize all of your operational metrics at the same time. A large fleet of EC2 instances is a good example of this: there is no need or value in having the disk IOPS or CPU utilization for an entire farm of hundreds of servers displayed simultaneously. This creates an anti-pattern where you can spend more time trying to dig-through your metrics than seeing the best (or worst) performing resources.</p> <p>Success</p> <p>Use your dashboards to show the ten or 20 of any given metric, and then focus on the symptoms this reveals. </p> <p>CloudWatch metrics allows you to search for the top N for any time series. For example, this query will return the busiest 20 EC2 instances by CPU utilization:</p> <pre><code>SORT(SEARCH('{AWS/EC2,InstanceId} MetricName=\"CPUUtilization\"', 'Average', 300), SUM, DESC, 10)\n</code></pre> <p>Use this approach, or similar with CloudWatch Metric Insights to identify the top or bottom performing metrics in your dashboards.</p>"},{"location":"tools/dashboards/#show-kpis-with-thresholds-visually","title":"Show KPIs with thresholds visually","text":"<p>Your KPIs should have a warning or error threshold, and dashboards can show this using a horizontal annotation. This will appear as a high water mark on a widget. Showing this visually can give human operators a forewarning if business outcomes or infrastructure are in jeopardy.</p> <p></p> <p>Success</p> <p>Horizontal annotations are a critical part of a well-developed dashboard.</p>"},{"location":"tools/dashboards/#the-importance-of-context","title":"The importance of context","text":"<p>People can easily misinterpret data. Their background and current context will colour how they view the data.</p> <p>So make sure you include text within your dashboard. What is this data for, and who? What does it mean? Link to documentation on the application, who supports it, the troubleshooting docs. You can also uses text displays to divide your dashboard display. se them on the left to set left-right context. Use them as full horizontal displays to divide your dashboard vertically.</p> <p>Success</p> <p>Having links to IT support, operations on-call, or business owners can give teams a fast path to contact people who can help support when issues occur.</p> <p>Tip</p> <p>Hyperlinks to ticketing systems is also a very useful addition for dashboards.</p>"},{"location":"tools/emf/","title":"Embedded Metric Format","text":"<p>The CloudWatch embedded metric format(EMF) is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values. With EMF, you can push the metric related data in terms of CloudWatch logs which gets discovered as metric in CloudWatch.</p> <p>Below is a sample EMG for mat encamp and JSON schema :</p> <pre><code>{\n  \"_aws\": {\n    \"Timestamp\": 1574109732004,\n    \"CloudWatchMetrics\": [\n      {\n        \"Namespace\": \"lambda-function-metrics\",\n        \"Dimensions\": [\n          [\n            \"functionVersion\"\n          ]\n        ],\n        \"Metrics\": [\n          {\n            \"Name\": \"time\",\n            \"Unit\": \"Milliseconds\"\n          }\n        ]\n      }\n    ]\n  },\n  \"functionVersion\": \"$LATEST\",\n  \"time\": 100,\n  \"requestId\": \"989ffbf8-9ace-4817-a57c-e4dd734019ee\"\n}\n</code></pre> <p>Thus, with help of EMF you can send high cardinality metrics without the need of making manual PutMetricData API calls.</p>"},{"location":"tools/internet_monitor/","title":"Internet Monitor","text":"<p>Warning</p> <p>As of this writing, Internet Monitor is available in preview in the CloudWatch console. The scope of features for general availability may change from what you experience today.</p> <p>Collecting telemetry from all tiers of your workload is a best practice, and one that can be a challenge. But what are the tiers of your workload? For some it may be web, application, and database servers. Other people might view their workload as front end and back end. And those operating web applications can use Real User Monitoring(RUM) to observe the health of these apps as experienced by end users. </p> <p>But what about the traffic between the client and the datacenter or cloud services provider? And for applications that are not served as web pages and therefore cannot use RUM?</p> <p></p> <p>Internet Monitor works at the networking level and evaluates the health of observed traffic, correlated against AWS existing knowledge of known Internet issues. In short, if there is an Internet Service Provider (ISP) that has a performance or availability issue and if your application has traffic that uses this ISP for client/server communication, then Internet Monitor can proactively inform you about this impact to your workload. Additionally, it can make recommendations to you based on your selected hosting region and use of CloudFront as a Content Delivery Network1.</p> <p>Tip</p> <p>Internet Monitor only evaluates traffic from networks that your workloads traverse. For example, if an ISP in another country is impacted, but your users do not use that carrier, then you will not have visibility into that issue.</p>"},{"location":"tools/internet_monitor/#create-monitors-for-applications-that-traverse-the-internet","title":"Create monitors for applications that traverse the Internet","text":"<p>The way that Internet Monitor operates is by watching for traffic that comes either into your CloudFront distributions or to your VPCs from impacted ISPs. This allows you to make decisions about application behaviour, routing, or user notification that helps offset business issues that arise as a result of network problems that are outside of your control.</p> <p></p> <p>Success</p> <p>Only create monitors that watch traffic which traverses the Internet. Private traffic, such as between two hosts in a private network (RFC1918) cannot be monitored using Internet Monitor.</p> <p>Success</p> <p>Prioritize traffic from mobile applications where applicable. Customers roaming between providers, or in remote geographical locations, may have different or unexpected experiences that you should be aware of.</p>"},{"location":"tools/internet_monitor/#enable-actions-through-eventbridge-and-cloudwatch","title":"Enable actions through EventBridge and CloudWatch","text":"<p>Observed issues will be published through EventBridge using a schema that contains the souce identified as <code>aws.internetmonitor</code>. EventBridge can be used to automatically create issues in your ticket management system, page your support teams, or even trigger automation that can alter your workload to mitigate some scenarios.</p> <pre><code>{\n\"source\": [\"aws.internetmonitor\"]\n}\n</code></pre> <p>Likewise, extensive details of traffic are available in CloudWatch Logs for observed cities, countries, metros, and subdivisions. This allows you to create highly-targeted actions which can notify impacted customers proactively about issues local to them. Here is an example of a country-level observation about a single provider:</p> <pre><code>{\n\"version\": 1,\n\"timestamp\": 1669659900,\n\"clientLocation\": {\n\"latitude\": 0,\n\"longitude\": 0,\n\"country\": \"United States\",\n\"subdivision\": \"\",\n\"metro\": \"\",\n\"city\": \"\",\n\"countryCode\": \"US\",\n\"subdivisionCode\": \"\",\n\"asn\": 00000,\n\"networkName\": \"MY-AWESOME-ASN\"\n},\n\"serviceLocation\": \"us-east-1\",\n\"percentageOfTotalTraffic\": 0.36,\n\"bytesIn\": 23,\n\"bytesOut\": 0,\n\"clientConnectionCount\": 0,\n\"internetHealth\": {\n\"availability\": {\n\"experienceScore\": 100,\n\"percentageOfTotalTrafficImpacted\": 0,\n\"percentageOfClientLocationImpacted\": 0\n},\n\"performance\": {\n\"experienceScore\": 100,\n\"percentageOfTotalTrafficImpacted\": 0,\n\"percentageOfClientLocationImpacted\": 0,\n\"roundTripTime\": {\n\"p50\": 71,\n\"p90\": 72,\n\"p95\": 73\n}\n}\n},\n\"trafficInsights\": {\n\"timeToFirstByte\": {\n\"currentExperience\": {\n\"serviceName\": \"VPC\",\n\"serviceLocation\": \"us-east-1\",\n\"value\": 48\n},\n\"ec2\": {\n\"serviceName\": \"EC2\",\n\"serviceLocation\": \"us-east-1\",\n\"value\": 48\n}\n}\n}\n}\n</code></pre> <p>Success</p> <p>Values such as <code>percentageOfTotalTraffic</code> can reveal powerful insights about where your customers access your workloads from and can be used for advanced analytics.</p> <p>Warning</p> <p>Note that log groups created by Internet Monitor will have a default retention period set to never expire. AWS does not delete your data without your consent, so be sure to set a retention period that makes sense for your needs.</p> <p>Success</p> <p>Each monitor will create at least 10 discrete CloudWatch metrics. These should be used for creating alarms just as you would with any other operational metric.</p>"},{"location":"tools/internet_monitor/#utilize-traffic-optimization-suggestions","title":"Utilize traffic optimization suggestions","text":"<p>Internet Monitor features traffic optimization recommendations that can advise you on where to best place your workloads so as to have the best customer experiences. For those workloads that are global, or have global customers, this feature is particularly valuable. </p> <p></p> <p>Success</p> <p>Pay close attention to the current, predicted, and lowest time-to-first-byte (TTFB) values in the traffic optimization suggestions view as these can indicate potentially poor end-user experiences that are otherwise difficult to observe.</p> <ol> <li> <p>See https://aws.amazon.com/blogs/aws/cloudwatch-internet-monitor-end-to-end-visibility-into-internet-performance-for-your-applications/ for our launch blog about this new feature.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/metrics/","title":"Metrics","text":"<p>Metrics are data about the performance of your system. Having all the metrics related to system or the resources available in a centralised place grants you the ability to compare metrics, analyse performance, and make better strategic decisions like scaling-up or scaling-in resources. Metrics are also important for the knowing the health of the resources and take proactive measures.</p> <p>Metric data is foundational and used to drive alarms, anomaly detection, events, dashboards and more.</p>"},{"location":"tools/metrics/#vended-metrics","title":"Vended metrics","text":"<p>CloudWatch metrics collects data about the performance of your systems. By default, most AWS services provide free metrics for their resources. This includes  Amazon EC2 instances, Amazon RDS, Amazon S3 buckets, and many more. </p> <p>We refer to these metrics as vended metrics. There is no charge for the collection of vended metrics in your AWS account.</p> <p>Info</p> <p>For a complete list of AWS services that emit metrics to CloudWatch see this page.</p>"},{"location":"tools/metrics/#querying-metrics","title":"Querying metrics","text":"<p>You can utilise the metric math feature in CloudWatch to query multiple metrics and use math expressions to analyse the metrics for more granularity. For example, you can write a metric math expression to find out the Lambda error rate by query as:</p> <pre><code>Errors/Requests\n</code></pre> <p>Below you see an example of how this can appear in the CloudWatch console:</p> <p></p> <p>Success</p> <p>Use metric math to get the most value from your data and derive values from the performance of separate data sources.</p> <p>CloudWatch also supports conditional statements. For example, to return a value of <code>1</code> for each timeseries where latency is over a specific threshold, and <code>0</code> for all other data points, a query would resemble this:</p> <pre><code>IF(latency&gt;threshold, 1, 0)\n</code></pre> <p>In the CloudWatch console we can use this logic to create boolean values, which in turn can trigger CloudWatch alarms or other actions. This can enable automatic actions from derived datapoints. An example from the CloudWatch console is below:</p> <p></p> <p>Success</p> <p>Use conditional statements to trigger alarms and notifications when performance exceeds thresholds for derived values. </p> <p>You can also use a <code>SEARCH</code> function to show the top <code>n</code> for any metric. When visualizing the best or worst performing metrics across a large number timeseries (e.g. thousands of servers) this approach allows you to see only the data that matters most. Here is an example of a search returning the top two CPU-consuming EC2 instances, averaged over the last five minutes:</p> <pre><code>SLICE(SORT(SEARCH('{AWS/EC2,InstanceId} MetricName=\"CPUUtilization\"', 'Average', 300), MAX, DESC),0, 2)\n</code></pre> <p>And a view of the same in the CloudWatch console:</p> <p></p> <p>Success</p> <p>Use the <code>SEARCH</code> approach to rapidly display the valuable or worst performing resources in your environment, and then display these in dashboards.</p>"},{"location":"tools/metrics/#collecting-metrics","title":"Collecting metrics","text":"<p>If you would like to have additional metrics like memory or disk space utilization for your EC2 instances, you use the CloudWatch agent to push this data to CloudWatch on your behalf. Or if you have custom processing data which needs to be visualised in graphical manner, and you want this data to be present as CloudWatch metric, then you can use <code>PutMetricData</code> API to publish custom metrics to CloudWatch.</p> <p>Success</p> <p>Use one of the AWS SDKs to push metric data to CloudWatch rather than the bare API.</p> <p><code>PutMetricData</code> API calls are charges on number of queries. The best practise to use the <code>PutMetricData</code> API optimally. Using the Values and Counts method in this API, enables you to publish up to 150 values per metric with one <code>PutMetricData</code> request, and supports retrieving percentile statistics on this data. Thus, instead of making separate API calls for each of the datapoint, you should group all your datapoints together and then push to CloudWatch in a single <code>PutMetricData</code> API call. This approach benefits the user in two ways:</p> <ol> <li>CloudWatch pricing</li> <li><code>PutMetricData</code> API throttling can be prevented</li> </ol> <p>Success</p> <p>When using <code>PutMetricData</code>, the best practice is to batch your data into single <code>PUT</code> operations whenever possible.</p> <p>Success</p> <p>If large volumes of metrics are emitted into CloudWatch then consider using Embedded Metric Format as an alternative approach. Note that Embedded Metric Format does not use, nor charge, for the use of <code>PutMetricData</code>, though it does incur billing from the use of CloudWatch Logs.</p>"},{"location":"tools/metrics/#anomaly-detection","title":"Anomaly detection","text":"<p>CloudWatch has an anomaly detection feature that augments your observability strategy by learning what normal is based on recorded metrics. The use of anomaly detection is a best practice for any metric signal collection system.</p> <p>Anomaly detection builds a model over a two-week period of time. </p> <p>Warning</p> <p>Anomaly detection only builds its model from the time of creation forward. It does not project backwards in time to find previous outliers.</p> <p>Warning</p> <p>Anomaly detection does not know what good is for a metric, only what normal is based on standard deviation.</p> <p>Success</p> <p>The best practice is to train your anomaly detection models to only analyze the times of day that normal behaviour is expected. You can define time periods to exclude from training (such as nights, weekends, or holidays). </p> <p>An example of an anomaly detection band can be seen here, with the band in grey.</p> <p></p> <p>Setting exclusion windows for anomaly detection can be done with the CloudWatch console, CloudFormation, or using one of the AWS SDKs.</p>"},{"location":"tools/rum/","title":"Real User Monitoring","text":"<p>With CloudWatch RUM, you can perform real user monitoring to collect and view client-side data about your web application performance from actual user sessions in near real time. The data that you can visualize and analyze includes page load times, client-side errors, and user behavior. When you view this data, you can see it all aggregated together, and also see breakdowns by the browsers and devices that your customers use.</p> <p></p>"},{"location":"tools/rum/#web-client","title":"Web client","text":"<p>The CloudWatch RUM web client is developed and built using Node.js version 16 or higher. The code is publicly available on GitHub. You can use the client with Angular and React applications.</p> <p>CloudWatch RUM is designed to create no perceptible impact to your application\u2019s load time, performance, and unload time.</p> <p>Note</p> <p>End user data that you collect for CloudWatch RUM is retained for 30 days and then automatically deleted. If you want to keep the RUM events for a longer time, you can choose to have the app monitor send copies of the events to CloudWatch Logs in your account.</p> <p>Tip</p> <p>If avoiding potential interruption by ad blockers is a concern for your web application then you may wish to host the web client on your own content delivery network, or even inside your own web site. Our documentation on GitHub provides guidance on hosting the web client from your own origin domain.</p>"},{"location":"tools/rum/#authorize-your-application","title":"Authorize Your Application","text":"<p>To use CloudWatch RUM, your application must have authorization through one of three options.</p> <ol> <li>Use authentication from an existing identity provider that you have already set up.</li> <li>Use an existing Amazon Cognito identity pool</li> <li>Let CloudWatch RUM create a new Amazon Cognito identity pool for the application</li> </ol> <p>Success</p> <p>Letting CloudWatch RUM create a new Amazon Cognito identity pool for the application requires the least effort to set up. It's the default option.</p> <p>Tip</p> <p>CloudWatch RUM can configured to separate unauthenticated users from authenticated users. See this blog post for details. </p>"},{"location":"tools/rum/#data-protection-privacy","title":"Data Protection &amp; Privacy","text":"<p>The CloudWatch RUM client can use cookies to help collect end user data. This is useful for the user journey feature, but is not required. See our detailed documentation for privacy related information.1</p> <p>Tip</p> <p>While the collection of web application telemetry using RUM is safe and does not expose personally identifiable information (PII) to you through the console or CloudWatch Logs, be mindful that you can collect custom attribute through the web client. Be careful not to expose sensitive data using this mechanism.</p>"},{"location":"tools/rum/#client-code-snippet","title":"Client Code Snippet","text":"<p>While the code snippet for the CloudWatch RUM web client will be automatically generated, you can also manually modify the code snippet to configure the client to your requirements. </p> <p>Success</p> <p>Use a cookie consent mechanism to dynamically enable cookie creation in singe page applications. See this blog post for more information.</p>"},{"location":"tools/rum/#disable-url-collection","title":"Disable URL Collection","text":"<p>Prevent the collection of resource URLs that might contain personal information.</p> <p>Success</p> <p>If your application uses URLs that contain personally identifiable information (PII), we strongly recommend that you disable the collection of resource URLs by setting <code>recordResourceUrl: false</code> in the code snippet configuration, before inserting it into your application.</p>"},{"location":"tools/rum/#enable-active-tracing","title":"Enable Active Tracing","text":"<p>Enable end-to-end tracing by setting <code>addXRayTraceIdHeader: true</code> in the web client. This causes the CloudWatch RUM web client to add an X-Ray trace header to HTTP requests.</p> <p>If you enable this optional setting, XMLHttpRequest and fetch requests made during user sessions sampled by the app monitor are traced. You can then see traces and segments from these user sessions in the RUM dashboard, the CloudWatch ServiceLens console, and the X-Ray console. </p> <p>Click the checkbox to enable active tracing when setting up your application monitor in the AWS Console to have the setting automatically enabled in your code snippet.</p> <p></p>"},{"location":"tools/rum/#inserting-the-snippet","title":"Inserting the Snippet","text":"<p>Insert the code snippet that you copied or downloaded in the previous section inside the <code>&lt;head&gt;</code> element of your application. Insert it before the <code>&lt;body&gt;</code> element or any other <code>&lt;script&gt;</code> tags.</p> <p>Success</p> <p>If your application has multiple pages, insert the code snippet in a shared header component that is included in all pages.</p> <p>Warning</p> <p>It is critical that the web client be as early in the <code>&lt;head&gt;</code> element as possible! Unlike passive web trackers that are loaded near the bottom of a page's HTML, for RUM to capture the most performance data requires it be instantiated early in the page render process.</p>"},{"location":"tools/rum/#use-custom-metadata","title":"Use Custom Metadata","text":"<p>You can add custom metadata to the CloudWatch RUM events default event metadata. Session attributes are added to all events in a user's session. Page attributes are added only to the pages specified.</p> <p>Success</p> <p>Avoid using reserved keywords noted on this page as key names for your custom attributes</p>"},{"location":"tools/rum/#use-page-groups","title":"Use Page Groups","text":"<p>Success</p> <p>Use page groups to associate different pages in your application with each other so that you can see aggregated analytics for groups of pages. For example, you might want to see the aggregated page load times of all of your pages by type and language.</p> <p><code>awsRum.recordPageView({ pageId: '/home', pageTags: ['en', 'landing']})</code></p>"},{"location":"tools/rum/#use-extended-metrics","title":"Use Extended Metrics","text":"<p>There is a default set of metrics automatically collected by CloudWatch RUM that are published in the metric namespace named <code>AWS/RUM</code>. These are free, vended metrics that RUM creates on your behalf.</p> <p>Success</p> <p>Send any of the CloudWatch RUM metrics to CloudWatch with additional dimensions so that the metrics give you a more fine-grained view.</p> <p>The following dimensions are supported for extended metrics:</p> <ul> <li>BrowserName</li> <li>CountryCode - ISO-3166 format (two-letter code)</li> <li>DeviceType</li> <li>FileType</li> <li>OSName</li> <li>PageId</li> </ul> <p>However, you can create your own metrics and alarms based on them using our guidance from this page. This approach allows you to monitor performance for any datapoint, URI, or other component that you need.</p> <ol> <li> <p>See our blog post discussing the considerations when using cookies with CloudWatch RUM.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/synthetics/","title":"Synthetic testing","text":"<p>Amazon CloudWatch Synthetics allows you to monitor applications from the perspective of your customer, even in the absence of actual users. By continuously testing your APIs and website experiences, you can gain visibility into intermittent issues that occur even when there is no user traffic.</p> <p>Canaries are configurable scripts, that you can run on a schedule to continually test your APIs and website experiences 24x7. They follow the same code paths and network routes as real-users, and can notify you of unexpected behavior including latency, page load errors, broken or dead links, and broken user workflows.</p> <p></p> <p>Important</p> <p>Ensure that you use Synthetics canaries to monitor only endpoints and APIs where you have ownership or permissions. Depending on the canary frequency settings, these endpoints might experience increased traffic.</p>"},{"location":"tools/synthetics/#getting-started","title":"Getting Started","text":""},{"location":"tools/synthetics/#full-coverage","title":"Full Coverage","text":"<p>Tip</p> <p>When developing your testing strategy, consider both public and private internal endpoints within your Amazon VPC.</p>"},{"location":"tools/synthetics/#recording-new-canaries","title":"Recording New Canaries","text":"<p>The CloudWatch Synthetics Recorder Chrome browser plugin allows you to quickly build new canary test scripts with complex workflows from scratch. The type and click actions taken during recording are converted into a Node.js script that you can use to create a canary. The known limitations of the CloudWatch Synthetics Recorder are noted on this page.</p>"},{"location":"tools/synthetics/#viewing-aggregate-metrics","title":"Viewing Aggregate Metrics","text":"<p>Take advantage of the out-of-the-box reporting on aggregate metrics collected from your fleet of canary scripts. CloudWatch Automatic Dashboard</p> <p></p>"},{"location":"tools/synthetics/#building-canaries","title":"Building Canaries","text":""},{"location":"tools/synthetics/#blueprints","title":"Blueprints","text":"<p>Use canary blueprints to simplify the setup process for multiple canary types.</p> <p></p> <p>Info</p> <p>Blueprints are a convenient way to start writing canaries and simple use cases can be covered with no code.</p>"},{"location":"tools/synthetics/#maintainability","title":"Maintainability","text":"<p>When you write your own canaries, they are tied to a runtime version. This will be a specific version of either Python with Selenium, or JavaScript with Puppeteer. See [this page] for a list of our currently-supported runtime versions and those that are deprecated. </p> <p>Success</p> <p>Improve the maintainability of your scripts by using environment variables to share data that can be accessed during the canary's execution.</p> <p>Success</p> <p>Upgrade your canaries to the latest runtime version when available. </p>"},{"location":"tools/synthetics/#string-secrets","title":"String Secrets","text":"<p>You can code your canaries to pull secrets (such as login credentials) from a secure system outside of your canary or its environment variables. Any system that can be reached by AWS Lambda can potentially provide secrets to your canaries at runtime.</p> <p>Success</p> <p>Execute your tests and secure sensitive data by storing secrets like database connection details, API keys, and application credentials using AWS Secrets Manager.</p>"},{"location":"tools/synthetics/#managing-canaries-at-scale","title":"Managing Canaries at Scale","text":""},{"location":"tools/synthetics/#check-for-broken-links","title":"Check for Broken Links","text":"<p>Success</p> <p>If your website contains a high-volume of dynamic content and links, you can use CloudWatch Synthetics to crawl your website, detect broken links, and find the reason for failure. Then use a failure threshold to optionally create a CloudWatch Alarm when a failure threshold has been violated.</p>"},{"location":"tools/synthetics/#multiple-heartbeat-urls","title":"Multiple Heartbeat URLs","text":"<p>Success</p> <p>Simplify your testing and optimize costs by batching multiple URLs in a single heartbeat monitoring canary test. You can then see the status, duration, associated screenshots, and failure reason for each URL in the step summary of the canary run report.</p>"},{"location":"tools/synthetics/#organize-in-groups","title":"Organize in Groups","text":"<p>Success</p> <p>Organize and track your canaries in groups to view aggregated metrics and more easily isolate and drill in to failures.</p> <p></p> <p>Warning</p> <p>Note that groups will require the exact name of the canary if you are creating a cross-region group.</p>"},{"location":"tools/synthetics/#runtime-options","title":"Runtime Options","text":""},{"location":"tools/synthetics/#versions-and-support","title":"Versions and Support","text":"<p>CloudWatch Synthetics currently supports runtimes that use Node.js for scripts and the Puppeteer framework, and runtimes that use Python for scripting and Selenium WebDriver for the framework.</p> <p>Success</p> <p>Always use the most recent runtime version for your canaries, to be able to use the latest features and updates made to the Synthetics library.</p> <p>CloudWatch Synthetics notifies you by email if you have canaries that use runtimes that are scheduled to be deprecated in the next 60 days.</p>"},{"location":"tools/synthetics/#code-samples","title":"Code Samples","text":"<p>Get started with code samples for both Node.js and Puppeteer and Python and Selenium.</p>"},{"location":"tools/synthetics/#import-for-selenium","title":"Import for Selenium","text":"<p>Create canaries in Python and Selenium from scratch or by importing existing scripts with minimal changes.</p>"},{"location":"tools/xray/","title":"AWS X-Ray","text":""},{"location":"tools/xray/#sampling-rules","title":"Sampling rules","text":"<p>Sampling rules using X-Ray can be configured in the AWS Console, through local configuration file, or both. The local configuration will override those set in the console. </p> <p>Success</p> <p>Use the X-Ray console, API, or CloudFormation whenever possible. This allows you to change the sampling behaviour of an application at runtime.</p> <p>You can set sample rates separately for each of these criteria:</p> <ul> <li>Service name (e.g. billing, payments)</li> <li>Service type (e.g. EC2, Container)</li> <li>HTTP method</li> <li>URL path</li> <li>Resource ARN</li> <li>Host (e.g. www.example.com)</li> </ul> <p>The best practice is to set a sample rate that collects enough data to diagnose issues and understand performance profiles, while not collecting so much data that it is unmanageable. For example, sampling 1% of traffic to a landing page, but 10% of requests to a payment page, would align well with a strong observability practice.</p> <p>Some transactions you may wish to capture 100% of. Be cautious though as traces are not intended for forensic audits of access to your workload!</p> <p>Warning</p> <p>As traces are not intended to be used for auditing or forensic analysis, avoid sample rates of 100%. This can set a false expectation that X-Ray (by default using a UDP emitter) will never lose a transaction trace.</p> <p>As a rule, capturing transaction traces should never create an onerous load on your staff, or your AWS bill. Add traces to your environment slowly while you learn the volume of data that your workload emits.</p> <p>Info</p> <p>By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p> <p>Success</p> <p>Always set a reservoir size that you can tolerate. The reservoir size determines the maximum number of requests per second that you will capture. This protects you from malicious attack, unwanted charges, and configuration errors.</p>"},{"location":"tools/xray/#daemon-configuration","title":"Daemon configuration","text":"<p>The X-Ray daemon is intended to offload the effort of sending telemetry to the X-Ray dataplane for analysis. As such, it should not consume too many resources on the server, container, or instance on which the source application runs.</p> <p>Success</p> <p>The best practice is to run the X-Ray daemon on another instance or container, thereby enforcing the separation of concerns and allowing your source system to be unencumbered. </p> <p>Success</p> <p>In a container orchestration pattern, such as Kubernetes, operating your X-Ray daemon as a sidecar is a common practice.</p> <p>The daemon has safe default settings and can operate in EC2, ECS, EKS, or Fargate environments without futher configuration in most instances. For hybrid and other cloud environments though, you may with to adjust the <code>Endpoint</code> to reflect a VPC endpoint if you are using a Direct Connect or VPN to integrate your remote environments.</p> <p>Tip</p> <p>If you must run the X-Ray daemon on the same instance or virtual machine as the source application, consider setting the <code>TotalBufferSizeMB</code> setting to ensure X-ray does not consume more system resources than you can afford.</p>"},{"location":"tools/xray/#annotations","title":"Annotations","text":"<p>AWS X-Ray supports arbitrary metadata to be sent along with your traces. These are called annotations. They are a powerful feature that allows you to group your traces logically. Annotations are indexed as well, making for an easy way to find traces that pertain to a single entity.</p> <p>When you use auto-instrumentation SDKs for X-Ray, annotations may not appear automatically. You need to add them to your code, which greatly enriches your traces and creates ways for you to generate X-Ray Insights, metrics based off of your annotations, alarms and anomaly detection models from your system behaviour, and automate ticketing and remediation when a component impacting your users is observed.</p> <p>Success</p> <p>Use annotations to understand the flow of data in your environment.</p> <p>Success</p> <p>Create alarms based on the performance and results of your annotated traces.</p>"},{"location":"tools/logs/","title":"Logging","text":"<p>The selection of logging tools is tied to your requirements for data transmission, filtering, retention, capture, and integration with the applications that generate your data. When using Amazon Web Services for observability (regardless whether you host on-premises or in another cloud environment), you can leverage the CloudWatch agent or another tool such as Fluentd to emit logging data for analysis. </p> <p>Here we will expand on the best practices for implementing the CloudWatch agent for logging, and the use of CloudWatch Logs within the AWS console or APIs.</p> <p>Info</p> <p>The CloudWatch agent can also be used for delivery of metric data to CloudWatch. See the metrics page for implementation details.</p>"},{"location":"tools/logs/#collecting-logs-with-the-cloudwatch-agent","title":"Collecting logs with the CloudWatch agent","text":""},{"location":"tools/logs/#forwarding","title":"Forwarding","text":"<p>When taking a cloud first approach to observability, as a rule, if you need to log into a machine to get its logs, you then have an anti-pattern. Your workloads should emit their logging data outside of their confines in near real time to a log analysis system, and latency between that transmission and the original event represents a potential loss of point-in-time information should a disaster befall your workload.</p> <p>As an architect you will have to determine what your acceptable loss for logging data is and adjust the CloudWatch agent's <code>force_flush_interval</code> to accommodate this.</p> <p>The <code>force_flush_interval</code> instructs the agent to send logging data to the data plane at a regular cadence, unless the buffer size is reached, in which case it will send all buffered logs immediately.</p> <p>Tip</p> <p>Edge devices may have very different requirements from low-latency, in-AWS workloads, and may need to have much longer <code>force_flush_interval</code> settings. For example, an IoT device on a low-bandwidth Internet connection may only need to flush logs every 15 minutes. </p> <p>Success</p> <p>Containerized or stateless workloads may be especially sensitive to log flush requirements. Consider a stateless Kubernetes application or EC2 fleet that can be scaled-in at any moment. Loss of logs may take place when these resources are suddenly terminated, leaving no way to extract logs from them in the future. The standard <code>force_flush_interval</code> is usually appropriate for these scenarios, but can be lowered if required.</p>"},{"location":"tools/logs/#log-groups","title":"Log groups","text":"<p>Within CloudWatch Logs, each collection of logs that logically applies to an application should be delivered to a single log group. Within that log group you want to have commonality among the source systems that create the log streams within.</p> <p>Consider a LAMP stack: the logs from Apache, MySQL, your PHP application, and hosting Linux operating system would each belong to a separate log group.</p> <p>This grouping is vital as it allows you to treat groups with the same retention period, encryption key, metric filters, subscription filters, and Contributor Insights rules.</p> <p>Success</p> <p>There is no limitation on the number of log streams in a log group, and you can search through the entire compliment of logs for your application in a single CloudWatch Logs Insights query. Having a separate log stream for each pod in a Kubernetes service, or for every EC2 instance in your fleet, is a standard pattern.</p> <p>Success</p> <p>The default retention period for a log group is indefinite. The best practice is to set the retention period at the time of creating the log group. </p> <p>While you can set this in the CloudWatch console at any time, the best practice is to do so either in-tandem with the log group creation using infrastructure as code (CloudFormation, Cloud Development Kit, etc.) or using the <code>retention_in_days</code> setting inside of the CloudWatch agent configuration. </p> <p>Either approach lets you set the log retention period proactively, and aligned with your project's data retention requirements.</p> <p>Success</p> <p>By default, your log groups will not be encrypted. The best practice is to set the encryption key at the time you create the log group, so as to prevent accidental leak of plaintext data. This can be done using infrastructure as code (CloudFormation, Cloud Development Kit, etc.). </p> <p>Using AWS Key Management Service to manage keys for CloudWatch Logs requires additional configuration and granting permissions to the keys for your users.1 </p>"},{"location":"tools/logs/#log-formatting","title":"Log formatting","text":"<p>CloudWatch Logs has the ability to index JSON data on ingestion and use this index for ad hoc queries. While any kind of logging data can be delivered to CloudWatch Logs, the automatic indexing of this data will not take place unless it is so structured. </p> <p>Unstructured logs can still be search, though only using a regular expression.</p> <p>Success</p> <p>There two best practices for log formats when using CloudWatch Logs:</p> <ol> <li>Use a structured log formatter such as Log4j, <code>python-json-logger</code>, or your framework's native JSON emitter. </li> <li>Send a single line of logging per event to your log destination.</li> </ol> <p>Note that when sending multiple lines of JSON logging, each line will be interpreted as a single event.</p>"},{"location":"tools/logs/#handling-stdout","title":"Handling <code>stdout</code>","text":"<p>As discussed in our log signals page, the best practice is to decouple logging systems from their generating applications. However to send data from <code>stdout</code> to a file is a common pattern for many (if not most) platforms. Container orchestration systems such as Kubernetes or Amazon Elastic Container Service manage this delivery of <code>stdout</code> to a log file automatically, allowing for collection of each log from a collector. The CloudWatch agent then reads this file in real time and forwards the data to a log group on your behalf.</p> <p>Success</p> <p>Use the pattern of simplified application logging to <code>stdout</code>, with collection by an agent, as much as possible.</p>"},{"location":"tools/logs/#filtering-logs","title":"Filtering logs","text":"<p>There are many reasons to filter your logs such as preventing the persistent storage of personal data, or only capturing data that is of a particular log level. In any event, the best practice is to perform this filtering as close to the originating system as possible. In the case of CloudWatch, this will mean before data is delivered into CloudWatch Logs for analysis. The CloudWatch agent can perform this filtering for you.</p> <p>Success</p> <p>Use the <code>filters</code> feature to <code>include</code> log levels that you want and <code>exclude</code> patterns that are known not to be desirable, e.g. credit card numbers, phone numbers, etc.</p> <p>Tip</p> <p>Filtering out certain forms of known data that can potentially leak into your logs can be time-consuming and error prone. However, for workloads that handle specific types of known undesirable data (e.g. credit card numbers, Social Security numbers), having a filter for these records can prevent a potentially damaging compliance issue in the future. For example, dropping all records that contain a Social Security number can be as simple as this configuration:</p> <p><code>\"filters\": [   {     \"type\": \"exclude\",     \"expression\": \"\\b(?!000|666|9\\d{2})([0-8]\\d{2}|7([0-6]\\d))([-]?|\\s{1})(?!00)\\d\\d\\2(?!0000)\\d{4}\\b\"   } ]</code></p>"},{"location":"tools/logs/#multi-line-logging","title":"Multi-line logging","text":"<p>The best practice for all logging is to use structured logging with a single line emitted for every discrete log event. However, there are many legacy and ISV-supported applications that do not have this option. For these workloads, CloudWatch Logs will interpret each line as a unique event unless they are emitted using a multi-line-aware protocol. The CloudWatch agent can perform this with the <code>multi_line_start_pattern</code> directive.</p> <p>Success</p> <p>Use the <code>multi_line_start_pattern</code> directive to ease the burden of ingesting muli-line logging into CloudWatch Logs.</p>"},{"location":"tools/logs/#search-with-cloudwatch-logs","title":"Search with CloudWatch Logs","text":""},{"location":"tools/logs/#manage-costs-with-query-scoping","title":"Manage costs with query scoping","text":"<p>With data delivered into CloudWatch Logs, you can now search through it as required. Be aware that CloudWatch Logs charges per gigabyte of data scanned. There are strategies for keeping your query scope under control, which will result in reduced data scanned.</p> <p>Success</p> <p>When searching your logs ensure that your time and date range is appropriate. CloudWatch Logs allows you to set relative or absolute time ranges for scans. If you are only looking for entries from the day before, then there is no need to include scans of logs from today!</p> <p>Success</p> <p>You can search multiple log groups in a single query, but doing so will cause more data to be scanned. When you have identified the log group(s) you need to target, reduce your query scope to match.</p> <p>Tip</p> <p>You can see how much data each query actually scans directly from the CloudWatch console. This approach can help you create queries that are efficient.</p> <p></p>"},{"location":"tools/logs/#share-successful-queries-with-others","title":"Share successful queries with others","text":"<p>While the CloudWatch Logs query syntax is not complex (there are only seven commands), it can still be time consuming to write some queries from scratch. Sharing your well-written queries with other users in the same AWS account can be accomplished directly from within the AWS console or using CloudFormation. This helps reduce the amount of rework required if others need to investigate application logs.</p> <p>Success</p> <p>Save queries that are often repeated into CloudWatch Logs so they can be prepopulated for your users.</p> <p></p> <ol> <li> <p>See How to search through your AWS Systems Manager Session Manager console logs \u2013 Part 1 for a practical example of CloudWatch Logs log group encryption with access privileges.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/logs/logs-insights-examples/","title":"CloudWatch Logs Insights Example Queries","text":"<p>CloudWatch Logs Insights provides a powerful platform for analyzing and querying CloudWatch log data. It allows you interactively search through your log data using a SQL like query language with a few simple but powerful commands. </p> <p>CloudWatch Logs insights provides out of the box example queries for the following categories:</p> <ul> <li>Lambda</li> <li>VPC Flow Logs</li> <li>CloudTrail</li> <li>Common Queries</li> <li>Route 53</li> <li>AWS AppSync</li> <li>NAT Gateway</li> </ul> <p>In this section of the best practices guide we provide some example queries for other types of logs that are not currently included in the out of the box examples. This list will evolve and change over time and you can submit your own examples for review by leaving an issue on the git hub.</p>"},{"location":"tools/logs/logs-insights-examples/#api-gateway","title":"API Gateway","text":""},{"location":"tools/logs/logs-insights-examples/#last-20-messages-containing-an-http-method-type","title":"Last 20 Messages containing an HTTP Method Type","text":"<pre><code>filter @message like /$METHOD/ | fields @timestamp, @message\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>This query will return the last 20 log messages containing a specific HTTP method sorted in descending timestamp order. Substitute METHOD for the method you are querying for. Here is an example of how to use this query:</p> <pre><code>filter @message like /POST/ | fields @timestamp, @message\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>Tip</p> <p>You can change the $limit value in order to return a different amount of messages.</p>"},{"location":"tools/logs/logs-insights-examples/#top-20-talkers-sorted-by-ip","title":"Top 20 Talkers Sorted by IP","text":"<pre><code>fields @timestamp, @message\n| stats count() by ip\n| sort ip asc\n| limit 20\n</code></pre> <p>This query will return the top 20 talkers sorted by IP. This can be useful for detecting malicious activity against your API.</p> <p>As a next step you could then add an additional filter for method type. For example, this query would show the top talkers by IP but only the \"PUT\" method call:</p> <pre><code>fields @timestamp, @message\n| filter @message like /PUT/\n| stats count() by ip\n| sort ip asc\n| limit 20\n</code></pre>"},{"location":"tools/logs/logs-insights-examples/#cloudtrail-logs","title":"CloudTrail Logs","text":""},{"location":"tools/logs/logs-insights-examples/#api-throttling-errors-grouped-by-error-category","title":"API throttling errors grouped by error category","text":"<pre><code>stats count(errorCode) as eventCount by eventSource, eventName, awsRegion, userAgent, errorCode\n| filter errorCode = 'ThrottlingException' \n| sort eventCount desc\n</code></pre> <p>This query allows you to see API throttling errors grouped by category and displayed in descending order.</p> <p>Tip</p> <p>In order to use this query you would first need to ensure you are sending CloudTrail logs to CloudWatch.</p>"},{"location":"tools/logs/logs-insights-examples/#vpc-flow-logs","title":"VPC Flow Logs","text":""},{"location":"tools/logs/logs-insights-examples/#filtering-flow-logs-for-selected-source-ip-address-with-action-as-reject","title":"Filtering flow logs for selected source IP address with action as REJECT.","text":"<pre><code>fields @timestamp, @message, @logStream, @log  | filter srcAddr like '$SOURCEIP' and action = 'REJECT'\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>This query will return the last 20 log messages containing a 'REJECT' from the $SOURCEIP. This can be used to detect if traffic is explicitly rejected, or if the issue is some type of client side network configuration problem.</p> <p>Tip</p> <p>Ensure that you substitute the value of the IP address you are interested in for '$SOURCEIP'</p> <pre><code>fields @timestamp, @message, @logStream, @log  | filter srcAddr like '10.0.0.5' and action = 'REJECT'\n| sort @timestamp desc\n| limit 20\n</code></pre>"}]}